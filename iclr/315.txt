Published as a conference paper at ICLR 2018

MODEL COMPRESSION VIA
DISTILLATION AND QUANTIZATION

Antonio Polino
ETH Z¨urich
antonio.polino1@gmail.com

Dan Alistarh
IST Austria
dan.alistarh@ist.ac.at

Razvan Pascanu
Google DeepMind
razp@google.com

ABSTRACT

Deep neural networks (DNNs) continue to make signiﬁcant advances, solving
tasks from image classiﬁcation to translation or reinforcement learning. One
aspect of the ﬁeld receiving considerable attention is efﬁciently executing deep
models in resource-constrained environments, such as mobile or embedded de-
vices. This paper focuses on this problem, and proposes two new compression
methods, which jointly leverage weight quantization and distillation of larger net-
works, called “teachers,” into compressed “student” networks. The ﬁrst method
we propose is called quantized distillation and leverages distillation during the
training process, by incorporating distillation loss, expressed with respect to the
teacher network, into the training of a smaller student network whose weights are
quantized to a limited set of levels. The second method, differentiable quanti-
zation, optimizes the location of quantization points through stochastic gradient
descent, to better ﬁt the behavior of the teacher model. We validate both methods
through experiments on convolutional and recurrent architectures. We show that
quantized shallow students can reach similar accuracy levels to state-of-the-art
full-precision teacher models, while providing up to order of magnitude compres-
sion, and inference speedup that is almost linear in the depth reduction. In sum,
our results enable DNNs for resource-constrained environments to leverage archi-
tecture and accuracy advances developed on more powerful devices.

1

INTRODUCTION

Background. Neural networks are extremely effective for solving several real world problems, like
image classiﬁcation (Krizhevsky et al., 2012; He et al., 2016a), translation (Vaswani et al., 2017),
voice synthesis (Oord et al., 2016) or reinforcement learning (Mnih et al., 2013; Silver et al., 2016).
At the same time, modern neural network architectures are often compute, space and power hungry,
typically requiring powerful GPUs to train and evaluate. The debate is still ongoing on whether
large models are necessary for good accuracy. It is known that individual network weights can be
redundant, and may not carry signiﬁcant information, e.g. Han et al. (2015). At the same time, large
models often have the ability to completely memorize datasets (Zhang et al., 2016), yet they do
not, but instead appear to learn generic task solutions. A standing hypothesis for why overcomplete
representations are necessary is that they make learning possible by transforming local minima into
saddle points (Dauphin et al., 2014) or to discover robust solutions, which do not rely on precise
weight values (Hochreiter & Schmidhuber, 1997; Keskar et al., 2016).
If large models are only needed for robustness during training, then signiﬁcant compression of these
models should be achievable, without impacting accuracy. This intuition is strengthened by two
related, but slightly different research directions. The ﬁrst direction is the work on training quantized
neural networks, e.g. Courbariaux et al. (2015); Rastegari et al. (2016); Hubara et al. (2016); Wu
et al. (2016a); Mellempudi et al. (2017); Ott et al. (2016); Zhu et al. (2016), which showed that
neural networks can converge to good task solutions even when weights are constrained to having
values from a set of integer levels. The second direction aims to compress already-trained models,
while preserving their accuracy. To this end, various elegant compression techniques have been

1

