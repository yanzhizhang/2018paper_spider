Published as a conference paper at ICLR 2018

A BAYESIAN PERSPECTIVE ON GENERALIZATION AND
STOCHASTIC GRADIENT DESCENT

Samuel L. Smith∗ & Quoc V. Le
Google Brain
{slsmith, qvl}@google.com

ABSTRACT

We consider two questions at the heart of machine learning; how can we predict
if a minimum will generalize to the test set, and why does stochastic gradient
descent ﬁnd minima that generalize well? Our work responds to Zhang et al.
(2016), who showed deep neural networks can easily memorize randomly labeled
training data, despite generalizing well on real labels of the same inputs. We show
that the same phenomenon occurs in small linear models. These observations are
explained by the Bayesian evidence, which penalizes sharp minima but is invariant
to model parameterization. We also demonstrate that, when one holds the learning
rate ﬁxed, there is an optimum batch size which maximizes the test set accuracy.
We propose that the noise introduced by small mini-batches drives the parameters
towards minima whose evidence is large. Interpreting stochastic gradient descent
B − 1) ≈
as a stochastic differential equation, we identify the “noise scale” g = ( N
N/B, where  is the learning rate, N the training set size and B the batch size.
Consequently the optimum batch size is proportional to both the learning rate and
the size of the training set, Bopt ∝ N. We verify these predictions empirically.

1

INTRODUCTION

This paper shows Bayesian principles can explain many recent observations in the deep learning
literature, while also discovering practical new insights. Zhang et al. (2016) trained deep convolu-
tional networks on ImageNet and CIFAR10, achieving excellent accuracy on both training and test
sets. They then took the same input images, but randomized the labels, and found that while their
networks were now unable to generalize to the test set, they still memorized the training labels. They
claimed these results contradict learning theory, although this claim is disputed (Kawaguchi et al.,
2017; Dziugaite & Roy, 2017). Nonetheless, their results beg the question; if our models can assign
arbitrary labels to the training set, why do they work so well in practice? Meanwhile Keskar et al.
(2016) observed that if we hold the learning rate ﬁxed and increase the batch size, the test accuracy
usually falls. This striking result shows improving our estimate of the full-batch gradient can harm
performance. Goyal et al. (2017) observed a linear scaling rule between batch size and learning rate
in a deep ResNet, while Hoffer et al. (2017) proposed a square root rule on theoretical grounds.
Many authors have suggested “broad minima” whose curvature is small may generalize better than
“sharp minima” whose curvature is large (Chaudhari et al., 2016; Hochreiter & Schmidhuber, 1997).
Indeed, Dziugaite & Roy (2017) argued the results of Zhang et al. (2016) can be understood using
“nonvacuous” PAC-Bayes generalization bounds which penalize sharp minima, while Keskar et al.
(2016) showed stochastic gradient descent (SGD) ﬁnds wider minima as the batch size is reduced.
However Dinh et al. (2017) challenged this interpretation, by arguing that the curvature of a mini-
mum can be arbitrarily increased by changing the model parameterization. In this work we show:
• The results of Zhang et al. (2016) are not unique to deep learning; we observe the same
phenomenon in a small “over-parameterized” linear model. We demonstrate that this phe-
nomenon is straightforwardly understood by evaluating the Bayesian evidence in favor of
each model, which penalizes sharp minima but is invariant to the model parameterization.

∗Work done as a member of the Google Brain Residency Program (g.co/brainresidency)

1

