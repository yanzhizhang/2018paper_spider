Published as a conference paper at ICLR 2018

NATURAL LANGUAGE INFERENCE OVER
INTERACTION SPACE

Yichen Gong†‡, Heng Luo‡, Jian Zhang‡
† New York University, New York, USA
‡ Horizon Robotics, Inc., Beijing, China
yichen.gong@nyu.edu, {heng.luo, jian.zhang}@hobot.cc

ABSTRACT

Natural Language Inference (NLI) task requires an agent to determine the logical
relationship between a natural language premise and a natural language hypoth-
esis. We introduce Interactive Inference Network (IIN), a novel class of neural
network architectures that is able to achieve high-level understanding of the sen-
tence pair by hierarchically extracting semantic features from interaction space.
We show that an interaction tensor (attention weight) contains semantic informa-
tion to solve natural language inference, and a denser interaction tensor contains
richer semantic information. One instance of such architecture, Densely Interac-
tive Inference Network (DIIN), demonstrates the state-of-the-art performance on
large scale NLI copora and large-scale NLI alike corpus. It’s noteworthy that DIIN
achieve a greater than 20% error reduction on the challenging Multi-Genre NLI
(MultiNLI; Williams et al. 2017) dataset with respect to the strongest published
system.

1

INTRODUCTION

Natural Language Inference (NLI also known as recognizing textual entiailment, or RTE) task re-
quires one to determine whether the logical relationship between two sentences is among entailment
(if the premise is true, then the hypothesis must be true), contradiction (if the premise is true, then
the hypothesis must be false) and neutral (neither entailment nor contradiction). NLI is known as a
fundamental and yet challenging task for natural language understanding(Williams et al., 2017), not
only because it requires one to identify the language pattern, but also to understand certain common
sense knowledge. In Table 1, three samples from MultiNLI corpus show solving the task requires
one to handle the full complexity of lexical and compositional semantics. The previous work on
NLI (or RTE) has extensively researched on conventional approaches(Fyodorov et al., 2000; Bos &
Markert, 2005; MacCartney & Manning, 2009). Recent progress on NLI is enabled by the availabil-
ity of 570k human annotated dataset(Bowman et al., 2015) and the advancement of representation
learning technique.
Among the core representation learning techniques, attention mechanism is broadly applied in many
NLU tasks since its introduction: machine translation(Bahdanau et al., 2014), abstractive summa-
rization(Rush et al., 2015), Reading Comprehension(Hermann et al., 2015), dialog system(Mei et al.,
2016), etc. As described by Vaswani et al. (2017), “An attention function can be described as map-
ping a query and a set of key-value pairs to an output, where the query, keys, values, and output
are all vectors. The output is computed as a weighted sum of the values, where the weight assigned
to each value is computed by a compatibility function of the query with the corresponding key”.
Attention mechanism is known for its alignment between representations, focusing one part of rep-
resentation over another, and modeling the dependency regardless of sequence length. Observing
attention’s powerful capability, we hypothesize that the attention weight can assist with machine to
understanding the text.
A regular attention weight, the core component of the attention mechanism, encodes the cross-
sentence word relationship into a alignment matrix. However, a multi-head attention weightVaswani
et al. (2017) can encode such interaction into multiple alignment matrices, which shows a more
powerful alignment. In this work, we push the multi-head attention to a extreme by building a word-

1

