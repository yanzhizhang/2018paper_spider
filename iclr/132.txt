Published as a conference paper at ICLR 2018

STOCHASTIC GRADIENT DESCENT PERFORMS VARIATIONAL
INFERENCE, CONVERGES TO LIMIT CYCLES FOR DEEP NETWORKS

Pratik Chaudhari, Stefano Soatto
Computer Science, University of California, Los Angeles.
Email: pratikac@ucla.edu, soatto@ucla.edu

ABSTRACT

Stochastic gradient descent (SGD) is widely believed to perform implicit
regularization when used to train deep neural networks, but the precise manner
in which this occurs has thus far been elusive. We prove that SGD minimizes an
average potential over the posterior distribution of weights along with an entropic
regularization term. This potential is however not the original loss function in
general. So SGD does perform variational inference, but for a different loss than
the one used to compute the gradients. Even more surprisingly, SGD does not even
converge in the classical sense: we show that the most likely trajectories of SGD
for deep networks do not behave like Brownian motion around critical points.
Instead, they resemble closed loops with deterministic components. We prove
that such “out-of-equilibrium” behavior is a consequence of highly non-isotropic
gradient noise in SGD; the covariance matrix of mini-batch gradients for deep
networks has a rank as small as 1% of its dimension. We provide extensive
empirical validation of these claims, proven in the appendix.

Keywords: deep networks, stochastic gradient descent, variational inference, gradient
noise, out-of-equilibrium, thermodynamics, Wasserstein metric, Fokker-Planck equation,
wide minima, Markov chain Monte Carlo

1

INTRODUCTION

Our ﬁrst result is to show precisely in what sense stochastic gradient descent (SGD) implicitly
performs variational inference, as is often claimed informally in the literature. For a loss function
f (x) with weights x ∈ Rd, if ρss is the steady-state distribution over the weights estimated by SGD,

(cid:104)

(cid:105)− η

2(cid:98)

ρss = arg min

ρ

E x∼ρ

Φ(x)

H(ρ);

where H(ρ) is the entropy of the distribution ρ and η and (cid:98) are the learning rate and batch-size,
respectively. The potential Φ(x), which we characterize explicitly, is related but not necessarily equal
to f (x). It is only a function of the architecture and the dataset. This implies that SGD implicitly
performs variational inference with a uniform prior, albeit of a different loss than the one used to
compute back-propagation gradients.
We next prove that the implicit potential Φ(x) is equal to our chosen loss f (x) if and only if the
noise in mini-batch gradients is isotropic. This condition, however, is not satisﬁed for deep networks.
Empirically, we ﬁnd gradient noise to be highly non-isotropic with the rank of its covariance matrix
being about 1% of its dimension. Thus, SGD on deep networks implicitly discovers locations where
∇Φ(x) = 0, these are not the locations where ∇ f (x) = 0. This is our second main result: the most
likely locations of SGD are not the local minima, nor the saddle points, of the original loss. The
deviation of these critical points, which we compute explicitly scales linearly with η/(cid:98) and is
typically large in practice.

1

