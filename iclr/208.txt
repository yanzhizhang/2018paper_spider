Published as a conference paper at ICLR 2018

LEARNING ONE-HIDDEN-LAYER NEURAL NETWORKS
WITH LANDSCAPE DESIGN

Rong Ge
Computer Science Department
Duke University
rongge@cs.duke.edu

Tengyu Ma
Facebook AI Research
tengyuma@cs.stanford.edu

Jason D. Lee
Data Sciences and Operations Department,
University of Southern California
jasonlee@marshall.usc.edu

ABSTRACT

We consider the problem of learning a one-hidden-layer neural network: we
assume the input x ∈ Rd is from Gaussian distribution and the label y =
a(cid:62)σ(Bx) + ξ, where a is a nonnegative vector in Rm with m ≤ d, B ∈ Rm×d
is a full-rank weight matrix, and ξ is a noise vector. We ﬁrst give an analytic for-
mula for the population risk of the standard squared loss and demonstrate that it
implicitly attempts to decompose a sequence of low-rank tensors simultaneously.
Inspired by the formula, we design a non-convex objective function G(·) whose
landscape is guaranteed to have the following properties:

1. All local minima of G are also global minima.
2. All global minima of G correspond to the ground truth parameters.
3. The value and gradient of G can be estimated using samples.
With these properties, stochastic gradient descent on G provably converges to
the global minimum and learn the ground-truth parameters. We also prove ﬁnite
sample complexity results and validate the results by simulations.

1

INTRODUCTION

Scalable optimization has played an important role in the success of deep learning, which has im-
mense applications in artiﬁcial intelligence. Remarkably, optimization issues are often addressed
through designing new models that make the resulting training objective functions easier to be
optimized. For example, over-parameterization (Livni et al., 2014), batch-normalization (Ioffe &
Szegedy, 2015), and residual networks (He et al., 2016a;b) are often considered as ways to improve
the optimization landscape of the resulting objective functions.
How do we design models and objective functions that allow efﬁcient optimization with guarantees?
Towards understanding this question in a principled way, this paper studies learning neural networks
with one hidden layer. Roughly speaking, we will show that when the input is from Gaussian
distribution and under certain simplifying assumptions on the weights, we can design an objective
function G(·), such that
[a] all local minima of G(·) are global minima
[b] all the global minima are the desired solutions, namely, the ground-truth parameters (up to per-
mutation and some ﬁxed transformation).
We note that designing such objective functions is challenging because 1) the natural (cid:96)2 loss objec-
tive does have bad local minimum, and 2) due to the permutation invariance1, the objective function
inherently has to contain an exponential number of isolated local minima.

1Permuting the rows of B(cid:63) and the coordinates of a(cid:63) correspondingly preserves the functionality of the

network.

1

