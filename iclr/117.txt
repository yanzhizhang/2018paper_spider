Published as a conference paper at ICLR 2018

KERNEL IMPLICIT VARIATIONAL INFERENCE

Shengyang Sun∗‡,

Jiaxin Shi∗†,
†Department of Computer Science & Technology, THU Lab for Brain and AI, Tsinghua University
‡Department of Computer Science, University of Toronto
shijx15@mails.tsinghua.edu.cn, ssy@cs.toronto.edu, dcszj@tsinghua.edu.cn

Jun Zhu†

ABSTRACT

Recent progress in variational inference has paid much attention to the ﬂexibility
of variational posteriors. One promising direction is to use implicit distributions,
i.e., distributions without tractable densities as the variational posterior. However,
existing methods on implicit posteriors still face challenges of noisy estimation
and computational infeasibility when applied to models with high-dimensional
latent variables. In this paper, we present a new approach named Kernel Implicit
Variational Inference that addresses these challenges. As far as we know, for the
ﬁrst time implicit variational inference is successfully applied to Bayesian neural
networks, which shows promising results on both regression and classiﬁcation
tasks.

1

INTRODUCTION

Bayesian methods have been playing vital roles in machine learning by providing a principled
approach for generative modeling, posterior inference and preventing over-ﬁtting (Ghahramani,
2015). As it becomes a common practice to build deep models that have many parameters (LeCun
et al., 2015), it is even more important to have a Bayesian formulation to capture the uncertainty
in these models. For example, Bayesian Neural Networks (BNNs) (Neal, 2012; Blundell et al.,
2015) have shown promise in reasoning about model conﬁdence and learning with few labeled data.
Another recent trend is to incorporate deep neural networks as a powerful function mapping between
random variables in a Bayesian network, such as deep generative models like variational autoencoders
(VAE) (Kingma & Welling, 2013).
Except a few simple examples, Bayesian inference is typically challenging, for which variational
inference (VI) has been a standard workhorse to approximate the true posterior (Zhu et al., 2017).
Traditional VI focuses on factorized variational posteriors to get analytical updates (known as Mean-
ﬁeld VI). While recent progress in this ﬁeld drives VI into stochastic, differentiable and amortized
(Hoffman et al., 2013; Paisley et al., 2012; Mnih & Gregor, 2014; Kingma & Welling, 2013), which
does not rely on analytical updates anymore, factorized posteriors are still commonly used as the
variational family. This greatly restricts the ﬂexibility of the variational posterior, especially in high-
dimensional spaces, which often leads to biased solutions as the true posterior is usually not factorized,
thus not in the family. There have been some works that try to improve the ﬂexibility of variational
posteriors, borrowing ideas from invertible transformation of probability distributions (Rezende &
Mohamed, 2015; Kingma et al., 2016). In their works, it is important for the transformation to be
invertible to ensure that the transformed distribution has a tractable density.
Although utilizing invertible transformation is a promising direction to increase the expressiveness
of the variational posterior, we argue that a more ﬂexible variational family can be constructed by
using general deterministic or stochastic transformations, which are not necessarily invertible. As a
common result, the variational posterior we get in this way does not have a tractable density, despite
that there is a way to sample from it. This kind of distribution is called implicit distributions, and for
variational methods that use an implicit variational posterior (also known as variational programs
(Ranganath et al., 2016) or wild variational approximations (Liu & Feng, 2016)), we refer to them as
Implicit Variational Inference (implicit VI). Most of the existing implicit VI methods (Mescheder
et al., 2017; Husz´ar, 2017; Tran et al., 2017) rely on a discriminator to produce estimates of the

∗These authors contribute equally; J.Z is the corresponding author.

1

