Published as a conference paper at ICLR 2018

MEMORIZATION PRECEDES GENERATION: LEARNING
UNSUPERVISED GANS WITH MEMORY NETWORKS

Youngjin Kim, Minjung Kim & Gunhee Kim
Department of Computer Science and Engineering, Seoul National University, Seoul, Korea
{youngjin.kim,minjung.kim,gunhee.kim}@vision.snu.ac.kr

ABSTRACT

We propose an approach to address two issues that commonly occur during train-
ing of unsupervised GANs. First, since GANs use only a continuous latent dis-
tribution to embed multiple classes or clusters of data, they often do not correctly
handle the structural discontinuity between disparate classes in a latent space.
Second, discriminators of GANs easily forget about past generated samples by
generators, incurring instability during adversarial training. We argue that these
two infamous problems of unsupervised GAN training can be largely alleviated
by a learnable memory network to which both generators and discriminators can
access. Generators can effectively learn representation of training samples to un-
derstand underlying cluster distributions of data, which ease the structure discon-
tinuity problem. At the same time, discriminators can better memorize clusters of
previously generated samples, which mitigate the forgetting problem. We propose
a novel end-to-end GAN model named memoryGAN, which involves a memory
network that is unsupervisedly trainable and integrable to many existing GAN
models. With evaluations on multiple datasets such as Fashion-MNIST, CelebA,
CIFAR10, and Chairs, we show that our model is probabilistically interpretable,
and generates realistic image samples of high visual ﬁdelity. The memoryGAN
also achieves the state-of-the-art inception scores over unsupervised GAN models
on the CIFAR10 dataset, without any optimization tricks and weaker divergences.

1

INTRODUCTION

Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are one of emerging branches
of unsupervised models for deep neural networks. They consist of two neural networks named
generator and discriminator that compete each other in a zero-sum game framework. GANs have
been successfully applied to multiple generation tasks, including image syntheses (e.g. (Reed et al.,
2016b; Radford et al., 2016; Zhang et al., 2016a)), image super-resolution (e.g. (Ledig et al., 2017;
Snderby et al., 2017)), image colorization (e.g. (Zhang et al., 2016b)), to name a few. Despite such
remarkable progress, GANs are notoriously difﬁcult to train. Currently, such training instability
problems have mostly tackled by ﬁnding better distance measures (e.g. (Li et al., 2015; Nowozin
et al., 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017; Gulrajani et al., 2017; Warde-Farley
& Bengio, 2017; Mroueh et al., 2017; Mroueh & Sercu, 2017)) or regularizers (e.g. (Salimans et al.,
2016; Metz et al., 2017; Che et al., 2017; Berthelot et al., 2017)).
We aim at alleviating two undesired properties of unsupervised GANs that cause instability during
training. The ﬁrst one is that GANs use a unimodal continuous latent space (e.g. Gaussian distribu-
tion), and thus fail to handle structural discontinuity between different classes or clusters. It partly
attributes to the infamous mode collapsing problem. For example, GANs embed both building and
cats into a common continuous latent distribution, even though there is no intermediate structure
between them. Hence, even a perfect generator would produce unrealistic images for some latent
codes that reside in transitional regions of two disparate classes. Fig.1 (a,c) visualize this problem
with examples of afﬁne-transformed MNIST and Fashion-MNIST datasets. There always exist la-
tent regions that cause unrealistic samples (red boxes) between different classes (blue boxes at the
corners). Another problem is the discriminator’s forgetting behavior about past synthesized samples
by the generator, during adversarial training of GANs. The catastrophic forgetting has explored
in deep network research, such as (Kirkpatrick et al., 2016; Kemker et al., 2017); in the context of

1

