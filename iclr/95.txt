Published as a conference paper at ICLR 2018

UNBIASED ONLINE RECURRENT OPTIMIZATION

Corentin Tallec
Laboratoire de Recherche en Informatique
Université Paris Sud
Gif-sur-Yvette, 91190, France
corentin.tallec@u-psud.fr

Yann Ollivier
Laboratoire de Recherche en Informatique
Université Paris Sud
Gif-sur-Yvette, 91190, France
yann@yann-ollivier.org

ABSTRACT

The novel Unbiased Online Recurrent Optimization (UORO) algorithm allows for
online learning of general recurrent computational graphs such as recurrent net-
work models. It works in a streaming fashion and avoids backtracking through
past activations and inputs. UORO is computationally as costly as Truncated
Backpropagation Through Time (truncated BPTT), a widespread algorithm for
online learning of recurrent networks Jaeger (2002). UORO is a modiﬁcation of
NoBackTrack Ollivier et al. (2015) that bypasses the need for model sparsity and
makes implementation easy in current deep learning frameworks, even for com-
plex models. Like NoBackTrack, UORO provides unbiased gradient estimates;
unbiasedness is the core hypothesis in stochastic gradient descent theory, without
which convergence to a local optimum is not guaranteed. On the contrary, trun-
cated BPTT does not provide this property, leading to possible divergence. On
synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For
instance, when a parameter has a positive short-term but negative long-term in-
ﬂuence, truncated BPTT diverges unless the truncation span is very signiﬁcantly
longer than the intrinsic temporal range of the interactions, while UORO performs
well thanks to the unbiasedness of its gradients.

Current recurrent network learning algorithms are ill-suited to online learning via a single pass
through long sequences of temporal data. Backpropagation Through Time (BPTT Jaeger (2002)), the
current standard for training recurrent architectures, is well suited to many short training sequences.
Treating long sequences with BPTT requires either storing all past inputs in memory and waiting
for a long time between each learning step, or arbitrarily splitting the input sequence into smaller
sequences, and applying BPTT to each of those short sequences, at the cost of losing long term
dependencies.
This paper introduces Unbiased Online Recurrent Optimization (UORO), an online and memoryless
learning algorithm for recurrent architectures: UORO processes and learns from data samples se-
quentially, one sample at a time. Contrary to BPTT, UORO does not maintain a history of previous
inputs and activations. Moreover, UORO is scalable: processing data samples with UORO comes
at a similar computational and memory cost as just running the recurrent model on those data.
Like most neural network training algorithms, UORO relies on stochastic gradient optimization. The
theory of stochastic gradient crucially relies on the unbiasedness of gradient estimates to provide
convergence to a local optimum. To this end, in the footsteps of NoBackTrack (NBT) Ollivier et al.
(2015), UORO provides provably unbiased gradient estimates, in a scalable, streaming fashion.
Unlike NBT, though, UORO can be easily implemented in a black-box fashion on top of an existing
recurrent model in current machine learning software, without delving into the structure and code of
the model.
The framework for recurrent optimization and UORO is introduced in Section 2. The ﬁnal algorithm
is reasonably simple (Alg. 1), but its derivation (Section 3) is more complex. In Section 6, UORO
is shown to provide convergence on a set of synthetic experiments where truncated BPTT fails to
display reliable convergence. An implementation of UORO is provided as supplementary material.

1

