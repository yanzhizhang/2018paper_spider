Published as a conference paper at ICLR 2018

TRAINING WIDE RESIDUAL NETWORKS FOR DEPLOY-
MENT USING A SINGLE BIT FOR EACH WEIGHT

Mark D. McDonnell ∗
Computational Learning Systems Laboratory (cls-lab.org)
School of Information Technology and Mathematical Sciences
University of South Australia
Mawson Lakes, SA 5095, AUSTRALIA
mark.mcdonnell@unisa.edu.au

ABSTRACT

For fast and energy-efﬁcient deployment of trained deep neural networks on
resource-constrained embedded hardware, each learned weight parameter should
ideally be represented and stored using a single bit. Error-rates usually increase
when this requirement is imposed. Here, we report large improvements in er-
ror rates on multiple datasets, for deep convolutional neural networks deployed
with 1-bit-per-weight.
Using wide residual networks as our main baseline,
our approach simpliﬁes existing methods that binarize weights by applying the
sign function in training; we apply scaling factors for each layer with constant
unlearned values equal to the layer-speciﬁc standard deviations used for ini-
tialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with 1-bit-
per-weight requiring less than 10 MB of parameter memory, we achieve error
rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We also
considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test re-
sults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error
rates halve previously reported values, and are within about 1% of our error-
rates for the same network with full-precision weights. For networks that over-
ﬁt, we also show signiﬁcant improvements in error rate by not learning batch
normalization scale and offset parameters. This applies to both full precision
and 1-bit-per-weight networks. Using a warm-restart learning-rate schedule,
we found that training for 1-bit-per-weight is just as fast as full-precision net-
works, with better accuracy than standard schedules, and achieved about 98%-
99% of peak performance in just 62 training epochs for CIFAR-10/100. For
full training code and trained models in MATLAB, Keras and PyTorch see
https://github.com/McDonnell-Lab/1-bit-per-weight/.

1

INTRODUCTION

Fast parallel computing resources, namely GPUs, have been integral to the resurgence of deep neu-
ral networks, and their ascendancy to becoming state-of-the-art methodologies for many computer
vision tasks. However, GPUs are both expensive and wasteful in terms of their energy requirements.
They typically compute using single-precision ﬂoating point (32 bits), which has now been recog-
nized as providing far more precision than needed for deep neural networks. Moreover, training
and deployment can require the availability of large amounts of memory, both for storage of trained
models, and for operational RAM. If deep-learning methods are to become embedded in resource-
constrained sensors, devices and intelligent systems, ranging from robotics to the internet-of-things
to self-driving cars, reliance on high-end computing resources will need to be reduced.
To this end, there has been increasing interest in ﬁnding methods that drive down the resource burden
of modern deep neural networks. Existing methods typically exhibit good performance but for the
∗This work was conducted, in part, during a hosted visit at the Institute for Neural Computation, University
of California, San Diego, and in part, during a sabbatical period at Consilium Technology, Adelaide, Australia.

1

