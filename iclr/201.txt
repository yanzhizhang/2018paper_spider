Published as a conference paper at ICLR 2018

BOUNDARY-SEEKING
GENERATIVE ADVERSARIAL NETWORKS

R Devon Hjelm∗
MILA, University of Montr´eal, IVADO
erroneus@gmail.com

Athul Paul Jacob∗
MILA, MSR, University of Waterloo
apjacob@edu.uwaterloo.ca

Tong Che
MILA, University of Montr´eal
tong.che@umontreal.ca

Kyunghyun Cho
New York University,
CIFAR Azrieli Global Scholar
kyunghyun.cho@nyu.edu

Adam Trischler
MSR
adam.trischler@microsoft.com

Yoshua Bengio
MILA, University of Montr´eal, CIFAR, IVADO
yoshua.bengio@umontreal.ca

ABSTRACT

Generative adversarial networks (GANs, Goodfellow et al., 2014) are a learning
framework that rely on training a discriminator to estimate a measure of difference
between a target and generated distributions. GANs, as normally formulated, rely
on the generated samples being completely differentiable w.r.t.
the generative
parameters, and thus do not work for discrete data. We introduce a method for
training GANs with discrete data that uses the estimated difference measure from
the discriminator to compute importance weights for generated samples, thus pro-
viding a policy gradient for training the generator. The importance weights have
a strong connection to the decision boundary of the discriminator, and we call our
method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of
the proposed algorithm with discrete image and character-based natural language
generation.
In addition, the boundary-seeking objective extends to continuous
data, which can be used to improve stability of training, and we demonstrate this
on Celeba, Large-scale Scene Understanding (LSUN) bedrooms, and Imagenet
without conditioning.

1

INTRODUCTION

Generative adversarial networks (GAN, Goodfellow et al., 2014) involve a unique generative learn-
ing framework that uses two separate models, a generator and discriminator, with opposing or adver-
sarial objectives. Training a GAN only requires back-propagating a learning signal that originates
from a learned objective function, which corresponds to the loss of the discriminator trained in an
adversarial manner. This framework is powerful because it trains a generator without relying on an
explicit formulation of the probability density, using only samples from the generator to train.
GANs have been shown to generate often-diverse and realistic samples even when trained on high-
dimensional large-scale continuous data (Radford et al., 2015). GANs however have a serious limi-
tation on the type of variables they can model, because they require the composition of the generator
and discriminator to be fully differentiable.
With discrete variables, this is not true. For instance, consider using a step function at the end of
a generator in order to generate a discrete value. In this case, back-propagation alone cannot pro-
vide the training signal, because the derivative of a step function is 0 almost everywhere. This is
problematic, as many important real-world datasets are discrete, such as character- or word-based

∗Denotes ﬁrst-author contributions.

1

