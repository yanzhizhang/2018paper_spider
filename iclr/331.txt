Published as a conference paper at ICLR 2018

SPECTRALNET: SPECTRAL CLUSTERING USING DEEP
NEURAL NETWORKS

Uri Shaham∗†, Kelly Stanton∗, Henry Li∗
Yale University
New Haven, CT, USA
{uri.shaham, kelly.stanton, henry.li}@yale.edu

Boaz Nadler, Ronen Basri
Weizmann Institute of Science
Rehovot, Israel
{boaz.nadler, ronen.basri}@gmail.com

Yuval Kluger
Yale University
New Haven, CT, USA
yuval.kluger@yale.edu

ABSTRACT

Spectral clustering is a leading and popular technique in unsupervised data anal-
ysis. Two of its major limitations are scalability and generalization of the spec-
tral embedding (i.e., out-of-sample-extension). In this paper we introduce a deep
learning approach to spectral clustering that overcomes the above shortcomings.
Our network, which we call SpectralNet, learns a map that embeds input data
points into the eigenspace of their associated graph Laplacian matrix and sub-
sequently clusters them. We train SpectralNet using a procedure that involves
constrained stochastic optimization. Stochastic optimization allows it to scale
to large datasets, while the constraints, which are implemented using a special-
purpose output layer, allow us to keep the network output orthogonal. More-
over, the map learned by SpectralNet naturally generalizes the spectral embed-
ding to unseen data points. To further improve the quality of the clustering, we
replace the standard pairwise Gaussian afﬁnities with afﬁnities learned from the
given unlabeled data using a Siamese network. Additional improvement of the
resulting clustering can be achieved by applying the network to code represen-
tations produced, e.g., by standard autoencoders. Our end-to-end learning pro-
cedure is fully unsupervised. In addition, we apply VC dimension theory to de-
rive a lower bound on the size of SpectralNet. State-of-the-art clustering results
are reported on the Reuters dataset. Our implementation is publicly available
at https://github.com/kstant0725/SpectralNet.

1

INTRODUCTION

Discovering clusters in unlabeled data is a task of signiﬁcant scientiﬁc and practical value. With
technological progress images, texts, and other types of data are acquired in large numbers. Their
labeling, however, is often expensive, tedious, or requires expert knowledge. Clustering techniques
provide useful tools to analyze such data and to reveal its underlying structure.
Spectral Clustering (Shi & Malik, 2000; Ng et al., 2002; Von Luxburg, 2007) is a leading and highly
popular clustering algorithm. It works by embedding the data in the eigenspace of the Laplacian
matrix, derived from the pairwise similarities between data points, and applying k-means to this
representation to obtain the clusters. Several properties make spectral clustering appealing: First,
its embedding optimizes a natural cost function, minimizing pairwise distances between similar
data points; moreover, this optimal embedding can be found analytically. Second, spectral clus-
tering variants arise as relaxations of graph balanced-cut problems (Von Luxburg, 2007). Third,
spectral clustering was shown to outperform other popular clustering algorithms such as k-means

∗Equal contribution.
†Also at Final Research, Herzliya, Israel.

1

