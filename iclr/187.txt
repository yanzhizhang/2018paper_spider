Published as a conference paper at ICLR 2018

UNSUPERVISED NEURAL MACHINE TRANSLATION

Mikel Artetxe, Gorka Labaka & Eneko Agirre
IXA NLP Group
University of the Basque Country (UPV/EHU)
{mikel.artetxe,gorka.labaka,e.agirre}@ehu.eus

Kyunghyun Cho
New York University
CIFAR Azrieli Global Scholar
kyunghyun.cho@nyu.edu

ABSTRACT

In spite of the recent success of neural machine translation (NMT) in standard
benchmarks, the lack of large parallel corpora poses a major practical problem
for many language pairs. There have been several proposals to alleviate this issue
with, for instance, triangulation and semi-supervised learning techniques, but they
still require a strong cross-lingual signal. In this work, we completely remove
the need of parallel data and propose a novel method to train an NMT system in
a completely unsupervised manner, relying on nothing but monolingual corpora.
Our model builds upon the recent work on unsupervised embedding mappings,
and consists of a slightly modiﬁed attentional encoder-decoder model that can be
trained on monolingual corpora alone using a combination of denoising and back-
translation. Despite the simplicity of the approach, our system obtains 15.56 and
10.21 BLEU points in WMT 2014 French → English and German → English
translation. The model can also proﬁt from small parallel corpora, and attains
21.81 and 15.24 points when combined with 100,000 parallel sentences, respec-
tively. Our implementation is released as an open source project1.

1

INTRODUCTION

Neural machine translation (NMT) has recently become the dominant paradigm to machine transla-
tion (Bahdanau et al., 2014; Sutskever et al., 2014). As opposed to the traditional statistical machine
translation (SMT), NMT systems are trained end-to-end, take advantage of continuous representa-
tions that greatly alleviate the sparsity problem, and make use of much larger contexts, thus mitigat-
ing the locality problem. Thanks to this, NMT has been reported to signiﬁcantly improve over SMT
both in automatic metrics and human evaluation (Wu et al., 2016).
Nevertheless, for the same reasons described above, NMT requires a large parallel corpus to be ef-
fective, and is known to fail when the training data is not big enough (Koehn & Knowles, 2017).
Unfortunately, the lack of large parallel corpora is a practical problem for the vast majority of lan-
guage pairs, including low-resource languages (e.g. Basque) as well as many combinations of major
languages (e.g. German-Russian). Several authors have recently tried to address this problem using
pivoting or triangulation techniques (Chen et al., 2017) as well as semi-supervised approaches (He
et al., 2016), but these methods still require a strong cross-lingual signal.
In this work, we eliminate the need of cross-lingual information and propose a novel method to train
NMT systems in a completely unsupervised manner, relying solely on monolingual corpora. Our
approach builds upon the recent work on unsupervised cross-lingual embeddings (Artetxe et al.,
2017; Zhang et al., 2017). Thanks to a shared encoder for both translation directions that uses
these ﬁxed cross-lingual embeddings, the entire system can be trained, with monolingual data, to
reconstruct its input. In order to learn useful structural information, noise in the form of random
token swaps is introduced in this input. In addition to denoising, we also incorporate backtranslation

1https://github.com/artetxem/undreamt

1

