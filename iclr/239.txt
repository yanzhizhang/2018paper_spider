Published as a conference paper at ICLR 2018

ASK THE RIGHT QUESTIONS:
ACTIVE QUESTION REFORMULATION WITH
REINFORCEMENT LEARNING

Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski,
Andrea Gesmundo, Neil Houlsby, Wei Wang
Google
{cbuck,jbulian,massi,wgaj,agesmundo,neilhoulsby,wangwe}@google.com

ABSTRACT

We frame Question Answering (QA) as a Reinforcement Learning task, an approach
that we call Active Question Answering. We propose an agent that sits between
the user and a black box QA system and learns to reformulate questions to elicit
the best possible answers. The agent probes the system with, potentially many,
natural language reformulations of an initial question and aggregates the returned
evidence to yield the best answer. The reformulation system is trained end-to-end to
maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset
of complex questions extracted from Jeopardy!. The agent outperforms a state-
of-the-art base model, playing the role of the environment, and other benchmarks.
We also analyze the language that the agent has learned while interacting with the
question answering system. We ﬁnd that successful question reformulations look
quite different from natural language paraphrases. The agent is able to discover
non-trivial reformulation strategies that resemble classic information retrieval
techniques such as term re-weighting (tf-idf) and stemming.

1

INTRODUCTION

Web and social media have become primary sources of information. Users’ expectations and
information seeking activities co-evolve with the increasing sophistication of these resources. Beyond
navigation, document retrieval, and simple factual question answering, users seek direct answers to
complex and compositional questions. Such search sessions may require multiple iterations, critical
assessment, and synthesis (Marchionini, 2006).
The productivity of natural language yields a myriad of ways to formulate a question (Chomsky,
1965). In the face of complex information needs, humans overcome uncertainty by reformulating
questions, issuing multiple searches, and aggregating responses. Inspired by humans’ ability to ask
the right questions, we present an agent that learns to carry out this process for the user. The agent sits
between the user and a backend QA system that we refer to as ‘the environment’. We call the agent
AQA, as it implements an active question answering strategy. AQA aims to maximize the chance of
getting the correct answer by sending a reformulated question to the environment. The agent seeks to
ﬁnd the best answer by asking many questions and aggregating the returned evidence. The internals
of the environment are not available to the agent, so it must learn to probe a black-box optimally using
only question strings. The key component of the AQA agent is a sequence-to-sequence model trained
with reinforcement learning (RL) using a reward based on the answer returned by the environment.
The second component to AQA combines the evidence from interacting with the environment using a
convolutional neural network to select an answer.
We evaluate on a dataset of Jeopardy! questions, SearchQA (Dunn et al., 2017). These questions
are hard to answer by design because they use convoluted language, e.g., Travel doesn’t seem to be
an issue for this sorcerer & onetime surgeon; astral projection & teleportation are no prob (answer:
Doctor Strange). Thus SearchQA tests the ability of AQA to reformulate questions such that the QA
system has the best chance of returning the correct answer. AQA improves over the performance
of a deep network built for QA, BiDAF (Seo et al., 2017a), which has produced state-of-the-art

1

