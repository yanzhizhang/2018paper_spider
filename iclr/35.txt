Published as a conference paper at ICLR 2018

TOWARDS NEURAL PHRASE-BASED MACHINE
TRANSLATION

Po-Sen Huang(cid:63), Chong Wang∗†, Sitao Huang∗‡, Dengyong Zhou∗†, Li Deng∗(cid:5)
(cid:63)Microsoft Research, †Google, ‡University of Illinois at Urbana-Champaign, (cid:5)Citadel
pshuang@microsoft.com, {chongw, dennyzhou}@google.com,
shuang91@illinois.edu, l.deng@ieee.org

ABSTRACT

In this paper, we present Neural Phrase-based Machine Translation (NPMT).1 Our
method explicitly models the phrase structures in output sequences using Sleep-
WAke Networks (SWAN), a recently proposed segmentation-based sequence mod-
eling method. To mitigate the monotonic alignment requirement of SWAN, we
introduce a new layer to perform (soft) local reordering of input sequences. Dif-
ferent from existing neural machine translation (NMT) approaches, NPMT does
not use attention-based decoding mechanisms. Instead, it directly outputs phrases
in a sequential order and can decode in linear time. Our experiments show that
NPMT achieves superior performances on IWSLT 2014 German-English/English-
German and IWSLT 2015 English-Vietnamese machine translation tasks com-
pared with strong NMT baselines. We also observe that our method produces
meaningful phrases in output languages.

1

INTRODUCTION

A word can be considered as a basic unit in languages. However, in many cases, we often need a
phrase to express a concrete meaning. For example, consider understanding the following sentence,
“machine learning is a field of computer science”.
It may become easier to comprehend if we
segment it as “[machine learning] [is] [a field of] [computer science]”, where the words in the
bracket ‘[]’ are regarded as “phrases”. These phrases have their own meanings, and can often be
reused in other contexts.
The goal of this paper is to explore the use of phrase structures aforementioned for neural network-
based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015). To this end,
we develop a neural machine translation method that explicitly models phrases in target language
sequences. Traditional phrase-based statistical machine translation (SMT) approaches have been
shown to consistently outperform word-based ones (Koehn et al., 2003; Koehn, 2009; Lopez, 2008).
However, modern neural machine translation (NMT) methods (Sutskever et al., 2014; Bahdanau
et al., 2015; Luong et al., 2015) do not have an explicit treatment on phrases, but they still work
surprisingly well and have been deployed to industrial systems (Zhou et al., 2016; Wu et al., 2016).
The proposed Neural Phrase-based Machine Translation (NPMT) method tries to explore the advan-
tages from both kingdoms. It builds upon Sleep-WAke Networks (SWAN), a segmentation-based
sequence modeling technique described in Wang et al. (2017a), where segments (or phrases) are
automatically discovered given the data. However, SWAN requires monotonic alignments between
inputs and outputs. This is often not an appropriate assumption in many language pairs. To mitigate
this issue, we introduce a new layer to perform (soft) local reordering on input sequences. Experi-
mental results show that NPMT outperforms attention-based NMT baselines in terms of the BLEU
score (Papineni et al., 2002) on IWSLT 2014 German-English/English-German and IWSLT 2015
English-Vietnamese translation tasks. We believe our method is one step towards the full integration
of the advantages from neural machine translation and phrase-based SMT.

∗Work performed while CW, DZ, and LD were at Microsoft Research and SH was interning at Microsoft

Research.

1The source codes are available at https://github.com/posenhuang/NPMT.

1

