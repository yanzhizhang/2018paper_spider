Published as a conference paper at ICLR 2018

LEARNING WASSERSTEIN EMBEDDINGS

Nicolas Courty∗
Université de Bretagne Sud
IRISA, UMR 6074, CNRS
ncourty@irisa.fr

Mélanie Ducoffe
Université Côte d’Azur
I3S, UMR 7271, CNRS
melanie.ducoffe@unice.fr

Rémi Flamary
Université Côte d’Azur, OCA
Lagrange, UMR 7293, CNRS
remi.flamary@unice.fr

ABSTRACT

The Wasserstein distance received a lot of attention recently in the community
of machine learning, especially for its principled way of comparing distributions.
It has found numerous applications in several hard problems, such as domain
adaptation, dimensionality reduction or generative models. However, its use is still
limited by a heavy computational cost. Our goal is to alleviate this problem by
providing an approximation mechanism that allows to break its inherent complexity.
It relies on the search of an embedding where the Euclidean distance mimics the
Wasserstein distance. We show that such an embedding can be found with a
siamese architecture associated with a decoder network that allows to move from
the embedding space back to the original input space. Once this embedding
has been found, computing optimization problems in the Wasserstein space (e.g.
barycenters, principal directions or even archetypes) can be conducted extremely
fast. Numerical experiments supporting this idea are conducted on image datasets,
and show the wide potential beneﬁts of our method.

1

INTRODUCTION

The Wasserstein distance is a powerful tool based on the theory of optimal transport to compare
data distributions with wide applications in image processing, computer vision and machine learn-
ing (Kolouri et al., 2017). In a context of machine learning, it has recently found numerous appli-
cations, e.g. domain adaptation (Courty et al., 2017), or word embedding (Huang et al., 2016). In
the context of deep learning, the Wasserstein appeared recently to be a powerful loss in generative
models (Arjovsky et al., 2017) and in multi-label classiﬁcation (Frogner et al., 2015). Its power comes
from two major reasons: i) it allows to operate on empirical data distributions in a non-parametric way
ii) the geometry of the underlying space can be leveraged to compare the distributions in a geometri-
cally sound way. The space of probability measures equipped with the Wasserstein distance can be
used to construct objects of interest such as barycenters (Agueh & Carlier, 2011) or geodesics (Seguy
& Cuturi, 2015) that can be used in data analysis and mining tasks.
More formally, let X be a metric space endowed with a metric dX. Let p ∈ (0,∞) and
Pp(X) the space of all Borel probability measures µ on X with ﬁnite moments of order p, i.e.
X dX (x, x0)pdµ(x) < ∞ for all x0 in X. The p-Wasserstein distance between µ and ν is deﬁned
as:

(cid:82)

(cid:18)

(cid:90)(cid:90)

(cid:19) 1

p

Wp(µ, ν) =

inf

π∈Π(µ,ν)

X×X

d(x, y)pdπ(x, y)

.

(1)

Here, Π(µ, ν) is the set of probabilistic couplings π on (µ, ν). As such, for every Borel subsets
A ⊆ X, we have that µ(A) = π(X × A) and ν(A) = π(A × X). It is well known that Wp deﬁnes a
metric over Pp(X) as long as p ≥ 1 (e.g. (Villani, 2009), Deﬁnition 6.2).

∗All three authors contributed equally

1

