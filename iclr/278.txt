Published as a conference paper at ICLR 2018

A COMPRESSED SENSING VIEW OF UNSUPERVISED
TEXT EMBEDDINGS, BAG-OF-n-GRAMS, AND LSTMS

Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi
Princeton University
{arora,mkhodak,nsaunshi}@cs.princeton.edu

Kiran Vodrahalli
Columbia University
kiran.vodrahalli@columbia.edu

ABSTRACT

Low-dimensional vector embeddings, computed using LSTMs or simpler tech-
niques, are a popular approach for capturing the “meaning” of text and a form of
unsupervised learning useful for downstream tasks. However, their power is not
theoretically understood. The current paper derives formal understanding by look-
ing at the subcase of linear embedding schemes. Using the theory of compressed
sensing we show that representations combining the constituent word vectors
are essentially information-preserving linear measurements of Bag-of-n-Grams
(BonG) representations of text. This leads to a new theoretical result about LSTMs:
low-dimensional embeddings derived from a low-memory LSTM are provably at
least as powerful on classiﬁcation tasks, up to small error, as a linear classiﬁer
over BonG vectors, a result that extensive empirical work has thus far been unable
to show. Our experiments support these theoretical ﬁndings and establish strong,
simple, and unsupervised baselines on standard benchmarks that in some cases are
state of the art among word-level methods. We also show a surprising new property
of embeddings such as GloVe and word2vec: they form a good sensing matrix for
text that is more efﬁcient than random matrices, the standard sparse recovery tool,
which may explain why they lead to better representations in practice.

1

INTRODUCTION

Much attention has been paid to using LSTMs (Hochreiter & Schmidhuber, 1997) and similar models
to compute text embeddings (Bengio et al., 2003; Collobert & Weston, 2008). Once trained, the
LSTM can sweep once or twice through a given piece of text, process it using only limited memory,
and output a vector with moderate dimensionality (a few hundred to a few thousand), which can be
used to measure text similarity via cosine similarity or as a featurization for downstream tasks.
The powers and limitations of this method have not been formally established. For example, can
such neural embeddings compete with and replace traditional linear classiﬁers trained on trivial
Bag-of-n-Grams (BonG) representations? Tweaked versions of BonG classiﬁers are known to be a
surprisingly powerful baseline (Wang & Manning, 2012) and have fast implementations (Joulin et al.,
2017). They continue to give better performance on many downstream supervised tasks such as IMDB
sentiment classiﬁcation (Maas et al., 2011) than purely unsupervised LSTM representations (Kiros
et al., 2015; Hill et al., 2016; Pagliardini et al., 2017). Even a very successful character-level (and thus
computation-intensive, taking a month of training) approach does not reach BonG performance on
datasets larger than IMDB (Radford et al., 2017). Meanwhile there is evidence suggesting that simpler
linear schemes give compact representations that provide most of the beneﬁts of word-level LSTM
embeddings (Wieting et al., 2016; Arora et al., 2017). These linear schemes consist of simply adding
up, with a few modiﬁcations, standard pretrained word embeddings such as GloVe or word2vec
(Mikolov et al., 2013; Pennington et al., 2014).

1

