Published as a conference paper at ICLR 2018

BACKPROPAGATION THROUGH THE VOID:
OPTIMIZING CONTROL VARIATES FOR
BLACK-BOX GRADIENT ESTIMATION

Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, David Duvenaud
University of Toronto and Vector Institute
{wgrathwohl, choidami, ywu, roeder, duvenaud}@cs.toronto.edu

ABSTRACT

Gradient-based optimization is the foundation of deep learning and reinforcement
learning, but is difﬁcult to apply when the mechanism being optimized is unknown
or not differentiable. We introduce a general framework for learning low-variance,
unbiased gradient estimators, applicable to black-box functions of discrete or
continuous random variables. Our method uses gradients of a surrogate neural
network to construct a control variate, which is optimized jointly with the original
parameters. We demonstrate this framework for training discrete latent-variable
models. We also give an unbiased, action-conditional extension of the advantage
actor-critic reinforcement learning algorithm.

1

INTRODUCTION

Gradient-based optimization has been key to most recent advances in machine learning and rein-
forcement learning. The back-propagation algorithm (Rumelhart & Hinton, 1986), also known as
reverse-mode automatic differentiation (Speelpenning, 1980; Rall, 1981) computes exact gradients
of deterministic, differentiable objective functions. The reparameterization trick (Williams, 1992;
Kingma & Welling, 2014; Rezende et al., 2014) allows backpropagation to give unbiased, low-
variance estimates of gradients of expectations of continuous random variables. This has allowed
effective stochastic optimization of large probabilistic latent-variable models.
Unfortunately, there are many objective functions relevant to the machine learning community for
which backpropagation cannot be applied. In reinforcement learning, for example, the function being
optimized is unknown to the agent and is treated as a black box (Schulman et al., 2015a). Similarly,
when ﬁtting probabilistic models with discrete latent variables, discrete sampling operations create
discontinuities giving the objective function zero gradient with respect to its parameters. Much recent
work has been devoted to constructing gradient estimators for these situations. In reinforcement
learning, advantage actor-critic methods (Sutton et al., 2000) give unbiased gradient estimates with
reduced variance obtained by jointly optimizing the policy parameters with an estimate of the value
function. In discrete latent-variable models, low-variance but biased gradient estimates can be given
by continuous relaxations of discrete variables (Maddison et al., 2016; Jang et al., 2016).
A recent advance by Tucker et al. (2017) used a continuous relaxation of discrete random variables to
build an unbiased and lower-variance gradient estimator, and showed how to tune the free parameters
of these relaxations to minimize the estimator’s variance during training. We generalize the method
of Tucker et al. (2017) to learn a free-form control variate parameterized by a neural network. This
gives a lower-variance, unbiased gradient estimator which can be applied to a wider variety of
problems. Most notably, our method is applicable even when no continuous relaxation is available, as
in reinforcement learning or black-box function optimization.

2 BACKGROUND: GRADIENT ESTIMATORS

How can we choose the parameters of a distribution to maximize an expectation? This problem
comes up in reinforcement learning, where we must choose the parameters θ of a policy distribu-
tion π(a|s, θ) to maximize the expected reward Eτ∼π [R] over state-action trajectories τ. It also

1

