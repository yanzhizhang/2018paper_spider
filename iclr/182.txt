Published as a conference paper at ICLR 2018

PIXELDEFEND: LEVERAGING GENERATIVE MODELS
TO UNDERSTAND AND DEFEND AGAINST
ADVERSARIAL EXAMPLES

Yang Song
Stanford University
yangsong@cs.stanford.edu

Sebastian Nowozin
Microsoft Research
nowozin@microsoft.com

Nate Kushman
Microsoft Research
nkushman@microsoft.com

Taesup Kim
Université de Montréal
taesup.kim@umontreal.ca

Stefano Ermon
Stanford University
ermon@cs.stanford.edu

ABSTRACT

Adversarial perturbations of normal images are usually imperceptible to humans,
but they can seriously confuse state-of-the-art machine learning models. What
makes them so special in the eyes of image classiﬁers? In this paper, we show em-
pirically that adversarial examples mainly lie in the low probability regions of the
training distribution, regardless of attack types and targeted models. Using statisti-
cal hypothesis testing, we ﬁnd that modern neural density models are surprisingly
good at detecting imperceptible image perturbations. Based on this discovery, we
devised PixelDefend, a new approach that puriﬁes a maliciously perturbed image
by moving it back towards the distribution seen in the training data. The puriﬁed
image is then run through an unmodiﬁed classiﬁer, making our method agnostic to
both the classiﬁer and the attacking method. As a result, PixelDefend can be used
to protect already deployed models and be combined with other model-speciﬁc
defenses. Experiments show that our method greatly improves resilience across
a wide variety of state-of-the-art attacking methods, increasing accuracy on the
strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for
CIFAR-10.

1

INTRODUCTION

Recent work has shown that small, carefully chosen modiﬁcations to the inputs of a neural net-
work classiﬁer can cause the model to give incorrect labels (Szegedy et al., 2013; Goodfellow et al.,
2014). This weakness of neural network models is particularly surprising because the modiﬁca-
tions required are often imperceptible, or barely perceptible, to humans. As deep neural networks
are being deployed in safety-critical applications such as self-driving cars (Amodei et al., 2016), it
becomes increasingly important to develop techniques to handle these kinds of inputs.

Rethinking adversarial examples The existence of such adversarial examples seems quite sur-
prising. A neural network classiﬁer can get super-human performance (He et al., 2015) on clean test
images, but will give embarrassingly wrong predictions on the same set of images if some impercep-
tible noise is added. What makes this noise so special to deep neural networks?
In this paper, we propose and empirically evaluate the following hypothesis: Even though they have
very small deviations from clean images, adversarial examples largely lie in the low probability
regions of the distribution that generated the data used to train the model. Therefore, they fool

1

