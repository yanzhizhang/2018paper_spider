Published as a conference paper at ICLR 2018

ON THE INFORMATION BOTTLENECK
THEORY OF DEEP LEARNING

Andrew M. Saxe, Yamini Bansal, Joel Dapello, Madhu Advani
Harvard University
{asaxe,madvani}@fas.harvard.edu,{ybansal,dapello}@g.harvard.edu

Artemy Kolchinsky, Brendan D. Tracey
Santa Fe Institute
{artemyk,tracey.brendan}@gmail.com

David D. Cox
Harvard University
MIT-IBM Watson AI Lab
davidcox@fas.harvard.edu
david.d.cox@ibm.com

ABSTRACT

The practical successes of deep neural networks have not been matched by theoret-
ical progress that satisfyingly explains their behavior. In this work, we study the
information bottleneck (IB) theory of deep learning, which makes three speciﬁc
claims: ﬁrst, that deep networks undergo two distinct phases consisting of an
initial ﬁtting phase and a subsequent compression phase; second, that the compres-
sion phase is causally related to the excellent generalization performance of deep
networks; and third, that the compression phase occurs due to the diffusion-like
behavior of stochastic gradient descent. Here we show that none of these claims
hold true in the general case. Through a combination of analytical results and
simulation, we demonstrate that the information plane trajectory is predominantly
a function of the neural nonlinearity employed: double-sided saturating nonlineari-
ties like tanh yield a compression phase as neural activations enter the saturation
regime, but linear activation functions and single-sided saturating nonlinearities
like the widely used ReLU in fact do not. Moreover, we ﬁnd that there is no evident
causal connection between compression and generalization: networks that do not
compress are still capable of generalization, and vice versa. Next, we show that
the compression phase, when it exists, does not arise from stochasticity in training
by demonstrating that we can replicate the IB ﬁndings using full batch gradient
descent rather than stochastic gradient descent. Finally, we show that when an
input domain consists of a subset of task-relevant and task-irrelevant information,
hidden representations do compress the task-irrelevant information, although the
overall information about the input may monotonically increase with training time,
and that this compression happens concurrently with the ﬁtting process rather than
during a subsequent compression period.

1

INTRODUCTION

Deep neural networks (Schmidhuber, 2015; LeCun et al., 2015) are the tool of choice for real-world
tasks ranging from visual object recognition (Krizhevsky et al., 2012), to unsupervised learning
(Goodfellow et al., 2014; Lotter et al., 2016) and reinforcement learning (Silver et al., 2016). These
practical successes have spawned many attempts to explain the performance of deep learning systems
(Kadmon & Sompolinsky, 2016), mostly in terms of the properties and dynamics of the optimization
problem in the space of weights (Saxe et al., 2014; Choromanska et al., 2015; Advani & Saxe, 2017),
or the classes of functions that can be efﬁciently represented by deep networks (Montufar et al.,
2014; Poggio et al., 2017). This paper analyzes a recent inventive proposal to study the dynamics of
learning through the lens of information theory (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby,
2017). In this view, deep learning is a question of representation learning: each layer of a deep neural
network can be seen as a set of summary statistics which contain some but not all of the information
present in the input, while retaining as much information about the target output as possible. The

1

