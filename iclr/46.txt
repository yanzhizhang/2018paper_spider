Published as a conference paper at ICLR 2018

THE IMPLICIT BIAS OF GRADIENT DESCENT ON SEPA-
RABLE DATA

Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson
Department of Electrical Engineering,Technion
Haifa, 320003, Israel
daniel.soudry@gmail.com
elad.hoffer@gmail.com
mor.shpigel@gmail.com

Nathan Srebro
Toyota Technological Institute at Chicago
Chicago, Illinois 60637, USA
nati@ttic.edu

ABSTRACT

We show that gradient descent on an unregularized logistic regression problem, for
almost all separable datasets, converges to the same direction as the max-margin
solution. The result generalizes also to other monotone decreasing loss functions
with an inﬁmum at inﬁnity, and we also discuss a multi-class generalizations to the
cross entropy loss. Furthermore, we show this convergence is very slow, and only
logarithmic in the convergence of the loss itself. This can help explain the beneﬁt
of continuing to optimize the logistic or cross-entropy loss even after the training
error is zero and the training loss is extremely small, and, as we show, even if the
validation loss increases. Our methodology can also aid in understanding implicit
regularization in more complex models and with other optimization methods.

1

INTRODUCTION

It is becoming increasingly clear that implicit biases introduced by the optimization algorithm play
a crucial role in deep learning and in the generalization ability of the learned models (Neyshabur
et al., 2014; 2015; Zhang et al., 2017; Keskar et al., 2017; Neyshabur et al., 2017; Wilson et al.,
2017). In particular, minimizing the training error, without any explicit regularization, over models
with more parameters and more capacity then the number of training examples, often yields good
generalization, despite the empirical optimization problem being highly underdetermined. That is,
there are many global minima of the training objective, most of which will not generalize well, but
the optimization algorithm (e.g. gradient descent) biases us toward a particular minimum that does
generalize well. Unfortunately, we still do not have a good understanding of the biases introduced by
different optimization algorithms in different situations.
We do have a decent understanding of the implicit regularization introduced by early stopping of
stochastic methods or, at an extreme, of one-pass (no repetition) stochastic optimization. However,
as discussed above, in deep learning we often beneﬁt from implicit bias even when optimizing the
(unregularized) training error to convergence, using stochastic or batch methods. For loss functions
with attainable, ﬁnite, minimizers, such as the squared loss, we have some understanding of this:
In particular, when minimizing an underdetermined least squares problem using gradient descent
starting from the origin, we know we will converge to the minimum Euclidean norm solution. But
the logistic loss, and its generalization the cross-entropy loss which is often used in deep learning, do
not admit a ﬁnite minimizer on separable problems. Instead, to drive the loss toward zero and thus
minimize it, the predictor must diverge toward inﬁnity.
Do we still beneﬁt from implicit regularization when minimizing the logistic loss on separable data?
Clearly the norm of the predictor itself is not minimized, since it grows to inﬁnity. However, for
prediction, only the direction of the predictor, i.e. the normalized w(t)/(cid:107)w(t)(cid:107), is important. How
does w(t)/(cid:107)w(t)(cid:107) behave as t → ∞ when we minimize the logistic (or similar) loss using gradient
descent on separable data, i.e., when it is possible to get zero misclassiﬁcation error and thus drive
the loss to zero?
In this paper, we show that even without any explicit regularization, for all most all datasets (except a
zero measure set), when minimizing linearly separable logistic regression problems using gradient

1

