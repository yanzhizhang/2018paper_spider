Published as a conference paper at ICLR 2018

STOCHASTIC VARIATIONAL VIDEO PREDICTION

Mohammad Babaeizadeh1, Chelsea Finn2, Dumitru Erhan3, Roy Campbell1, and Sergey Levine2,3

1University of Illinois at Urbana-Champaign

2University of California, Berkeley

3Google Brain

mb2@uiuc.edu, cbfinn@eecs.berkeley.edu, dumitru@google.com,

rhc@illinois.edu, svlevine@eecs.berkeley.edu

ABSTRACT

Predicting the future in real-world settings, particularly from raw sensory obser-
vations such as images, is exceptionally challenging. Real-world events can be
stochastic and unpredictable, and the high dimensionality and complexity of nat-
ural images require the predictive model to build an intricate understanding of
the natural world. Many existing methods tackle this problem by making simpli-
fying assumptions about the environment. One common assumption is that the
outcome is deterministic and there is only one plausible future. This can lead to
low-quality predictions in real-world settings with stochastic dynamics. In this
paper, we develop a stochastic variational video prediction (SV2P) method that
predicts a different possible future for each sample of its latent variables. To
the best of our knowledge, our model is the ﬁrst to provide effective stochastic
multi-frame prediction for real-world videos. We demonstrate the capability of
the proposed method in predicting detailed future frames of videos on multiple
real-world datasets, both action-free and action-conditioned. We ﬁnd that our pro-
posed method produces substantially improved video predictions when compared
to the same model without stochasticity, and to other stochastic video prediction
methods. Our SV2P implementation will be open sourced upon publication.

1

INTRODUCTION

Understanding the interaction dynamics of objects and predicting what happens next is one of the
key capabilities of humans which we heavily rely on to make decisions in everyday life (Bubic
et al., 2010). A model that can accurately predict future observations of complex sensory modalities
such as vision must internally represent the complex dynamics of real-world objects and people, and
therefore is more likely to acquire a representation that can be used for a variety of visual perception
tasks, such as object tracking and action recognition (Srivastava et al., 2015; Lotter et al., 2017;
Denton & Birodkar, 2017). Furthermore, such models can be inherently useful themselves, for
example, to allow an autonomous agent or robot to decide how to interact with the world to bring
about a desired outcome (Oh et al., 2015; Finn & Levine, 2017).
However, modeling future distributions over images is a challenging task, given the high dimension-
ality of the data and the complex dynamics of the environment. Hence, it is common to make various
simplifying assumptions. One particularly common assumption is that the environment is determin-
istic and that there is only one possible future (Chiappa et al., 2017; Srivastava et al., 2015; Boots
et al., 2014; Lotter et al., 2017). Models conditioned on the actions of an agent frequently make
this assumption, since the world is more deterministic in these settings (Oh et al., 2015; Finn et al.,
2016). However, most real-world prediction tasks, including the action-conditioned settings, are in
fact not deterministic, and a deterministic model can lose many of the nuances that are present in
real physical interactions. Given the stochastic nature of video prediction, any deterministic model is
obliged to predict a statistic of all the possible outcomes. For example, deterministic models trained
with a mean squared error loss function generate the expected value of all the possibilities for each
pixel independently, which is inherently blurry (Mathieu et al., 2016).

1

