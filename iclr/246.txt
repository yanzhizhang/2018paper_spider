Published as a conference paper at ICLR 2018

EVALUATING THE ROBUSTNESS OF NEURAL NET-
WORKS: AN EXTREME VALUE THEORY APPROACH

Tsui-Wei Weng1∗, Huan Zhang2∗, Pin-Yu Chen3, Jinfeng Yi4, Dong Su3, Yupeng Gao3,
Cho-Jui Hsieh2, Luca Daniel1
1Massachusetts Institute of Technology, Cambridge, MA 02139
2University of California, Davis, CA 95616
3IBM Research AI, Yorktown Heights, NY 10598
4Tencent AI Lab, Bellevue, WA 98004
twweng@mit.edu, ecezhang@ucdavis.edu,
pin-yu.chen@ibm.com, jinfengyi.ustc@gmail.com,
{dong.su,yupeng.gao}@ibm.com, chohsieh@ucdavis.edu, dluca@mit.edu

ABSTRACT

The robustness of neural networks to adversarial examples has received great at-
tention due to security implications. Despite various attack approaches to crafting
visually imperceptible adversarial examples, little has been developed towards a
comprehensive measure of robustness.
In this paper, we provide a theoretical
justiﬁcation for converting robustness analysis into a local Lipschitz constant es-
timation problem, and propose to use the Extreme Value Theory for efﬁcient eval-
uation. Our analysis yields a novel robustness metric called CLEVER, which is
short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed
CLEVER score is attack-agnostic and computationally feasible for large neural
networks. Experimental results on various networks, including ResNet, Inception-
v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indica-
tion measured by the (cid:96)2 and (cid:96)∞ norms of adversarial examples from powerful
attacks, and (ii) defended networks using defensive distillation or bounded ReLU
indeed achieve better CLEVER scores. To the best of our knowledge, CLEVER
is the ﬁrst attack-independent robustness metric that can be applied to any neural
network classiﬁer.

1

INTRODUCTION

Recent studies have highlighted the lack of robustness in state-of-the-art neural network models, e.g.,
a visually imperceptible adversarial image can be easily crafted to mislead a well-trained network
(Szegedy et al., 2013; Goodfellow et al., 2015; Chen et al., 2017a). Even worse, researchers have
identiﬁed that these adversarial examples are not only valid in the digital space but also plausible
in the physical world (Kurakin et al., 2016a; Evtimov et al., 2017). The vulnerability to adversarial
examples calls into question safety-critical applications and services deployed by neural networks,
including autonomous driving systems and malware detection protocols, among others.
In the literature, studying adversarial examples of neural networks has twofold purposes: (i) se-
curity implications: devising effective attack algorithms for crafting adversarial examples, and (ii)
robustness analysis: evaluating the intrinsic model robustness to adversarial perturbations to normal
examples. Although in principle the means of tackling these two problems are expected to be inde-
pendent, that is, the evaluation of a neural network’s intrinsic robustness should be agnostic to attack
methods, and vice versa, existing approaches extensively use different attack results as a measure
of robustness of a target neural network. Speciﬁcally, given a set of normal examples, the attack
success rate and distortion of the corresponding adversarial examples crafted from a particular at-
tack algorithm are treated as robustness metrics. Consequently, the network robustness is entangled
with the attack algorithms used for evaluation and the analysis is limited by the attack capabilities.
More importantly, the dependency between robustness evaluation and attack approaches can cause

∗Tsui-Wei Weng and Huan Zhang contributed equally

1

