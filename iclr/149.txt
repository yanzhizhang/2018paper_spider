Published as a conference paper at ICLR 2018

A SCALABLE LAPLACE APPROXIMATION
FOR NEURAL NETWORKS

Hippolyt Ritter1∗, Aleksandar Botev1, David Barber1 2
1University College London 2Alan Turing Institute

ABSTRACT

We leverage recent insights from second-order optimisation for neural networks
to construct a Kronecker factored Laplace approximation to the posterior over the
weights of a trained network. Our approximation requires no modiﬁcation of the
training procedure, enabling practitioners to estimate the uncertainty of their mod-
els currently used in production without having to retrain them. We extensively
compare our method to using Dropout and a diagonal Laplace approximation for
estimating the uncertainty of a network. We demonstrate that our Kronecker fac-
tored method leads to better uncertainty estimates on out-of-distribution data and
is more robust to simple adversarial attacks. Our approach only requires calcu-
lating two square curvature factor matrices for each layer. Their size is equal to
the respective square of the input and output size of the layer, making the method
efﬁcient both computationally and in terms of memory usage. We illustrate its
scalability by applying it to a state-of-the-art convolutional network architecture.

1

INTRODUCTION

Neural networks are most commonly trained in a maximum a posteriori (MAP) setting, which only
yields point estimates of the parameters, ignoring any uncertainty about them. This often leads to
overconﬁdent predictions, especially in regimes that are weakly covered by training data or far away
from the data manifold. While the conﬁdence of wrong predictions is usually irrelevant in a research
context, it is essential that a Machine Learning algorithm knows when it does not know in the real
world, as the consequences of mistakes can be fatal, be it when driving a car or diagnosing a disease.
The Bayesian framework of statistics provides a principled way for avoiding overconﬁdence in the
parameters by treating them as unknown quantities and integrating over all possible values. Speciﬁ-
cally, for the prediction of new data under a model, it ﬁts a posterior distribution over the parameters
given the training data and weighs the contribution of each setting of the parameters to the predic-
tion by the probability of the data under those parameters times their prior probability. However, the
posterior of neural networks is usually intractable due to their size and nonlinearity.
There has been previous interest in integrating neural networks into the Bayesian framework
(MacKay, 1992; Hinton & Van Camp, 1993; Neal, 1993; Barber & Bishop, 1998), however these ap-
proaches were designed for small networks by current standards. Recent adaptations to architectures
of modern scale rely on crude approximations of the posterior to become tractable. All of (Graves,
2011; Hern´andez-Lobato & Adams, 2015; Blundell et al., 2015) assume independence between the
individual weights. While they achieve good results on small datasets, this strong restriction of the
posterior is susceptible to underestimating the uncertainty, in particular when optimising the vari-
ational bound. The approach in (Gal & Ghahramani, 2016) requires the use of certain stochastic
regularisers which are not commonly present in most recent architectures. Furthermore, it is not
clear if the approximate posterior deﬁned by these regularisers is a good ﬁt to the true posterior.
Recent work on second-order optimisation of neural networks (Martens & Grosse, 2015; Botev
et al., 2017) has demonstrated that the diagonal blocks of the curvature can be well approximated
by a Kronecker product. We combine this insight with the idea of modelling the posterior over the
weights as a Gaussian, using a Laplace approximation (MacKay, 1992) with Kronecker factored
covariance matrices. This leads to a computationally efﬁcient matrix normal posterior distribution

∗Corresponding author: j.ritter@cs.ucl.ac.uk

1

