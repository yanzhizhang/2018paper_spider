Published as a conference paper at ICLR 2018

ESPRESSO: EFFICIENT FORWARD PROPAGATION FOR
BINARY DEEP NEURAL NETWORKS

Fabrizio Pedersoli
University of Victoria
fpeder@uvic.ca

George Tzanetakis
University of Victoria
gtzan@uvic.ca

Andrea Tagliasacchi
University of Victoria
ataiya@uvic.ca

ABSTRACT

There are many applications scenarios for which the computational performance
and memory footprint of the prediction phase of Deep Neural Networks (DNNs)
need to be optimized. Binary Deep Neural Networks (BDNNs) have been shown
to be an effective way of achieving this objective. In this paper, we show how
Convolutional Neural Networks (CNNs) can be implemented using binary repre-
sentations. Espresso is a compact, yet powerful library written in C/CUDA that
features all the functionalities required for the forward propagation of CNNs, in
a binary ﬁle less than 400KB, without any external dependencies. Although it
is mainly designed to take advantage of massive GPU parallelism, Espresso also
provides an equivalent CPU implementation for CNNs. Espresso provides spe-
cial convolutional and dense layers for BCNNs, leveraging bit-packing and bit-
wise computations for efﬁcient execution. These techniques provide a speed-up
of matrix-multiplication routines, and at the same time, reduce memory usage
when storing parameters and activations. We experimentally show that Espresso
is signiﬁcantly faster than existing implementations of optimized binary neural
networks (≈ 2 orders of magnitude). Espresso is released under the Apache 2.0
license and is available at http://github.com/fpeder/espresso.

1

INTRODUCTION

Convolutional Neural Networks have revolutionized computer vision, pushing the task of object
recognition beyond human capabilities (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014;
Szegedy et al., 2015). Deep Neural Networks (DNN), have also been successfully applied in other
ﬁelds, such as speech recognition (Graves et al., 2013; Hinton et al., 2012) and automated trans-
lation (Bahdanau et al., 2014; Sutskever et al., 2014). Despite achieving impressive classiﬁcation
accuracy results, DNNs require too much memory and power to be used effectively on embedded or
low-power devices. Many networks consume a considerable amount of memory. Memory remains
a very limited resource on mobile platforms making harder the usage of trained DNNs 1. Even
when memory is not an issue, DNNs remain very computationally intensive, and can quickly drain
the battery. Reducing the computational load does not only improve energy efﬁciency, but can also
enable further applications. For example, when processing real-time object classiﬁcation on mobile,
being able to perform faster predictions frees up computational resources that can be spent on tasks
such as speech recognition and analysis. Therefore, there is a substantial interest in reducing the
computational and memory requirements of DNNs.

Efﬁcient deep neural networks One way to achieve this target is to use specialized hardware for
DNNs. Another strategy is to reduce the network’s memory footprint and associated computation,
hence increasing its efﬁciency. Such solutions are preferable as they can be implemented in soft-
ware without requiring specialized hardware. In our research we follow the software approach, and
focus our attention to quantized networks. In this case, the parameters are stored as “small” integers

1for example, the popular AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan & Zisserman, 2014)

architectures consume respectively ≈ 250 MB and ≈ 520 MB

1

