Published as a conference paper at ICLR 2018

MAXIMUM A POSTERIORI POLICY OPTIMISATION

Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos,
Nicolas Heess, Martin Riedmiller
DeepMind, London, UK
{aabdolmaleki,springenberg,tassa,munos,heess,riedmiller}@google.com

ABSTRACT

We introduce a new algorithm for reinforcement learning called Maximum a-
posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-
entropy objective. We show that several existing methods can directly be related
to our derivation. We develop two off-policy algorithms and demonstrate that
they are competitive with the state-of-the-art in deep reinforcement learning. In
particular, for continuous control, our method outperforms existing methods with
respect to sample efﬁciency, premature convergence and robustness to hyperpa-
rameter settings.

1

INTRODUCTION

Model free reinforcement learning algorithms can acquire sophisticated behaviours by interacting
with the environment while receiving simple rewards. Recent experiments (Mnih et al., 2015; Jader-
berg et al., 2016; Heess et al., 2017) successfully combined these algorithms with powerful deep
neural-network approximators while beneﬁting from the increase of compute capacity.
Unfortunately, the generality and ﬂexibility of these algorithms comes at a price: They can require
a large number of samples and – especially in continuous action spaces – suffer from high gradi-
ent variance. Taken together these issues can lead to unstable learning and/or slow convergence.
Nonetheless, recent years have seen signiﬁcant progress, with improvements to different aspects of
learning algorithms including stability, data-efﬁciency and speed, enabling notable results on a vari-
ety of domains, including locomotion (Heess et al., 2017; Peng et al., 2016), multi-agent behaviour
(Bansal et al., 2017) and classical control (Duan et al., 2016).
Two types of algorithms currently dominate scalable learning for continuous control problems: First,
Trust-Region Policy Optimisation (TRPO; Schulman et al. 2015) and the derivative family of Proxi-
mal Policy Optimisation algorithms (PPO; Schulman et al. 2017b). These policy-gradient algorithms
are on-policy by design, reducing gradient variance through large batches and limiting the allowed
change in parameters. They are robust, applicable to high-dimensional problems, and require mod-
erate parameter tuning, making them a popular ﬁrst choice (Ho & Ermon, 2016). However, as
on-policy algorithms, they suffer from poor sample efﬁciency.
In contrast, off-policy value-gradient algorithms such as the Deep Deterministic Policy Gradient
(DDPG, Silver et al. 2014; Lillicrap et al. 2016), Stochastic Value Gradient (SVG, Heess et al.
2015), and the related Normalized Advantage Function formulation (NAF, Gu et al. 2016b) rely on
experience replay and learned (action-)value functions. These algorithms exhibit much better data
efﬁciency, approaching the regime where experiments with real robots are possible (Gu et al., 2016a;
Andrychowicz et al., 2017). While also popular, these algorithms can be difﬁcult to tune, especially
for high-dimensional domains like general robot manipulation tasks.
In this paper we propose a novel off-policy algorithm that beneﬁts from the best properties of both
classes. It exhibits the scalability, robustness and hyperparameter insensitivity of on-policy algo-
rithms, while offering the data-efﬁciency of off-policy, value-based methods.
To derive our algorithm, we take advantage of the duality between control and estimation by using
Expectation Maximisation (EM), a powerful tool from the probabilistic estimation toolbox, in order
to solve control problems. This duality can be understood as replacing the question “what are the
actions which maximise future rewards?” with the question “assuming future success in maximising

1

