Published as a conference paper at ICLR 2018

LEARNING ROBUST REWARDS WITH ADVERSARIAL
INVERSE REINFORCEMENT LEARNING

Justin Fu, Katie Luo, Sergey Levine
Department of Electrical Engineering and Computer Science
University of California, Berkeley
Berkeley, CA 94720, USA
justinjfu@eecs.berkeley.edu,katieluo@berkeley.edu,
svlevine@eecs.berkeley.edu

ABSTRACT

Reinforcement learning provides a powerful and general framework for decision
making and control, but its application in practice is often hindered by the need
for extensive feature and reward engineering. Deep reinforcement learning meth-
ods can remove the need for explicit engineering of policy or value features, but
still require a manually speciﬁed reward function. Inverse reinforcement learning
holds the promise of automatic reward acquisition, but has proven exceptionally
difﬁcult to apply to large, high-dimensional problems with unknown dynamics. In
this work, we propose AIRL, a practical and scalable inverse reinforcement learn-
ing algorithm based on an adversarial reward learning formulation. We demon-
strate that AIRL is able to recover reward functions that are robust to changes
in dynamics, enabling us to learn policies even under signiﬁcant variation in the
environment seen during training. Our experiments show that AIRL greatly out-
performs prior methods in these transfer settings.

1

INTRODUCTION

While reinforcement learning (RL) provides a powerful framework for automating decision making
and control, signiﬁcant engineering of elements such as features and reward functions has typically
been required for good practical performance. In recent years, deep reinforcement learning has al-
leviated the need for feature engineering for policies and value functions, and has shown promising
results on a range of complex tasks, from vision-based robotic control (Levine et al., 2016) to video
games such as Atari (Mnih et al., 2015) and Minecraft (Oh et al., 2016). However, reward engineer-
ing remains a signiﬁcant barrier to applying reinforcement learning in practice. In some domains,
this may be difﬁcult to specify (for example, encouraging “socially acceptable” behavior), and in
others, a na¨ıvely speciﬁed reward function can produce unintended behavior (Amodei et al., 2016).
Moreover, deep RL algorithms are often sensitive to factors such as reward sparsity and magnitude,
making well performing reward functions particularly difﬁcult to engineer.
Inverse reinforcement learning (IRL) (Russell, 1998; Ng & Russell, 2000) refers to the problem of
inferring an expert’s reward function from demonstrations, which is a potential method for solv-
ing the problem of reward engineering. However, inverse reinforcement learning methods have
generally been less efﬁcient than direct methods for learning from demonstration such as imitation
learning (Ho & Ermon, 2016), and methods using powerful function approximators such as neural
networks have required tricks such as domain-speciﬁc regularization and operate inefﬁciently over
whole trajectories (Finn et al., 2016b). There are many scenarios where IRL may be preferred over
direct imitation learning, such as re-optimizing a reward in novel environments (Finn et al., 2017) or
to infer an agent’s intentions, but IRL methods have not been shown to scale to the same complexity
of tasks as direct imitation learning. However, adversarial IRL methods (Finn et al., 2016b;a) hold
promise for tackling difﬁcult tasks due to the ability to adapt training samples to improve learning
efﬁciency.
Part of the challenge is that IRL is an ill-deﬁned problem, since there are many optimal policies
that can explain a set of demonstrations, and many rewards that can explain an optimal policy (Ng

1

