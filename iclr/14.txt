Published as a conference paper at ICLR 2018

A NEW METHOD OF REGION EMBEDDING FOR TEXT
CLASSIFICATION

Chao Qiao⇤‡, Bo Huang†‡, Guocheng Niu‡, Daren Li‡,
Daxiang Dong‡§, Wei He‡, Dianhai Yu‡§, Hua Wu‡§
‡ Baidu Inc., Beijing, China
§ National Engineering Laboratory of Deep Learning Technology and Application, China
{qiaochao, huangbo02, niuguocheng, lidaren,
daxiangdong, hewei06, yudianhai, wu hua}@baidu.com

ABSTRACT

To represent a text as a bag of properly identiﬁed “phrases” and use the represen-
tation for processing the text is proved to be useful. The key question here is how
to identify the phrases and represent them. The traditional method of utilizing
n-grams can be regarded as an approximation of the approach. Such a method can
suffer from data sparsity, however, particularly when the length of n-gram is large.
In this paper, we propose a new method of learning and utilizing task-speciﬁc dis-
tributed representations of n-grams, referred to as “region embeddings”. Without
loss of generality we address text classiﬁcation. We speciﬁcally propose two mod-
els for region embeddings. In our models, the representation of a word has two
parts, the embedding of the word itself, and a weighting matrix to interact with the
local context, referred to as local context unit. The region embeddings are learned
and used in the classiﬁcation task, as parameters of the neural network classiﬁer.
Experimental results show that our proposed method outperforms existing meth-
ods in text classiﬁcation on several benchmark datasets. The results also indicate
that our method can indeed capture the salient phrasal expressions in the texts.

1

INTRODUCTION

Text classiﬁcation is an important task for many applications, including topic categorization, search
query classiﬁcation, and sentiment analysis, which has been studied for years. A simple yet effec-
tive approach for text classiﬁcation is to represent documents as bag-of-words, and train a classiﬁer
on the basis of the representations using methods such as logistic regression, support vector ma-
chines (Joachims, 1998; Fan et al., 2008), and naive Bayes (McCallum et al., 1998). Although
bag-of-words methods are effective and efﬁcient, they also have limitations. The representations do
not take into account the word order information which has been proved to be useful at least in some
applications such as sentiment analysis (Pang et al., 2002).
To make effective use of word order information for text classiﬁcation, people have traditionally
exploited n-grams, i.e., short sequences of words in the texts. Previous work shows that the use of
n-grams is effective in the text classiﬁcation task (Pang et al., 2002; Wang & Manning, 2012; Joulin
et al., 2016). Although n-grams are very useful, they have certain limitations. 1) The number of
n-grams increases exponentially when the length of n-gram n increases. This makes it difﬁcult to
exploit large n-grams (e.g., n > 4). 2) Since the number of parameters in an n-gram model is very
large, the estimation of the parameters usually suffers from the data sparsity problem.
Recently, the method of FastText has been proposed (Joulin et al., 2016), which can learn and use
distributed embeddings of n-grams. More speciﬁcally, the embedding of an n-gram is deﬁned as a
low-dimensional vector representation of the n-gram. Note that the n-grams in a vocabulary can also
be represented as one-hot vectors Wang & Manning (2012).

⇤chao.qiao@outlook.com
†bohuang0321@gmail.com

1

