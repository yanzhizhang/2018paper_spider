Published as a conference paper at ICLR 2018

THE POWER OF DEEPER NETWORKS
FOR EXPRESSING NATURAL FUNCTIONS

David Rolnick, Max Tegmark
Massachusetts Institute of Technology
{drolnick, tegmark}@mit.edu

ABSTRACT

It is well-known that neural networks are universal approximators, but that deeper
networks tend in practice to be more powerful than shallower ones. We shed light
on this by proving that the total number of neurons m required to approximate nat-
ural classes of multivariate polynomials of n variables grows only linearly with n
for deep neural networks, but grows exponentially when merely a single hidden
layer is allowed. We also provide evidence that when the number of hidden layers
is increased from 1 to k, the neuron requirement grows exponentially not with n
but with n1/k, suggesting that the minimum number of layers required for practi-
cal expressibility grows only logarithmically with n.

1

INTRODUCTION

Deep learning has lately been shown to be a very powerful tool for a wide range of problems, from
image segmentation to machine translation. Despite its success, many of the techniques developed
by practitioners of artiﬁcial neural networks (ANNs) are heuristics without theoretical guarantees.
Perhaps most notably, the power of feedforward networks with many layers (deep networks) has not
been fully explained. The goal of this paper is to shed more light on this question and to suggest
heuristics for how deep is deep enough.
It is well-known (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989; Barron, 1994; Pinkus, 1999)
that neural networks with a single hidden layer can approximate any function under reasonable
assumptions, but it is possible that the networks required will be extremely large. Recent authors
have shown that some functions can be approximated by deeper networks much more efﬁciently
(i.e. with fewer neurons) than by shallower ones. Often, these results admit one or more of the
following limitations: “existence proofs” without explicit constructions of the functions in question;
explicit constructions, but relatively complicated functions; or applicability only to types of network
rarely used in practice.
It is important and timely to extend this work to make it more concrete and actionable, by deriving
resource requirements for approximating natural classes of functions using today’s most common
neural network architectures. Lin et al. (2017) recently proved that it is exponentially more efﬁcient
to use a deep network than a shallow network when Taylor-approximating the product of input
variables. In the present paper, we move far beyond this result in the following ways: (i) we use
standard uniform approximation instead of Taylor approximation, (ii) we show that the exponential
advantage of depth extends to all general sparse multivariate polynomials, and (iii) we address the
question of how the number of neurons scales with the number of layers. Our results apply to
standard feedforward neural networks and are borne out by empirical tests.
Our primary contributions are as follows:

• It is possible to achieve arbitrarily close approximations of simple multivariate and uni-
variate polynomials with neural networks having a bounded number of neurons (see §3).
• Such polynomials are exponentially easier to approximate with deep networks than

with shallow networks (see §4).

1

