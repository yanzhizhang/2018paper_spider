Published as a conference paper at ICLR 2018

DEEP GRADIENT COMPRESSION:
REDUCING THE COMMUNICATION BANDWIDTH FOR
DISTRIBUTED TRAINING

Yujun Lin ∗
Tsinghua University
Beijing National Research Center
for Information Science and Technology
linyy14@mails.tsinghua.edu.cn

Song Han †
Stanford University
Google Brain
songhan@stanford.edu

Huizi Mao
Stanford University
huizi@stanford.edu

William J. Dally
Stanford University
NVIDIA
dally@stanford.edu

Yu Wang
Tsinghua University
Beijing National Research Center
for Information Science and Technology
yu-wang@mail.tsinghua.edu.cn

ABSTRACT

Large-scale distributed training requires signiﬁcant communication bandwidth for
gradient exchange that limits the scalability of multi-node training, and requires
expensive high-bandwidth network infrastructure. The situation gets even worse
with distributed training on mobile devices (federated learning), which suffers
from higher latency, lower throughput, and intermittent poor connections. In this
paper, we ﬁnd 99.9% of the gradient exchange in distributed SGD are redundant,
and propose Deep Gradient Compression (DGC) to greatly reduce the communi-
cation bandwidth. To preserve accuracy during this compression, DGC employs
four methods: momentum correction, local gradient clipping, momentum factor
masking, and warm-up training. We have applied Deep Gradient Compression
to image classiﬁcation, speech recognition, and language modeling with multiple
datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus.
On these scenarios, Deep Gradient Compression achieves a gradient compression
ratio from 270× to 600× without losing accuracy, cutting the gradient size of
ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB.
Deep gradient compression enables large-scale distributed training on inexpensive
commodity 1Gbps Ethernet and facilitates distributed training on mobile.

1

INTRODUCTION

Large-scale distributed training improves the productivity of training deeper and larger models
(Chilimbi et al., 2014; Xing et al., 2015; Moritz et al., 2015; Zinkevich et al., 2010). Synchronous
stochastic gradient descent (SGD) is widely used for distributed training. By increasing the num-
ber of training nodes and taking advantage of data parallelism, the total computation time of the
forward-backward passes on the same size training data can be dramatically reduced. However,
gradient exchange is costly and dwarfs the savings of computation time (Li et al., 2014; Wen et al.,

∗Work done while at Stanford CVA lab.
†Joining MIT in 2018.

1

