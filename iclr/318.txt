Published as a conference paper at ICLR 2018

FRATERNAL DROPOUT

Konrad ˙Zołna1,2,∗, Devansh Arpit2, Dendi Suhubdy2 & Yoshua Bengio2,3

1Jagiellonian University
2MILA, Université de Montréal
3CIFAR Senior Fellow

ABSTRACT

Recurrent neural networks (RNNs) form an important class of architectures
among neural networks useful for language modeling and sequential prediction.
However, optimizing RNNs is known to be harder compared to feed-forward neu-
ral networks. A number of techniques have been proposed in literature to address
this problem. In this paper we propose a simple technique called fraternal dropout
that takes advantage of dropout to achieve this goal. Speciﬁcally, we propose to
train two identical copies of an RNN (that share parameters) with different dropout
masks while minimizing the difference between their (pre-softmax) predictions.
In this way our regularization encourages the representations of RNNs to be in-
variant to dropout mask, thus being robust. We show that our regularization term is
upper bounded by the expectation-linear dropout objective which has been shown
to address the gap due to the difference between the train and inference phases of
dropout. We evaluate our model and achieve state-of-the-art results in sequence
modeling tasks on two benchmark datasets – Penn Treebank and Wikitext-2. We
also show that our approach leads to performance improvement by a signiﬁcant
margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10)
tasks.

1

INTRODUCTION

Recurrent neural networks (RNNs) like long short-term memory (LSTM; Hochreiter & Schmidhu-
ber (1997)) networks and gated recurrent unit (GRU; Chung et al. (2014)) are popular architectures
for sequence modeling tasks like language generation, translation, speech synthesis, and machine
comprehension. However, they are harder to optimize compared to feed-forward networks due to
challenges like variable length input sequences, repeated application of the same transition operator
at each time step, and largely-dense embedding matrix that depends on the vocabulary size. Due to
these optimization challenges in RNNs, the application of batch normalization and its variants (layer
normalization, recurrent batch normalization, recurrent normalization propagation) have not been as
successful as their counterparts in feed-forward networks (Laurent et al., 2016), although they do
considerably provide performance gains. Similarly, naive application of dropout (Srivastava et al.,
2014) has been shown to be ineffective in RNNs (Zaremba et al., 2014). Therefore, regularization
techniques for RNNs is an active area of research.
To address these challenges, Zaremba et al. (2014) proposed to apply dropout only to the non-
recurrent connections in multi-layer RNNs. Variational dropout (Gal & Ghahramani (2016)) uses
the same dropout mask throughout a sequence during training. DropConnect (Wan et al., 2013)
applies the dropout operation on the weight matrices. Zoneout (Krueger et al. (2016)), in a similar
spirit with dropout, randomly chooses to use the previous time step hidden state instead of using
the current one. Similarly as a substitute for batch normalization, layer normalization normalizes
the hidden units within each sample to have zero mean and unit standard deviation. Recurrent batch
normalization applies batch normalization but with unshared mini-batch statistics for each time step
(Cooijmans et al., 2016).

∗konrad.zolna@gmail.com

1

