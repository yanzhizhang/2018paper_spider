Published as a conference paper at ICLR 2018

NON-AUTOREGRESSIVE
NEURAL MACHINE TRANSLATION

Jiatao Gu†∗, James Bradbury‡, Caiming Xiong‡, Victor O.K. Li†& Richard Socher‡
‡Salesforce Research
{james.bradbury,cxiong,rsocher}@salesforce.com
†The University of Hong Kong
{jiataogu, vli}@eee.hku.hk

ABSTRACT

Existing approaches to neural machine translation condition each output word on
previously generated outputs. We introduce a model that avoids this autoregressive
property and produces its outputs in parallel, allowing an order of magnitude lower
latency during inference. Through knowledge distillation, the use of input token
fertilities as a latent variable, and policy gradient ﬁne-tuning, we achieve this at
a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer
network used as a teacher. We demonstrate substantial cumulative improvements
associated with each of the three aspects of our training strategy, and validate
our approach on IWSLT 2016 English–German and two WMT language pairs.
By sampling fertilities in parallel at inference time, our non-autoregressive model
achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–
Romanian.

1

INTRODUCTION

Neural network based models outperform traditional statistical models for machine translation (MT)
(Bahdanau et al., 2015; Luong et al., 2015). However, state-of-the-art neural models are much
slower than statistical MT approaches at inference time (Wu et al., 2016). Both model families use
autoregressive decoders that operate one step at a time: they generate each token conditioned on
the sequence of tokens previously generated. This process is not parallelizable, and, in the case of
neural MT models, it is particularly slow because a computationally intensive neural network is used
to generate each token.
While several recently proposed models avoid recurrence at train time by leveraging convolu-
tions (Kalchbrenner et al., 2016; Gehring et al., 2017; Kaiser et al., 2017) or self-attention (Vaswani
et al., 2017) as more-parallelizable alternatives to recurrent neural networks (RNNs), use of autore-
gressive decoding makes it impossible to take full advantage of parallelism during inference.
We introduce a non-autoregressive translation model based on the Transformer network (Vaswani
et al., 2017). We modify the encoder of the original Transformer network by adding a module that
predicts fertilities, sequences of numbers that form an important component of many traditional
machine translation models (Brown et al., 1993). These fertilities are supervised during training
and provide the decoder at inference time with a globally consistent plan on which to condition its
simultaneously computed outputs.

2 BACKGROUND

2.1 AUTOREGRESSIVE NEURAL MACHINE TRANSLATION
Given a source sentence X = {x1, ..., xT (cid:48)}, a neural machine translation model factors the distribu-
tion over possible output sentences Y = {y1, ..., yT} into a chain of conditional probabilities with a

∗This work was completed while the ﬁrst author was interning at Salesforce Research.

1

