Published as a conference paper at ICLR 2018

ON THE EXPRESSIVE POWER OF OVERLAPPING
ARCHITECTURES OF DEEP LEARNING

Or Sharir & Amnon Shashua
The Hebrew University of Jerusalem
{or.sharir,shashua}@cs.huji.ac.il

ABSTRACT

Expressive efﬁciency refers to the relation between two architectures A and B,
whereby any function realized by B could be replicated by A, but there exists
functions realized by A, which cannot be replicated by B unless its size grows sig-
niﬁcantly larger. For example, it is known that deep networks are exponentially
efﬁcient with respect to shallow networks, in the sense that a shallow network
must grow exponentially large in order to approximate the functions represented
by a deep network of polynomial size. In this work, we extend the study of ex-
pressive efﬁciency to the attribute of network connectivity and in particular to the
effect of ”overlaps” in the convolutional process, i.e., when the stride of the con-
volution is smaller than its ﬁlter size (receptive ﬁeld). To theoretically analyze
this aspect of network’s design, we focus on a well-established surrogate for Con-
vNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate
empirically that our results hold for standard ConvNets as well. Speciﬁcally, our
analysis shows that having overlapping local receptive ﬁelds, and more broadly
denser connectivity, results in an exponential increase in the expressive capacity of
neural networks. Moreover, while denser connectivity can increase the expressive
capacity, we show that the most common types of modern architectures already
exhibit exponential increase in expressivity, without relying on fully-connected
layers.

1

INTRODUCTION

One of the most fundamental attributes of deep networks, and the reason for driving its empirical
success, is the “Depth Efﬁciency” result which states that deeper models are exponentially more
expressive than shallower models of similar size. Formal studies of Depth Efﬁciency include the
early work on boolean or thresholded circuits (Sipser, 1983; Yao, 1989; H˚astad and Goldmann,
1991; Hajnal et al., 1993), and the more recent studies covering the types of networks used in
practice (Pascanu et al., 2013; Mont´ufar et al., 2014; Eldan and Shamir, 2016; Cohen et al., 2016a;
Cohen and Shashua, 2016; Telgarsky, 2016; Safran and Shamir, 2016; Raghu et al., 2016; Poole
et al., 2016). What makes the Depth Efﬁciency attribute so desirable, is that it brings exponential
increase in expressive power through merely a polynomial change in the model, i.e. the addition
of more layers. Nevertheless, depth is merely one among many architectural attributes that deﬁne
modern networks. The deep networks used in practice consist of architectural features deﬁned by
various schemes of connectivity, convolution ﬁlter deﬁned by size and stride, pooling geometry and
activation functions. Whether or not those relate to expressive efﬁciency, as depth has proven to be,
remains an open question.
In order to study the effect of network design on expressive efﬁciency we should ﬁrst deﬁne ”efﬁ-
ciency” in broader terms. Given two network architectures A and B, we say that architecture A is
expressively efﬁcient with respect to architecture B, if the following two conditions hold: (i) any
function h realized by B of size rB can be realized (or approximated) by A with size rA ∈ O(rB);
(ii) there exist a function h realized by A with size rA, that cannot be realized (or approximated)
by B, unless rB ∈ Ω(f (rA)) for some super-linear function f. The exact deﬁnition of the sizes rA
and rB depends on the measurement we care about, e.g. the number of parameters, or the number
of “neurons”. The nature of the function f in condition (ii) determines the type of efﬁciency taking
place – if f is exponential then architecture A is said to be exponentially efﬁcient with respect to

1

