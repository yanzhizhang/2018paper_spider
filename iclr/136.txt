Published as a conference paper at ICLR 2018

NEUMANN OPTIMIZER: A PRACTICAL OPTIMIZATION
ALGORITHM FOR DEEP NEURAL NETWORKS

Shankar Krishnan & Ying Xiao & Rif A. Saurous
Machine Perception, Google Research
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
{skrishnan,yingxiao,rif}@google.com

ABSTRACT

Progress in deep learning is slowed by the days or weeks it takes to train large
models. The natural solution of using more hardware is limited by diminishing re-
turns, and leads to inefﬁcient use of additional resources. In this paper, we present
a large batch, stochastic optimization algorithm that is both faster than widely
used algorithms for ﬁxed amounts of computation, and also scales up substan-
tially better as more computational resources become available. Our algorithm
implicitly computes the inverse Hessian of each mini-batch to produce descent
directions; we do so without either an explicit approximation to the Hessian or
Hessian-vector products. We demonstrate the effectiveness of our algorithm by
successfully training large ImageNet models (Inception-V3, Resnet-50, Resnet-
101 and Inception-Resnet-V2) with mini-batch sizes of up to 32000 with no loss
in validation error relative to current baselines, and no increase in the total num-
ber of steps. At smaller mini-batch sizes, our optimizer improves the validation
error in these models by 0.8-0.9%. Alternatively, we can trade off this accuracy
to reduce the number of training steps needed by roughly 10-30%. Our work is
practical and easily usable by others – only one hyperparameter (learning rate)
needs tuning, and furthermore, the algorithm is as computationally cheap as the
commonly used Adam optimizer.

1

INTRODUCTION

Large deep neural networks trained on massive data sets have led to major advances in machine
learning performance (LeCun et al. (2015)). Current practice is to train networks using gradient
descent (SGD) and momentum optimizers, along with natural-gradient-like methods (Hinton et al.
(2012); Zeiler (2012); Duchi et al. (2011); Kingma & Ba (2015)). As distributed computation avail-
ability increases, total wall-time to train large models has become a substantial bottleneck, and
approaches that decrease total wall-time without sacriﬁcing model generalization are very valuable.
In the simplest version of mini-batch SGD, one computes the average gradient of the loss over a
small set of examples, and takes a step in the direction of the negative gradient. It is well known
that the convergence of the original SGD algorithm (Robbins & Monro (1951)) has two terms, one
of which depends on the variance of the gradient estimate. In practice, decreasing the variance by
increasing the batch size suffers from diminishing returns, often resulting in speedups that are sub-
linear in batch size, and even worse, in degraded generalization performance (Keskar et al. (2017)).
Some recent work (Goyal et al. (2017); You et al. (2017a;b)) suggests that by carefully tuning learn-
ing rates and other hyperparameter schedules, it is possible to train architectures like ResNets and
AlexNet on Imagenet with large mini-batches of up to 8192 with no loss of accuracy, shortening
training time to hours instead of days or weeks.
There have been many attempts to incorporate second-order Hessian information into stochastic op-
timizers (see related work below). Such algorithms either explicitly approximate the Hessian (or its
inverse), or exploit the use of Hessian-vector products. Unfortunately, the additional computational
cost and implementation complexity often outweigh the beneﬁt of improved descent directions. Con-

1

