Published as a conference paper at ICLR 2018

LOSS-AWARE WEIGHT QUANTIZATION OF DEEP NET-
WORKS

Lu Hou, James T. Kwok
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Hong Kong
{lhouab, jamesk}@cse.ust.hk

ABSTRACT

The huge size of deep networks hinders their use in small computing devices. In
this paper, we consider compressing the network by weight quantization. We ex-
tend a recently proposed loss-aware weight binarization scheme to ternarization,
with possibly different scaling parameters for the positive and negative weights,
and m-bit (where m > 2) quantization. Experiments on feedforward and recur-
rent neural networks show that the proposed scheme outperforms state-of-the-art
weight quantization algorithms, and is as accurate (or even more accurate) than
the full-precision network.

1

INTRODUCTION

The last decade has witnessed huge success of deep neural networks in various domains. Examples
include computer vision, speech recognition, and natural language processing (LeCun et al., 2015).
However, their huge size often hinders deployment to small computing devices such as cell phones
and the internet of things. Many attempts have been recently made to reduce the model size. One
common approach is to prune a trained dense network (Han et al., 2015; 2016). However, most of
the pruned weights may come from the fully-connected layers where computations are cheap, and
the resultant time reduction is insigniﬁcant. Li et al. (2017b) and Molchanov et al. (2017) proposed
to prune ﬁlters in the convolutional neural networks based on their magnitudes or signiﬁcance to the
loss. However, the pruned network has to be retrained, which is again expensive.
Another direction is to use more compact models. GoogleNet (Szegedy et al., 2015) and ResNet (He
et al., 2016) replace the fully-connected layers with simpler global average pooling. However, they
are also deeper. SqueezeNet (Iandola et al., 2016) reduces the model size by replacing most of the
3 × 3 ﬁlters with 1 × 1 ﬁlters. This is less efﬁcient on smaller networks because the dense 1 × 1
convolutions are costly. MobileNet (Howard et al., 2017) compresses the model using separable
depth-wise convolution. ShufﬂeNet (Zhang et al., 2017) utilizes pointwise group convolution and
channel shufﬂe to reduce the computation cost while maintaining accuracy. However, highly opti-
mized group convolution and depth-wise convolution implementations are required. Alternatively,
Novikov et al. (2015) compressed the model by using a compact multilinear format to represent the
dense weight matrix. The CP and Tucker decompositions have also been used on the kernel tensor
in CNNs (Lebedev et al., 2014; Kim et al., 2016). However, they often need expensive ﬁne-tuning.
Another effective approach to compress the network and accelerate training is by quantizing each
full-precision weight to a small number of bits. This can be further divided to two sub-categories,
depending on whether pre-trained models are used (Lin et al., 2016a; Mellempudi et al., 2017) or
the quantized model is trained from scratch (Courbariaux et al., 2015; Li et al., 2017a). Some of
these also directly learn with low-precision weights, but they usually suffer from severe accuracy
deterioration (Li et al., 2017a; Miyashita et al., 2016). By keeping the full-precision weights during
learning, Courbariaux et al. (2015) pioneered the BinaryConnect algorithm, which uses only one bit
for each weight while still achieving state-of-the-art classiﬁcation results. Rastegari et al. (2016)
further incorporated weight scaling, and obtained better results. Instead of simply ﬁnding the clos-
est binary approximation of the full-precision weights, a loss-aware scheme is proposed in (Hou
et al., 2017). Beyond binarization, TernaryConnect (Lin et al., 2016b) quantizes each weight to

1

