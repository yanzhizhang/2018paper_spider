Published as a conference paper at ICLR 2018

IMPROVING THE IMPROVED TRAINING OF
WASSERSTEIN GANS:
A CONSISTENCY TERM AND ITS DUAL EFFECT

Xiang Wei1,2∗, Boqing Gong3∗, Zixia Liu1, Wei Lu2, Liqiang Wang1
1Department of Computer Science, University of Central Florida, Orlando, FL, USA 32816
2School of Software Engineering, Beijing Jiaotong University, Beijing, China 100044
3Tencent AI Lab, Bellevue, WA, USA 98004
yqweixiang@knights.ucf.edu, boqinggo@outlook.com
zixia@knights.ucf.edu, luwei@bjtu.edu.cn, lwang@cs.ucf.edu

ABSTRACT

Despite being impactful on a variety of problems and applications, the generative
adversarial nets (GANs) are remarkably difﬁcult to train. This issue is formally an-
alyzed by Arjovsky & Bottou (2017), who also propose an alternative direction to
avoid the caveats in the minmax two-player training of GANs. The corresponding
algorithm, called Wasserstein GAN (WGAN), hinges on the 1-Lipschitz continu-
ity of the discriminator. In this paper, we propose a novel approach to enforcing
the Lipschitz continuity in the training procedure of WGANs. Our approach seam-
lessly connects WGAN with one of the recent semi-supervised learning methods.
As a result, it gives rise to not only better photo-realistic samples than the pre-
vious methods but also state-of-the-art semi-supervised learning results. In par-
ticular, our approach gives rise to the inception score of more than 5.0 with only
1,000 CIFAR-10 images and is the ﬁrst that exceeds the accuracy of 90% on the
CIFAR-10 dataset using only 4,000 labeled images, to the best of our knowledge.

1

INTRODUCTION

We have witnessed a great surge of interests in deep generative networks in recent years (Kingma &
Welling, 2013; Goodfellow et al., 2014; Li et al., 2015). The central idea therein is to feed a random
vector to a (e.g., feedforward) neural network and then take the output as the desired sample. This
sampling procedure is very efﬁcient without the need of any Markov chains.
In order to train such a deep generative network, two broad categories of methods are proposed.
The ﬁrst is to use stochastic variational inference (Kingma & Welling, 2013; Rezende et al., 2014;
Kingma et al., 2014) to optimize the lower bound of the data likelihood. The other is to use the
samples as a proxy to minimize the distribution divergence between the model and the real through
a two-player game (Goodfellow et al., 2014; Salimans et al., 2016), maximum mean discrepancy (Li
et al., 2015; Dziugaite et al., 2015; Li et al., 2017b), f-divergence (Nowozin et al., 2016; Nock et al.,
2017), and the most recent Wasserstein distance (Arjovsky et al., 2017; Gulrajani et al., 2017).
With no doubt, the generative adversarial networks (GANs) among them (Goodfellow et al., 2014)
have the biggest impact thus far on a variety of problems and applications (Radford et al., 2015;
Denton et al., 2015; Im et al., 2016; Isola et al., 2016; Springenberg, 2015; Sutskever et al., 2015;
Odena, 2016; Zhu et al., 2017). GANs learn the generative network (generator) by playing a two-
player game between the generator and an auxiliary discriminator network. While the generator has
no difference from other deep generative models in the sense that it translates a random vector into a
desired sample, it is impossible to calculate the sample likelihood from it. Instead, the discriminator
serves to evaluate the quality of the generated samples by checking how difﬁcult it is to differentiate
them from real data points.

∗Equal contribution.

1

