Published as a conference paper at ICLR 2018

NOT-SO-RANDOM FEATURES

Yi Zhang

Brian Bullins
Cyril Zhang
Department of Computer Science
Princeton University
Princeton, NJ 08544, USA
{bbullins, cyril.zhang, y.zhang}@cs.princeton.edu

ABSTRACT

We propose a principled method for kernel learning, which relies on a Fourier-
analytic characterization of translation-invariant or rotation-invariant kernels. Our
method produces a sequence of feature maps, iteratively reﬁning the SVM margin.
We provide rigorous guarantees for optimality and generalization, interpreting our
algorithm as online equilibrium-ﬁnding dynamics in a certain two-player min-max
game. Evaluations on synthetic and real-world datasets demonstrate scalability
and consistent improvements over related random features-based methods.

1

INTRODUCTION

Choosing the right kernel is a classic question that has riddled machine learning practitioners and
theorists alike. Conventional wisdom instructs the user to select a kernel which captures the structure
and geometric invariances in the data. Efforts to formulate this principle have inspired vibrant areas
of study, going by names from feature selection to multiple kernel learning (MKL).
We present a new, principled approach for selecting a translation-invariant or rotation-invariant
kernel to maximize the SVM classiﬁcation margin. We ﬁrst describe a kernel-alignment subroutine,
which ﬁnds a peak in the Fourier transform of an adversarially chosen data-dependent measure.
Then, we deﬁne an iterative procedure that produces a sequence of feature maps, progressively
improving the margin. The resulting algorithm is strikingly simple and scalable.
Intriguingly, our analysis interprets the main algorithm as no-regret learning dynamics in a zero-sum
min-max game, whose value is the classiﬁcation margin. Thus, we are able to quantify convergence
guarantees towards the largest margin realizable by a kernel with the assumed invariance. Finally, we
exhibit experiments on synthetic and benchmark datasets, demonstrating consistent improvements
over related random features-based kernel methods.

1.1 RELATED WORK

There is a vast literature on MKL, from which we use the key concept of kernel alignment (Cris-
tianini et al., 2002). Otherwise, our work bears few similarities to traditional MKL; this and much
related work (e.g. Cortes et al. (2012); G¨onen & Alpaydın (2011); Lanckriet et al. (2004)) are con-
cerned with selecting a kernel by combining a collection of base kernels, chosen beforehand. Our
method allows for greater expressivity and even better generalization guarantees.
Instead, we take inspiration from the method of random features (Rahimi & Recht, 2007). In this
pioneering work, originally motivated by scalability, feature maps are sampled according to the
Fourier transform of a chosen kernel. The idea of optimizing a kernel in random feature space was
studied by Sinha & Duchi (2016). In this work, which is most similar to ours, kernel alignment is
optimized via importance sampling on a ﬁxed, ﬁnitely supported proposal measure. However, the
proposal can fail to contain informative features, especially in high dimension; indeed, they highlight
efﬁciency, rather than showing performance improvements over RBF features.
Learning a kernel in the Fourier domain (without the primal feature maps) has also been consid-
ered previously: Oliva et al. (2016) and Yang et al. (2015b) model the Fourier spectrum parametri-
cally, which limits expressivity; the former also require complicated posterior inference procedures.

1

