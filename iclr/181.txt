Published as a conference paper at ICLR 2018

SPARSE PERSISTENT RNNS:
SQUEEZING LARGE RECURRENT NETWORKS ON-
CHIP

Feiwen Zhu∗, Jeff Pool∗, Michael Andersch, Jeremy Appleyard & Fung Xie
NVIDIA
{mzhu,jpool,mandersch,jappleyard,ftse}@nvidia.com

ABSTRACT

Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based
problems, but their efﬁcacy and execution time are dependent on the size of the
network. Following recent work in simplifying these networks with model pruning
and a novel mapping of work onto GPUs, we design an efﬁcient implementation
for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport
timestamps, wide memory loads, and a bank-aware weight layout. With these
optimizations, we achieve speedups of over 6× over the next best algorithm for
a hidden layer of size 2304, batch size of 4, and a density of 30%. Further, our
technique allows for models of over 5× the size to ﬁt on a GPU for a speedup of
2×, enabling larger networks to help advance the state-of-the-art. We perform case
studies on NMT and speech recognition tasks in the appendix, accelerating their
recurrent layers by up to 3×.

1

INTRODUCTION

Many sequence-based problems, including Speech Recognition (Amodei et al., 2015) and Neural
Machine Translation (NMT) (Bahdanau et al., 2014), can be solved effectively with Recurrent
Neural Networks (RNNs). Appleyard et al. (2016) showed that these networks can run efﬁciently
on massively parallel processors such as GPUs, and Diamos et al. (2016) found that if the network
is small enough to ﬁt in the register ﬁle of a GPU, a persistent approach can be used to increase
performance.
In parallel, many network compression methods (Han et al., 2016c; 2015; Guo et al., 2016) have
been shown to reduce the model size of both Convolutional Neural Networks (CNNs) and RNNs.
Recent work in this area has found that model pruning, in particular, can lead to signiﬁcant reductions
in the number of important network parameters for RNNs (Narang et al., 2017; See et al., 2016)
We present an approach that combines both of these techniques into an efﬁcient and expandable
approach. In particular, our work makes the following contributions:
• Larger sparse RNNs can be run more efﬁciently on GPUs.
• Sparse RNNs can be run with smaller batch sizes more efﬁciently on GPUs.
• Various optimizations on top of the naïve implementation, necessary to achieve high perfor-

mance.

• Case studies using our technique showing 1) generalization to LSTMs, 2) practical network

design considerations, and 3) speedups of up to 3× on two non-synthetic workloads.

A naïve implementation of the idea, presented in Section 3, leads to limited beneﬁt; we present
a series of optimizations in Section 4 that help to achieve a high level of performance. Section 5
describes the experimental setup and results, and we discuss future work and our conclusions in
Sections 6 and 7. The appendix presents a case study on a machine translation task.

∗Indicates equal contribution

1

