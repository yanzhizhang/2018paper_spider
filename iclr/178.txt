Published as a conference paper at ICLR 2018

COMBINING SYMBOLIC EXPRESSIONS AND BLACK-
BOX FUNCTION EVALUATIONS IN NEURAL PROGRAMS

Forough Arabshahi
University of California
Irvine, CA
farabsha@uci.edu

Sameer Singh
University of California
Irvine, CA
sameer@uci.edu

Animashree Anandkumar
California Institute of Technology
Pasadena, CA
anima@caltech.edu

ABSTRACT

Neural programming involves training neural networks to learn programs, math-
ematics, or logic from data. Previous works have failed to achieve good general-
ization performance, especially on problems and programs with high complexity
or on large domains. This is because they mostly rely either on black-box func-
tion evaluations that do not capture the structure of the program, or on detailed
execution traces that are expensive to obtain, and hence the training data has poor
coverage of the domain under consideration. We present a novel framework that
utilizes black-box function evaluations, in conjunction with symbolic expressions
that deﬁne relationships between the given functions. We employ tree LSTMs to
incorporate the structure of the symbolic expression trees. We use tree encoding for
numbers present in function evaluation data, based on their decimal representation.
We present an evaluation benchmark for this task to demonstrate our proposed
model combines symbolic reasoning and function evaluation in a fruitful manner,
obtaining high accuracies in our experiments. Our framework generalizes signiﬁ-
cantly better to expressions of higher depth and is able to ﬁll partial equations with
valid completions.

1

INTRODUCTION

Human beings possess impressive abilities for abstract mathematical and logical thinking. It has
long been the dream of computer scientists to design machines with such capabilities: machines that
can automatically learn and reason, thereby removing the need to manually program them. Neural
programming, where neural networks are used to learn programs, mathematics, or logic, has recently
shown promise towards this goal. Examples of neural programming include neural theorem provers,
neural Turing machines, and neural program inducers, e.g. Loos et al. (2017); Graves et al. (2014);
Neelakantan et al. (2015); Boˇsnjak et al. (2017); Allamanis et al. (2017). They aim to solve tasks
such as learning functions in logic, mathematics, or computer programs (e.g. logical or, addition, and
sorting), prove theorems and synthesize programs.
Most works on neural programming either rely only on black-box function evaluations (Graves et al.,
2014; Balog et al., 2017) or on the availability of detailed program execution traces, where entire
program runs are recorded under different input conditions (Reed & De Freitas, 2016; Cai et al.,
2017). Black-box function evaluations are easy to obtain since we only need to generate inputs and
outputs to various functions in the domain. However, by themselves, they do not result in powerful
generalizable models, since they do not have sufﬁcient information about the underlying structure of
the domain. On the other hand, execution traces capture the underlying structure, but, are generally
harder to obtain under many different input conditions; even if they are available, the computational
complexity of incorporating them is signiﬁcant. Due to the lack of good coverage, these approaches
fail to generalize to programs of higher complexity and to domains with a large number of functions.
Moreover, the performance of these frameworks is severely dependent on the nature of execution
traces: more efﬁcient programs lead to a drastic improvement in performance (Cai et al., 2017), but
such programs may not be readily available.
In many problem domains, in addition to function evaluations, one typically has access to more
information such as symbolic representations that encode the relationships between the given variables

1

