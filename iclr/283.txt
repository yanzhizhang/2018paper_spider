Published as a conference paper at ICLR 2018

DECISION BOUNDARY ANALYSIS OF ADVERSARIAL
EXAMPLES

Warren He, Bo Li & Dawn Song
Computer Science Division
University of California, Berkeley

ABSTRACT

Deep neural networks (DNNs) are vulnerable to adversarial examples, which are
carefully crafted instances aiming to cause prediction errors for DNNs. Recent
research on adversarial examples has examined local neighborhoods in the input
space of DNN models. However, previous work has limited what regions to con-
sider, focusing either on low-dimensional subspaces or small balls. In this paper,
we argue that information from larger neighborhoods, such as from more direc-
tions and from greater distances, will better characterize the relationship between
adversarial examples and the DNN models. First, we introduce an attack, OPT-
MARGIN, which generates adversarial examples robust to small perturbations.
These examples successfully evade a defense that only considers a small ball
around an input instance. Second, we analyze a larger neighborhood around in-
put instances by looking at properties of surrounding decision boundaries, namely
the distances to the boundaries and the adjacent classes. We ﬁnd that the bound-
aries around these adversarial examples do not resemble the boundaries around
benign examples. Finally, we show that, under scrutiny of the surrounding deci-
sion boundaries, our OPTMARGIN examples do not convincingly mimic benign
examples. Although our experiments are limited to a few speciﬁc attacks, we hope
these ﬁndings will motivate new, more evasive attacks and ultimately, effective de-
fenses.

1

INTRODUCTION

Recent research in adversarial examples in deep learning has examined local neighborhoods in the
input space of deep learning models. Liu et al. (2017) and Tram`er et al. (2017) examine limited
regions around benign samples to study why some adversarial examples transfer across different
models. Madry et al. (2017) explore regions around benign samples to validate the robustness of an
adversarially trained model. Tabacof & Valle (2016) examine regions around adversarial examples to
estimate the examples’ robustness to random noise. Cao & Gong (2017) determine that considering
the region around an input instance produces more robust classiﬁcation than looking at the input
instance alone as a single point.
These previous works have limited what regions they consider. Liu et al. and Tram`er et al. focus on
low-dimensional subspaces around a model’s gradient direction. Tabacof & Valle and Cao & Gong
explore many directions, but they focus on a small ball.
In this paper, we argue that information from larger neighborhoods—both in more directions and at
greater distances—will better help us understand adversarial examples in high-dimensional datasets.
First, we describe a concrete limitation in a system that utilizes information in small neighborhoods.
Cao & Gong’s region classiﬁcation defense (2017) takes the majority prediction in a small ball
around an input instance. We introduce an attack method, OPTMARGIN, for generating adversarial
examples that are robust to small perturbations, which can evade this defense.
Second, we provide an example of how to analyze an input instance’s surroundings in the model’s
input space. We introduce a technique that looks at the decision boundaries around an input in-
stance, and we use this technique to characterize our robust OPTMARGIN adversarial examples.
Our analysis reveals that, while OPTMARGIN adversarial examples are robust enough to fool region

1

