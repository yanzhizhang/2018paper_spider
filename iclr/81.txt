Published as a conference paper at ICLR 2018

EMERGENT COMPLEXITY VIA MULTI-AGENT
COMPETITION

Trapit Bansal∗
UMass Amherst

Jakub Pachocki
OpenAI

Szymon Sidor
OpenAI

Ilya Sutskever
OpenAI

Igor Mordatch
OpenAI

ABSTRACT

Reinforcement learning algorithms can train agents that solve problems in com-
plex, interesting environments. Normally, the complexity of the trained agent is
closely related to the complexity of the environment. This suggests that a highly
capable agent requires a complex environment for training. In this paper, we point
out that a competitive multi-agent environment trained with self-play can produce
behaviors that are far more complex than the environment itself. We also point out
that such environments come with a natural curriculum, because for any skill level,
an environment full of agents of this level will have the right level of difﬁculty.
This work introduces several competitive multi-agent environments where agents
compete in a 3D world with simulated physics. The trained agents learn a
wide variety of complex and interesting skills, even though the environment
themselves are relatively simple. The skills include behaviors such as running,
blocking, ducking, tackling, fooling opponents, kicking, and defending using
both arms and legs. A highlight of the learned behaviors can be found here:
https://goo.gl/eR7fbX.

1

INTRODUCTION

Reinforcement Learning (RL) is exciting because good reinforcement learning algorithms exist
(Mnih et al., 2015; Silver et al., 2016; Schulman et al., 2015a; Mnih et al., 2016; Schulman et al.,
2015b; Lillicrap et al., 2015; Schulman et al., 2017), allowing us to train agents that accomplish a
great variety of interesting tasks. We can train an agent to play Atari games from pixels (Mnih et al.,
2015) or get humanoids to walk (Schulman et al., 2017). RL is exciting partly because it is easy
to envision an RL algorithm producing a broadly competent agent when trained on an appropriate
curriculum of environments.
In general, training an agent to perform a highly complex task requires a highly complex environ-
ment, and these can be difﬁcult to create. However, there exists a class of environments where the
behavior produced by the agents can be far more complex than the environments; this is the class
of the competitive multi-agent environments trained with self-play. Such environments have two
very attractive properties: (1) Even very simple competitive multi-agent environments can produce
extremely complex behaviors. For example, the game of Go has very simple rules, but the strategies
needed to win are extremely complex. This is because the complexity of these environments is pro-
duced by the competing agents that act in it. Thus, as the other agents become more competent, the
environment effectively becomes more complex. (2) When trained with self-play, the competitive
multi-agent environment provides the agents with a perfect curriculum. This happens because no
matter how weak or strong an agent is, an environment populated with other agents of comparable
strength provides the right challenge to the agent, facilitating maximally rapid learning and avoiding
getting stuck.
Self-play in competitive multi-agent environments is not a new idea – it has already been explored
in TD-gammon (Tesauro, 1995) and reﬁned in AlphaGo (Silver et al., 2016) and Dota 2 (OpenAI).
In both cases, the resulting behavior was far more complex than the environment itself, and the
self-play approach provided the agents with a perfectly tuned curriculum for each task.
In this
paper, we investigate whether the idea of competitive multi-agent environments can yield fruit in

∗Work done as an intern at OpenAI. Correspondence to tbansal@cs.umass.edu

1

