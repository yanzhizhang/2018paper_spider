Published as a conference paper at ICLR 2018

QUANTITATIVELY EVALUATING GANS
WITH DIVERGENCES PROPOSED FOR TRAINING

Daniel Jiwoong Im1,2, He Ma3,4, Graham Taylor3,4, & Kristin Branson1
1Janelia Research Campus, HHMI, 2AIFounded Inc. 3University of Guelph, 4Vector Institute
{imd, bransonk}@janelia.hhmi.org
{hma02, gtaylor}@uoguelph.ca

ABSTRACT

Generative adversarial networks (GANs) have been extremely effective in ap-
proximating complex distributions of high-dimensional, input data samples, and
substantial progress has been made in understanding and improving GAN per-
formance in terms of both theory and application. However, we currently lack
quantitative methods for model assessment. Because ot this, while many GAN
variants being proposed, we have relatively little understanding of their relative
abilities. In this paper, we evaluate the performance of various types of GANs using
divergence and distance functions typically used only for training. We observe
consistency across the various proposed metrics and, interestingly, the test-time
metrics do not favour networks that use the same training-time criterion. We also
compare the proposed metrics to human perceptual scores.

1

INTRODUCTION

Generative adversarial networks (GANs) aim to approximate a data distribution P , using a parame-
terized model distribution Q. They achieve this by jointly optimizing generative and discriminative
networks (Goodfellow et al., 2014). GANs are end-to-end differentiable, and samples from the
generative network are propagated forward to a discriminative network, and error signals are then
propagated backwards from the discriminative network to the generative network. The discriminative
network is often viewed as learned, adaptive loss function for the generative network.
GANs have achieved state-of-the-art results for a number of applications (Goodfellow, 2016), pro-
ducing more realistic, sharper samples than other popular generative models, such as variational
autoencoders (Kingma & Welling, 2014). Because of their success, many GAN frameworks have
been proposed. However, it has been difﬁcult to compare these algorithms and understand their
strengths and weaknesses because we are currently lacking in quantitative methods for assessing the
learned generators.
In this work, we propose new metrics for measuring how realistic samples generated from GANs are.
These criteria are based on a formulation of divergence between the distributions P and Q (Nowozin
et al., 2016; Sriperumbudur et al., 2009):
sup
f∈F

(1)
Here, different choices of µ, υ, and F can correspond to different f-divergences (Nowozin et al.,
2016) or different integral probability metrics (IPMs) (Sriperumbudur et al., 2009). Importantly,
J(Q) can be estimated using samples from P and Q, and does not require us to be able to estimate
P (x) or Q(x) for samples x. Instead, evaluating J(Q) involves ﬁnding the function f ∈ F that is
maximally different with respect to P and Q.
This measure of divergence between the distributions P and Q is related to the GAN criterion if we
restrict the function class F to be neural network functions parameterized by the vector φ and the
class of approximating distributions to correspond to neural network generators Gθ parameterized by
the vector θ, allowing formulation as a min-max problem:

EP (x) [µ (f (x))] − EQ(x) [υ (f (x))]

inf
Q

J(Q) = inf
Q

min

θ

J(θ) = min

θ

max

φ

EP (x) [µ (Dφ(x))] − EQθ(x) [υ (Dφ(x))] ,

(2)

1

