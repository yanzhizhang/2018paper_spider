Published as a conference paper at ICLR 2018

UNSUPERVISED MACHINE TRANSLATION
USING MONOLINGUAL CORPORA ONLY

Guillaume Lample † ‡ , Alexis Conneau † , Ludovic Denoyer ‡ , Marc’Aurelio Ranzato †
† Facebook AI Research,
‡ Sorbonne Universit´es, UPMC Univ Paris 06, LIP6 UMR 7606, CNRS
{gl,aconneau,ranzato}@fb.com,ludovic.denoyer@lip6.fr

ABSTRACT

Machine translation has recently achieved impressive performance thanks to re-
cent advances in deep learning and the availability of large-scale parallel corpora.
There have been numerous attempts to extend these successes to low-resource lan-
guage pairs, yet requiring tens of thousands of parallel sentences. In this work, we
take this research direction to the extreme and investigate whether it is possible to
learn to translate even without any parallel data. We propose a model that takes
sentences from monolingual corpora in two different languages and maps them
into the same latent space. By learning to reconstruct in both languages from this
shared feature space, the model effectively learns to translate without using any
labeled data. We demonstrate our model on two widely used datasets and two lan-
guage pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT
English-French datasets, without using even a single parallel sentence at training
time.

1

INTRODUCTION

Thanks to recent advances in deep learning (Sutskever et al., 2014; Bahdanau et al., 2015) and the
availability of large-scale parallel corpora, machine translation has now reached impressive perfor-
mance on several language pairs (Wu et al., 2016). However, these models work very well only
when provided with massive amounts of parallel data, in the order of millions of parallel sentences.
Unfortunately, parallel corpora are costly to build as they require specialized expertise, and are often
nonexistent for low-resource languages. Conversely, monolingual data is much easier to ﬁnd, and
many languages with limited parallel data still possess signiﬁcant amounts of monolingual data.
There have been several attempts at leveraging monolingual data to improve the quality of ma-
chine translation systems in a semi-supervised setting (Munteanu et al., 2004; Irvine, 2013; Irvine
& Callison-Burch, 2015; Zheng et al., 2017). Most notably, Sennrich et al. (2015a) proposed a
very effective data-augmentation scheme, dubbed “back-translation”, whereby an auxiliary transla-
tion system from the target language to the source language is ﬁrst trained on the available parallel
data, and then used to produce translations from a large monolingual corpus on the target side. The
pairs composed of these translations with their corresponding ground truth targets are then used as
additional training data for the original translation system.
Another way to leverage monolingual data on the target side is to augment the decoder with a
language model (Gulcehre et al., 2015). And ﬁnally, Cheng et al. (2016); He et al. (2016) have
proposed to add an auxiliary auto-encoding task on monolingual data, which ensures that a translated
sentence can be translated back to the original one. All these works still rely on several tens of
thousands parallel sentences, however.
Previous work on zero-resource machine translation has also relied on labeled information, not from
the language pair of interest but from other related language pairs (Firat et al., 2016; Johnson et al.,
2016; Chen et al., 2017) or from other modalities (Nakayama & Nishida, 2017; Lee et al., 2017).
The only exception is the work by Ravi & Knight (2011); Pourdamghani & Knight (2017), where
the machine translation problem is reduced to a deciphering problem. Unfortunately, their method
is limited to rather short sentences and it has only been demonstrated on a very simplistic setting
comprising of the most frequent short sentences, or very closely related languages.

1

