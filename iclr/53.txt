Published as a conference paper at ICLR 2018

MULTI-SCALE DENSE NETWORKS
FOR RESOURCE EFFICIENT IMAGE CLASSIFICATION

Gao Huang
Cornell University

Danlu Chen
Fudan University

Tianhong Li
Tsinghua University

Felix Wu
Cornell University

Laurens van der Maaten
Facebook AI Research

Kilian Weinberger
Cornell University

ABSTRACT

In this paper we investigate image classiﬁcation with computational resource lim-
its at test time. Two such settings are: 1. anytime classiﬁcation, where the net-
work’s prediction for a test example is progressively updated, facilitating the out-
put of a prediction at any time; and 2. budgeted batch classiﬁcation, where a ﬁxed
amount of computation is available to classify a set of examples that can be spent
unevenly across “easier” and “harder” inputs. In contrast to most prior work, such
as the popular Viola and Jones algorithm, our approach is based on convolution-
al neural networks. We train multiple classiﬁers with varying resource demands,
which we adaptively apply during test time. To maximally re-use computation
between the classiﬁers, we incorporate them as early-exits into a single deep con-
volutional neural network and inter-connect them with dense connectivity. To fa-
cilitate high quality classiﬁcation early on, we use a two-dimensional multi-scale
network architecture that maintains coarse and ﬁne level features all-throughout
the network. Experiments on three image-classiﬁcation tasks demonstrate that our
framework substantially improves the existing state-of-the-art in both settings.

1

INTRODUCTION

Recent years have witnessed a surge in demand for applications of visual object recognition, for
instance, in self-driving cars (Bojarski et al., 2016) and content-based image search (Wan et al.,
2014). This demand has in part been fueled through the promise generated by the astonishing
progress of convolutional networks (CNNs) on visual object recognition benchmark competition
datasets, such as ILSVRC (Deng et al., 2009) and COCO (Lin et al., 2014), where state-of-the-art
models may have even surpassed human-level performance (He et al., 2015; 2016).
However, the requirements of such competitions differ from real-
world applications, which tend to incentivize resource-hungry mod-
els with high computational demands at inference time. For exam-
ple, the COCO 2016 competition was won by a large ensemble of
computationally intensive CNNs1 — a model likely far too compu-
tationally expensive for any resource-aware application. Although
much smaller models would also obtain decent error, very large,
computationally intensive models seem necessary to correctly clas-
sify the hard examples that make up the bulk of the remaining mis-
classiﬁcations of modern algorithms. To illustrate this point, Fig-
ure 1 shows two images of horses. The left image depicts a horse
in canonical pose and is easy to classify, whereas the right image is
taken from a rare viewpoint and is likely in the tail of the data dis-
tribution. Computationally intensive models are needed to classify
such tail examples correctly, but are wasteful when applied to canonical images such as the left one.
In real-world applications, computation directly translates into power consumption, which should
be minimized for environmental and economical reasons, and is a scarce commodity on mobile

Figure 1: Two images containing
a horse. The left image is canon-
ical and easy to detect even with
a small model, whereas the right
image requires a computational-
ly more expensive network archi-
tecture. (Copyright Pixel Addict
and Doyle (CC BY-ND 2.0).)

1http://image-net.org/challenges/talks/2016/GRMI-COCO-slidedeck.pdf

1

