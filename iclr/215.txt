Published as a conference paper at ICLR 2018

DEFENSE-GAN: PROTECTING CLASSIFIERS
AGAINST ADVERSARIAL ATTACKS USING
GENERATIVE MODELS

Pouya Samangouei∗, Maya Kabkab∗, and Rama Chellappa
Department of Electrical and Computer Engineering
University of Maryland Institute for Advanced Computer Studies
University of Maryland, College Park, MD 20742
{pouya, mayak, rama}@umiacs.umd.edu

ABSTRACT

In recent years, deep neural network approaches have been widely adopted for
machine learning tasks, including classiﬁcation. However, they were shown to
be vulnerable to adversarial perturbations: carefully crafted small perturbations
can cause misclassiﬁcation of legitimate images. We propose Defense-GAN, a
new framework leveraging the expressive capability of generative models to de-
fend deep neural networks against such attacks. Defense-GAN is trained to model
the distribution of unperturbed images. At inference time, it ﬁnds a close output
to a given image which does not contain the adversarial changes. This output is
then fed to the classiﬁer. Our proposed method can be used with any classiﬁca-
tion model and does not modify the classiﬁer structure or training procedure. It
can also be used as a defense against any attack as it does not assume knowl-
edge of the process for generating the adversarial examples. We empirically show
that Defense-GAN is consistently effective against different attack methods and
improves on existing defense strategies.

1

INTRODUCTION

Despite their outstanding performance on several machine learning tasks, deep neural networks have
been shown to be susceptible to adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015).
These attacks come in the form of adversarial examples: carefully crafted perturbations added to
a legitimate input sample. In the context of classiﬁcation, these perturbations cause the legitimate
sample to be misclassiﬁed at inference time (Szegedy et al., 2014; Goodfellow et al., 2015; Papernot
et al., 2016b; Liu et al., 2017). Such perturbations are often small in magnitude and do not affect
human recognition but can drastically change the output of the classiﬁer.
Recent literature has considered two types of threat models: black-box and white-box attacks. Under
the black-box attack model, the attacker does not have access to the classiﬁcation model parameters;
whereas in the white-box attack model, the attacker has complete access to the model architecture
and parameters, including potential defense mechanisms (Papernot et al., 2017; Tram`er et al., 2017;
Carlini & Wagner, 2017).
Various defenses have been proposed to mitigate the effect of adversarial attacks. These defenses
can be grouped under three different approaches: (1) modifying the training data to make the clas-
siﬁer more robust against attacks, e.g., adversarial training which augments the training data of the
classiﬁer with adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015), (2) modifying
the training procedure of the classiﬁer to reduce the magnitude of gradients, e.g., defensive distil-
lation (Papernot et al., 2016d), and (3) attempting to remove the adversarial noise from the input
samples (Hendrycks & Gimpel, 2017; Meng & Chen, 2017). All of these approaches have limita-
tions in the sense that they are effective against either white-box attacks or black-box attacks, but not
both (Tram`er et al., 2017; Meng & Chen, 2017). Furthermore, some of these defenses are devised
with speciﬁc attack models in mind and are not effective against new attacks.

∗The ﬁrst two authors contributed equally.

1

