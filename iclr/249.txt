Published as a conference paper at ICLR 2018

A PAC-BAYESIAN APPROACH TO
SPECTRALLY-NORMALIZED MARGIN BOUNDS
FOR NEURAL NETWORKS

Behnam Neyshabur, Srinadh Bhojanapalli, Nathan Srebro
Toyota Technological Institute at Chicago
{bneyshabur, srinadh, nati}@ttic.edu

ABSTRACT

We present a generalization bound for feedforward neural networks with ReLU
activations in terms of the product of the spectral norm of the layers and the
Frobenius norm of the weights. The key ingredient is a bound on the changes
in the output of a network with respect to perturbation of its weights, thereby
bounding the sharpness of the network. We combine this perturbation bound with
the PAC-Bayes analysis to derive the generalization bound.

1

INTRODUCTION

Learning with deep neural networks has enjoyed great success across a wide variety of tasks. Even
though learning neural networks is a hard problem, even for one hidden layer (Blum & Rivest, 1993),
simple optimization methods such as stochastic gradient descent (SGD) have been able to minimize
the training error. More surprisingly, solutions found this way also have small test error, even though
the networks used have more parameters than the number of training samples, and have the capacity
to easily ﬁt random labels (Zhang et al., 2017).
Harvey et al. (2017) provided a generalization error bound by showing that the VC dimension of
d-layer networks is depth times the number parameters improving over earlier bounds by Bartlett
et al. (1999). Such VC bounds which depend on the number of parameters of the networks, cannot
explain the generalization behavior in the over parametrized settings, where the number of samples is
much smaller than the number of parameters.
For linear classiﬁers we know that the generalization behavior depends on the norm and margin of
the classiﬁer and not on the actual number of parameters. Hence, a generalization bound for neural
networks that only depends on the norms of its layers, and not the actual number of parameters, can
explain the good generalization behavior of the over parametrized networks.
Bartlett & Mendelson (2002) showed generalization bounds for feedforward networks in terms of
unit-wise (cid:96)1 norm with exponential dependence on depth. In a more recent work, Neyshabur et al.
(2015a) provided generalization bounds for general class of group norms including Frobenius norm
with same exponential dependence on the depth and showed that the exponential dependence is
unavoidable in the worst case.
Bartlett et al. (2017a) showed a margin based generalization bound that depends on spectral norm
and (cid:96)1 norm of the layers of the networks. They show this bound using a complex covering number
argument. This bound does not depend directly on the number of parameters of the network but
depends on the norms of its layers. However the (cid:96)1 term has high dependence on the number of hidden
units if the weights are dense, which is typically the case for modern deep learning architectures.
Keskar et al. (2016) suggested a sharpness based measure to predict the difference in generalization
behavior of networks trained with different batch size SGD. However sharpness is not a scale invariant
measure and cannot predict the generalization behavior (Neyshabur et al., 2017). Instead sharpness
when combined with the norms of the network can predict the generalization behavior according to
the PAC-Bayes framework (McAllester, 1999). Dziugaite & Roy (2017) numerically evaluated a
generalization error bound from the PAC-Bayes framework showing that it can predict the difference
in generalization behavior of networks trained on true vs random labels. This generalization result is

1

