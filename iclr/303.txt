Published as a conference paper at ICLR 2018

PARAMETRIZED HIERARCHICAL PROCEDURES FOR
NEURAL PROGRAMMING

Roy Fox, Richard Shin, Sanjay Krishnan, Ken Goldberg, Dawn Song, and Ion Stoica
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley
{royf,ricshin,sanjaykrishnan,goldberg,dawnsong,istoica}@berkeley.edu

ABSTRACT

Neural programs are highly accurate and structured policies that perform algorith-
mic tasks by controlling the behavior of a computation mechanism. Despite the
potential to increase the interpretability and the compositionality of the behavior of
artiﬁcial agents, it remains difﬁcult to learn from demonstrations neural networks
that represent computer programs. The main challenges that set algorithmic do-
mains apart from other imitation learning domains are the need for high accuracy,
the involvement of speciﬁc structures of data, and the extremely limited observabil-
ity. To address these challenges, we propose to model programs as Parametrized
Hierarchical Procedures (PHPs). A PHP is a sequence of conditional operations,
using a program counter along with the observation to select between taking an
elementary action, invoking another PHP as a sub-procedure, and returning to
the caller. We develop an algorithm for training PHPs from a set of supervisor
demonstrations, only some of which are annotated with the internal call structure,
and apply it to efﬁcient level-wise training of multi-level PHPs. We show in two
benchmarks, NanoCraft and long-hand addition, that PHPs can learn neural pro-
grams more accurately from smaller amounts of both annotated and unannotated
demonstrations.

1

INTRODUCTION

Representing the logic of a computer program with a parametrized model, such as a neural network,
is a central challenge in AI with applications including reinforcement learning, robotics, natural
language processing, and programming by example. A salient feature of recently-proposed approaches
for learning programs (Reed & De Freitas, 2016; Cai et al., 2017; Li et al., 2017) is their ability to
leverage the hierarchical structure of procedure invocations present in well-designed programs.
Explicitly exposing this hierarchical structure enables learning neural programs with empirically
superior generalization, compared to baseline methods that learn only from elementary computer
operations, but requires training data that does not consists only of low-level computer operations
but is annotated with the higher-level procedure calls (Reed & De Freitas, 2016; Cai et al., 2017).
Li et al. (2017) tackled the problem of learning hierarchical neural programs from a mixture of
annotated training data (hereafter called strong supervision) and unannotated training data where only
the elementary operations are given without their call-stack annotations (called weak supervision). In
this paper, we propose to learn hierarchical neural programs from a mixture of strongly supervised
and weakly supervised data via the Expectation–Gradient method and an explicit program counter, in
lieu of a high-dimensional real-valued state of a recurrent neural network.
Our approach is inspired by recent work in robot learning and control. In Imitation Learning (IL), an
agent learns to behave in its environment using supervisor demonstrations of the intended behavior.
However, existing approaches to IL are largely insufﬁcient for addressing algorithmic domains, in
which the target policy is program-like in its accurate and structured manipulation of inputs and data
structures. An example of such a domain is long-hand addition, where the computer loops over the
digits to be added, from least to most signiﬁcant, calculating the sum and carry. In more complicated
examples, the agent must correctly manipulate data structures to compute the right output.

1

