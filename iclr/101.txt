Published as a conference paper at ICLR 2018

ENHANCING THE RELIABILITY OF
OUT-OF-DISTRIBUTION IMAGE DETECTION IN
NEURAL NETWORKS

Shiyu Liang
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
sliang26@illinois.edu

R. Srikant
Coordinated Science Lab, Department of ECE
University of Illinois at Urbana-Champaign
rsrikant@illinois.edu

Yixuan Li
Facebook Research
yixuanl@fb.com

ABSTRACT

We consider the problem of detecting out-of-distribution images in neural networks.
We propose ODIN, a simple and effective method that does not require any change
to a pre-trained neural network. Our method is based on the observation that using
temperature scaling and adding small perturbations to the input can separate the
softmax score distributions between in- and out-of-distribution images, allowing
for more effective detection. We show in a series of experiments that ODIN
is compatible with diverse network architectures and datasets. It consistently
outperforms the baseline approach (Hendrycks & Gimpel, 2017) by a large margin,
establishing a new state-of-the-art performance on this task. For example, ODIN
reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet
(applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.

1

INTRODUCTION

Modern neural networks are known to generalize well when the training and testing data are sampled
from the same distribution (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016;
Cho et al., 2014; Zhang et al., 2017). However, when deploying neural networks in real-world
applications, there is often very little control over the testing data distribution. Recent works
have shown that neural networks tend to make high conﬁdence predictions even for completely
unrecognizable (Nguyen et al., 2015) or irrelevant inputs (Hendrycks & Gimpel, 2017; Szegedy et al.,
2014; Moosavi-Dezfooli et al., 2017). It has been well documented (Amodei et al., 2016) that it is
important for classiﬁers to be aware of uncertainty when shown new kinds of inputs, i.e., out-of-
distribution examples. Therefore, being able to accurately detect out-of-distribution examples can
be practically important for visual recognition tasks (Krizhevsky et al., 2012; Farabet et al., 2013; Ji
et al., 2013).
A seemingly straightforward approach of detecting out-of-distribution images is to enlarge the training
set of both in- and out-of-distribution examples. However, the number of out-of-distribution examples
can be inﬁnitely many, making the re-training approach computationally expensive and intractable.
Moreover, to ensure that a neural network accurately classiﬁes in-distribution samples into correct
classes while correctly detecting out-of-distribution samples, one might need to employ exceedingly
large neural network architectures, which further complicates the training process.
Hendrycks & Gimpel (2017) proposed a baseline method to detect out-of-distribution examples
without further re-training networks. The method is based on an observation that a well-trained neural
network tends to assign higher softmax scores to in-distribution examples than out-of-distribution
examples. In this paper, we go further. We observe that after using temperature scaling in the softmax
function (Hinton et al., 2015; Pereyra et al., 2017) and adding small controlled perturbations to inputs,

1

