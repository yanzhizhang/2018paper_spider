Published as a conference paper at ICLR 2018

EMERGENT TRANSLATION
IN MULTI-AGENT COMMUNICATION

Kyunghyun Cho
New York University
Facebook AI Research
kyunghyun.cho@nyu.edu

Jason Weston
Facebook AI Research
jase@fb.com

Jason Lee∗
New York University
jason@cs.nyu.edu

Douwe Kiela
Facebook AI Research
dkiela@fb.com

ABSTRACT

While most machine translation systems to date are trained on large parallel cor-
pora, humans learn language in a different way: by being grounded in an environ-
ment and interacting with other humans. In this work, we propose a communi-
cation game where two agents, native speakers of their own respective languages,
jointly learn to solve a visual referential task. We ﬁnd that the ability to understand
and translate a foreign language emerges as a means to achieve shared goals. The
emergent translation is interactive and multimodal, and crucially does not require
parallel corpora, but only monolingual, independent text and corresponding im-
ages. Our proposed translation model achieves this by grounding the source and
target languages into a shared visual modality, and outperforms several baselines
on both word-level and sentence-level translation tasks. Furthermore, we show
that agents in a multilingual community learn to translate better and faster than in
a bilingual communication setting.

1

INTRODUCTION

Building intelligent machines that can converse with humans is a longstanding challenge in artiﬁcial
intelligence. Remarkable successes have been achieved in natural language processing (NLP) via
the use of supervised learning approaches on large-scale datasets (Bahdanau et al., 2015; Wu et al.,
2016; Gehring et al., 2017; Sennrich et al., 2017). Machine translation is no exception: most trans-
lation systems are trained to derive statistical patterns from huge parallel corpora. Parallel corpora,
however, are expensive and difﬁcult to obtain for many language pairs. This is especially the case
for low resource languages, where parallel texts are often small or nonexistent. We address these
issues by designing a multi-agent communication task, where agents interact with each other in their
own native languages and try to work out what the other agent meant to communicate. We ﬁnd that
the ability to translate foreign languages emerges as a means to achieve a common goal.
Aside from the beneﬁt of not requiring parallel data, we argue that our approach to learning to trans-
late is also more natural than learning from large corpora. Humans learn languages by interacting
with other humans and referring to their shared environment, i.e., by being grounded in physical
reality. More abstract knowledge is built on top of this concrete foundation. It is natural to use
vision as an intermediary: when communicating with someone who does not speak our language,
we often directly refer to our surroundings. Even linguistically distant languages will, by physical
and cognitive necessity, still refer to scenes and objects in the same visual space.
We compare our model against a number of baselines, including a nearest neighbor method and a
recently proposed model (Nakayama & Nishida, 2017) that maps languages and images to a shared
space, but lacks communication. We evaluate performance on both word- and sentence-level trans-
lation, and show that our model outperforms the baselines in both settings. Additionally, we show

∗Work done while the author was interning at Facebook AI Research.

1

