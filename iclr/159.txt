Published as a conference paper at ICLR 2018

TRAINING CONFIDENCE-CALIBRATED CLASSIFIERS
FOR DETECTING OUT-OF-DISTRIBUTION SAMPLES

Honglak Lee§ ,†

Kimin Lee∗
Jinwoo Shin∗
∗Korea Advanced Institute of Science and Technology, Daejeon, Korea
†University of Michigan, Ann Arbor, MI 48109
§Google Brain, Mountain View, CA 94043

Kibok Lee†

ABSTRACT

The problem of detecting whether a test sample is from in-distribution (i.e., train-
ing distribution by a classiﬁer) or out-of-distribution sufﬁciently different from it
arises in many real-world machine learning applications. However, the state-of-art
deep neural networks are known to be highly overconﬁdent in their predictions,
i.e., do not distinguish in- and out-of-distributions. Recently, to handle this is-
sue, several threshold-based detectors have been proposed given pre-trained neu-
ral classiﬁers. However, the performance of prior works highly depends on how
to train the classiﬁers since they only focus on improving inference procedures.
In this paper, we develop a novel training method for classiﬁers so that such in-
ference algorithms can work better. In particular, we suggest two additional terms
added to the original loss (e.g., cross entropy). The ﬁrst one forces samples from
out-of-distribution less conﬁdent by the classiﬁer and the second one is for (im-
plicitly) generating most effective training samples for the ﬁrst one. In essence,
our method jointly trains both classiﬁcation and generative neural networks for
out-of-distribution. We demonstrate its effectiveness using deep convolutional
neural networks on various popular image datasets.

1

INTRODUCTION

Deep neural networks (DNNs) have demonstrated state-of-the-art performance on many classiﬁ-
cation tasks, e.g., speech recognition (Hannun et al., 2014), image classiﬁcation (Girshick, 2015),
video prediction (Villegas et al., 2017) and medical diagnosis (Caruana et al., 2015). Even though
DNNs achieve high accuracy, it has been addressed (Lakshminarayanan et al., 2017; Guo et al.,
2017) that they are typically overconﬁdent in their predictions. For example, DNNs trained to clas-
sify MNIST images often produce high conﬁdent probability 91% even for random noise (see the
work of (Hendrycks & Gimpel, 2016)). Since evaluating the quality of their predictive uncertainty
is hard, deploying them in real-world systems raises serious concerns in AI Safety (Amodei et al.,
2016), e.g., one can easily break a secure authentication system that can be unlocked by detecting
the gaze and iris of eyes using DNNs (Shrivastava et al., 2017).
The overconﬁdence issue of DNNs is highly related to the problem of detecting out-of-distribution:
detect whether a test sample is from in-distribution (i.e., training distribution by a classiﬁer) or out-
of-distribution sufﬁciently different from it. Formally, it can be formulated as a binary classiﬁcation
problem. Let an input x ∈ X and a label y ∈ Y = {1, . . . , K} be random variables that follow a
joint data distribution Pin (x, y) = Pin (y|x) Pin (x). We assume that a classiﬁer Pθ (y|x) is trained
on a dataset drawn from Pin (x, y), where θ denotes the model parameter. We let Pout (x) denote
an out-of-distribution which is ‘far away’ from in-distribution Pin (x). Our problem of interest is
determining if input x is from Pin or Pout, possibly utilizing a well calibrated classiﬁer Pθ (y|x).
In other words, we aim to build a detector, g (x) : X → {0, 1}, which assigns label 1 if data is from
in-distribution, and label 0 otherwise.
There have been recent efforts toward developing efﬁcient detection methods where they mostly
have studied simple threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) uti-
lizing a pre-trained classiﬁer. For each input x, it measures some conﬁdence score q(x) based on a
pre-trained classiﬁer, and compares the score to some threshold δ > 0. Then, the detector assigns

1

