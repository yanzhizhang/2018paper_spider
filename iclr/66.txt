Published as a conference paper at ICLR 2018

IMPROVING GANS USING OPTIMAL TRANSPORT

Tim Salimans∗
OpenAI
tim@openai.com

Han Zhang∗†
Rutgers University
han.zhang@cs.rutgers.edu

Alec Radford
OpenAI
alec@openai.com

Dimitris Metaxas
Rutgers University
dnm@cs.rutgers.edu

ABSTRACT

We present Optimal Transport GAN (OT-GAN), a variant of generative adversar-
ial nets minimizing a new metric measuring the distance between the generator
distribution and the data distribution. This metric, which we call mini-batch en-
ergy distance, combines optimal transport in primal form with an energy distance
deﬁned in an adversarially learned feature space, resulting in a highly discrimi-
native distance function with unbiased mini-batch gradients. Experimentally we
show OT-GAN to be highly stable when trained with large mini-batches, and we
present state-of-the-art results on several popular benchmark problems for image
generation.

1

INTRODUCTION

Generative modeling is a major sub-ﬁeld of Machine Learning that studies the problem of how
to learn models that generate images, audio, video, text or other data. Applications of generative
models include image compression, generating speech from text, planning in reinforcement learn-
ing, semi-supervised and unsupervised representation learning, and many others. Since generative
models can be trained on unlabeled data, which is almost endlessly available, they have enormous
potential in the development of artiﬁcial intelligence.
The central problem in generative modeling is how to train a generative model such that the distri-
bution of its generated data will match the distribution of the training data. Generative adversarial
nets (GANs) represent an advance in solving this problem, using a neural network discriminator or
critic to distinguish between generated data and training data. The critic deﬁnes a distance between
the model distribution and the data distribution which the generative model can optimize to produce
data that more closely resembles the training data.
A closely related approach to measuring the distance between the distributions of generated data
and training data is provided by optimal transport theory. By framing the problem as optimally
transporting one set of data points to another, it represents an alternative method of specifying a
metric over probability distributions and provides another objective for training generative models.
The dual problem of optimal transport is closely related to GANs, as discussed in the next section.
However, the primal formulation of optimal transport has the advantage that it allows for closed
form solutions and can thus more easily be used to deﬁne tractable training objectives that can be
evaluated in practice without making approximations. A complication in using primal form optimal
transport is that it may give biased gradients when used with mini-batches (see Bellemare et al.,
2017) and may therefore be inconsistent as a technique for statistical estimation.
In this paper we present OT-GAN, a variant of generative adversarial nets incorporating primal form
optimal transport into its critic. We derive and justify our model by deﬁning a new metric over
probability distributions, which we call Mini-batch Energy Distance, combining optimal transport

∗equal contribution
†work performed during an internship at OpenAI

1

