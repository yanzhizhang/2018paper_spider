Published as a conference paper at ICLR 2018

GENERATING WIKIPEDIA BY SUMMARIZING LONG
SEQUENCES

Peter J. Liu∗, Mohammad Saleh∗,
Etienne Pot†, Ben Goodrich, Ryan Sepassi, Łukasz Kaiser, Noam Shazeer
Google Brain
Mountain View, CA
{peterjliu,msaleh,epot,bgoodrich,rsepassi,lukaszkaiser,noam}@google.com

ABSTRACT

We show that generating English Wikipedia articles can be approached as a multi-
document summarization of source documents. We use extractive summarization
to coarsely identify salient information and a neural abstractive model to generate
the article. For the abstractive model, we introduce a decoder-only architecture
that can scalably attend to very long sequences, much longer than typical encoder-
decoder architectures used in sequence transduction. We show that this model can
generate ﬂuent, coherent multi-sentence paragraphs and even whole Wikipedia
articles. When given reference documents, we show it can extract relevant factual
information as reﬂected in perplexity, ROUGE scores and human evaluations.

1

INTRODUCTION

The sequence-to-sequence framework has demonstrated success in natural-language sequence trans-
duction tasks such as machine translation. More recently, neural techniques have been applied to do
single-document, abstractive (paraphrasing) text summarization of news articles (Rush et al. (2015),
Nallapati et al. (2016)). In this prior work, the input to supervised models ranged from the ﬁrst sen-
tence to the entire text of an article, and they are trained end-to-end to predict reference summaries.
Doing this end-to-end requires a signiﬁcant number of parallel article-summary pairs since language
understanding is a pre-requisite to generate ﬂuent summaries.
In contrast, we consider the task of multi-document summarization, where the input is a collection
of related documents from which a summary is distilled. Prior work has focused on extractive
summarization, which select sentences or phrases from the input to form the summaries, rather
than generating new text. There has been limited application of abstractive neural methods and one
possible reason is the paucity of large, labeled datasets.
In this work, we consider English Wikipedia as a supervised machine learning task for multi-
document summarization where the input is comprised of a Wikipedia topic (title of article) and
a collection of non-Wikipedia reference documents, and the target is the Wikipedia article text. We
describe the ﬁrst attempt to abstractively generate the ﬁrst section, or lead, of Wikipedia articles con-
ditioned on reference text. In addition to running strong baseline models on the task, we modify the
Transformer architecture (Vaswani et al., 2017) to only consist of a decoder, which performs better
in the case of longer input sequences compared to recurrent neural network (RNN) and Transformer
encoder-decoder models. Finally we show our modeling improvements allow us to generate entire
Wikipedia articles.

∗Joint ﬁrst-authors. Ordered randomly.
†Work done as a member of the Google Brain Residency (g.co/brainresidency)

1

