Published as a conference paper at ICLR 2018

THE REACTOR:
A FAST AND SAMPLE-EFFICIENT ACTOR-CRITIC
AGENT FOR REINFORCEMENT LEARNING

Audr¯unas Gruslys,
DeepMind
audrunas@google.com

Will Dabney,
DeepMind
wdabney@google.com

Mohammad Gheshlaghi Azar,
DeepMind
mazar@google.com

Bilal Piot,
DeepMind
piot@google.com

Marc G. Bellemare,
Google Brain
bellemare@google.com

Rémi Munos,
DeepMind
munos@google.com

ABSTRACT

In this work, we present a new agent architecture, called Reactor, which combines
multiple algorithmic and architectural contributions to produce an agent with higher
sample-efﬁciency than Prioritized Dueling DQN (Wang et al., 2017) and Categori-
cal DQN (Bellemare et al., 2017), while giving better run-time performance than
A3C (Mnih et al., 2016). Our ﬁrst contribution is a new policy evaluation algorithm
called Distributional Retrace, which brings multi-step off-policy updates to the
distributional reinforcement learning setting. The same approach can be used to
convert several classes of multi-step policy evaluation algorithms, designed for
expected value evaluation, into distributional algorithms. Next, we introduce the
β-leave-one-out policy gradient algorithm, which improves the trade-off between
variance and bias by using action values as a baseline. Our ﬁnal algorithmic con-
tribution is a new prioritized replay algorithm for sequences, which exploits the
temporal locality of neighboring observations for more efﬁcient replay prioritiza-
tion. Using the Atari 2600 benchmarks, we show that each of these innovations
contribute to both sample efﬁciency and ﬁnal agent performance. Finally, we
demonstrate that Reactor reaches state-of-the-art performance after 200 million
frames and less than a day of training.

1

INTRODUCTION

Model-free deep reinforcement learning has achieved several remarkable successes in domains
ranging from super-human-level control in video games (Mnih et al., 2015) and the game of Go
(Silver et al., 2016; 2017), to continuous motor control tasks (Lillicrap et al., 2015; Schulman et al.,
2015).
Much of the recent work can be divided into two categories. First, those of which that, often building
on the DQN framework, act -greedily according to an action-value function and train using mini-
batches of transitions sampled from an experience replay buffer (Van Hasselt et al., 2016; Wang et al.,
2015; He et al., 2017; Anschel et al., 2017). These value-function agents beneﬁt from improved
sample complexity, but tend to suffer from long runtimes (e.g. DQN requires approximately a week
to train on Atari). The second category are the actor-critic agents, which includes the asynchronous
advantage actor-critic (A3C) algorithm, introduced by Mnih et al. (2016). These agents train on
transitions collected by multiple actors running, and often training, in parallel (Schulman et al., 2017;
Vezhnevets et al., 2017). The deep actor-critic agents train on each trajectory only once, and thus
tend to have worse sample complexity. However, their distributed nature allows signiﬁcantly faster
training in terms of wall-clock time. Still, not all existing algorithms can be put in the above two
categories and various hybrid approaches do exist (Zhao et al., 2016; O’Donoghue et al., 2017; Gu
et al., 2017; Wang et al., 2017).

1

