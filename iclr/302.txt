Published as a conference paper at ICLR 2018

BOOSTING THE ACTOR WITH DUAL CRITIC

Bo Dai*1, Albert Shaw*1, Niao He2, Lihong Li3, Le Song1, 4
1 Georgia Institute of Technology, 2 University of Illinois at Urbana-Champaign
3 Google AI, 4 Ant Financial Services Group
1 {bodai, ashaw596}@gatech.edu, lsong@cc.gatech.edu
2 niaohe@illinois.edu, 3 lihongli.cs@gmail.com

ABSTRACT

This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or
Dual-AC. It is derived in a principled way from the Lagrangian dual form of the
Bellman optimality equation, which can be viewed as a two-player game between
the actor and a critic-like function, which is named as dual critic. Compared to
its actor-critic relatives, Dual-AC has the desired property that the actor and dual
critic are updated cooperatively to optimize the same objective function, provid-
ing a more transparent way for learning the critic that is directly related to the
objective function of the actor. We then provide a concrete algorithm that can
effectively solve the minimax optimization problem, using techniques of multi-
step bootstrapping, path regularization, and stochastic dual ascent algorithm. We
demonstrate that the proposed algorithm achieves state-of-the-art performance
across several benchmarks.

1

INTRODUCTION

Reinforcement learning (RL) algorithms aim to learn a policy that maximizes the long-term return
by sequentially interacting with an unknown environment. Value-function-based algorithms ﬁrst
approximate the optimal value function, which can then be used to derive a good policy. These
methods (Sutton, 1988; Watkins, 1989) often take advantage of the Bellman equation and use boot-
strapping to make learning more sample efﬁcient than Monte Carlo estimation (Sutton & Barto,
1998). However, the relation between the quality of the learned value function and the quality of the
derived policy is fairly weak (Bertsekas & Tsitsiklis, 1996). Policy-search-based algorithms such
as REINFORCE (Williams, 1992) and others (Kakade, 2002; Schulman et al., 2015a), on the other
hand, assume a ﬁxed space of parameterized policies and search for the optimal policy parameter
based on unbiased Monte Carlo estimates. The parameters are often updated incrementally along
stochastic directions that on average are guaranteed to increase the policy quality. Unfortunately,
they often have a greater variance that results in a higher sample complexity.
Actor-critic methods combine the beneﬁts of these two classes, and have proved successful in a
number of challenging problems such as robotics (Deisenroth et al., 2013), meta-learning (Bello
et al., 2016), and games (Mnih et al., 2016). An actor-critic algorithm has two components: the
actor (policy) and the critic (value function). As in policy-search methods, actor is updated towards
the direction of policy improvement. However, the update directions are computed with the help of
the critic, which can be more efﬁciently learned as in value-function-based methods (Sutton et al.,
1999; Konda & Tsitsiklis, 2003; Peters et al., 2005; Bhatnagar et al., 2009; Schulman et al., 2015b).
Although the use of a critic may introduce bias in learning the actor, its reduces variance and thus
the sample complexity as well, compared to pure policy-search algorithms.
While the use of a critic is important for the efﬁciency of actor-critic algorithms, it is not entirely
clear how the critic should be optimized to facilitate improvement of the actor. For some parametric
family of policies, it is known that a certain compatibility condition ensures the actor parameter
update is an unbiased estimate of the true policy gradient (Sutton et al., 1999). In practice, temporal-
difference methods are perhaps the most popular choice to learn the critic, especially when nonlinear
function approximation is used (e.g., Schulman et al. (2015b)).

*Both authors equally contributed to the paper.

1

