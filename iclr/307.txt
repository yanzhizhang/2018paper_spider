Published as a conference paper at ICLR 2018

DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC
POLICY GRADIENTS

Gabriel Barth-Maron˚, Matthew W. Hoffman˚, David Budden, Will Dabney,
Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, Timothy Lillicrap
DeepMind
London, UK
{gabrielbm, mwhoffman, budden, wdabney, horgan, dhruvat,
alimuldal, heess, countzero}@google.com

ABSTRACT

This work adopts the very successful distributional perspective on reinforcement
learning and adapts it to the continuous control setting. We combine this within a
distributed framework for off-policy learning in order to develop what we call the
Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG.
We also combine this technique with a number of additional, simple improvements
such as the use of N-step returns and prioritized experience replay. Experimen-
tally we examine the contribution of each of these individual components, and
show how they interact, as well as their combined contributions. Our results show
that across a wide variety of simple control tasks, difﬁcult manipulation tasks, and
a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state
of the art performance.

1

INTRODUCTION

The ability to solve complex control tasks with high-dimensional input and action spaces is a key
milestone in developing real-world artiﬁcial intelligence. The use of reinforcement learning to solve
these types of tasks has exploded following the work of the Deep Q Network (DQN) algorithm
(Mnih et al., 2015), capable of human-level performance on many Atari games. Similarly, ground
breaking achievements have been made in classical games such as Go (Silver et al., 2016). However,
these algorithms are restricted to problems with a ﬁnite number of discrete actions.
In control tasks, commonly seen in the robotics domain, continuous action spaces are the norm.
For algorithms such as DQN the policy is only implicitly deﬁned in terms of its value function,
with actions selected by maximizing this function. In the continuous control domain this would
require either a costly optimization step or discretization of the action space. While discretization is
perhaps the most straightforward solution, this can prove a particularly poor approximation in high-
dimensional settings or those that require ﬁner grained control. Instead, a more principled approach
is to parameterize the policy explicitly and directly optimize the long term value of following this
policy.
In this work we consider a number of modiﬁcations to the Deep Deterministic Policy Gradient
(DDPG) algorithm (Lillicrap et al., 2015). This algorithm has several properties that make it ideal
for the enhancements we consider, which is at its core an off-policy actor-critic method. In particular,
the policy gradient used to update the actor network depends only on a learned critic. This means
that any improvements to the critic learning procedure will directly improve the quality of the actor
updates. In this work we utilize a distributional (Bellemare et al., 2017) version of the critic update
which provides a better, more stable learning signal. Such distributions model the randomness due
to intrinsic factors, among these is the inherent uncertainty imposed by function approximation in
a continuous environment. We will see that using this distributional update directly results in better
gradients and hence improves the performance of the learning algorithm.
Due to the fact that DDPG is capable of learning off-policy it is also possible to modify the way
in which experience is gathered. In this work we utilize this fact to run many actors in parallel,
all feeding into a single replay table. This allows us to seamlessly distribute the task of gathering

˚Authors contributed equally.

1

