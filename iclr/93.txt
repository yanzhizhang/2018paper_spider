Published as a conference paper at ICLR 2018

LARGE-SCALE OPTIMAL TRANSPORT AND MAPPING
ESTIMATION

Vivien Seguy
Kyoto University
Graduate School of Informatics
vivien.seguy@iip.ist.i.kyoto-u.ac.jp

Bharath Bhushan Damodaran
Universit´e de Bretagne Sud
IRISA, UMR 6074, CNRS
bharath-bhushan.damodaran@irisa.fr

R´emi Flamary
Universit´e Cˆote dAzur
Lagrange, UMR 7293, CNRS, OCA
remi.flamary@unice.fr

Nicolas Courty
Universit´e de Bretagne Sud
IRISA, UMR 6074, CNRS
courty@univ-ubs.fr

Antoine Rolet
Kyoto University
Graduate School of Informatics
antoine.rolet@iip.ist.i.kyoto-u.ac.jp

Mathieu Blondel
NTT Communication Science Laboratories
mathieu@mblondel.org

ABSTRACT

This paper presents a novel two-step approach for the fundamental problem of
learning an optimal map from one distribution to another. First, we learn an opti-
mal transport (OT) plan, which can be thought as a one-to-many map between the
two distributions. To that end, we propose a stochastic dual approach of regular-
ized OT, and show empirically that it scales better than a recent related approach
when the amount of samples is very large. Second, we estimate a Monge map as
a deep neural network learned by approximating the barycentric projection of the
previously-obtained OT plan. This parameterization allows generalization of the
mapping outside the support of the input measure. We prove two theoretical stabil-
ity results of regularized OT which show that our estimations converge to the OT
plan and Monge map between the underlying continuous measures. We showcase
our proposed approach on two applications: domain adaptation and generative
modeling.

INTRODUCTION

1
Mapping one distribution to another Given two random variables X and Y taking values in X
and Y respectively, the problem of ﬁnding a map f such that f (X) and Y have the same distribution,
denoted f (X) ∼ Y henceforth, ﬁnds applications in many areas. For instance, in domain adaptation,
given a source dataset and a target dataset with different distributions, the use of a mapping to align
the source and target distributions is a natural formulation (Gopalan et al., 2011) since theory has
shown that generalization depends on the similarity between the two distributions (Ben-David et al.,
2010). Current state-of-the-art methods for computing generative models such as generative adver-
sarial networks (Goodfellow et al., 2014), generative moments matching networks (Li et al., 2015)
or variational auto encoders (Kingma & Welling, 2013) also rely on ﬁnding f such that f (X) ∼ Y .
In this setting, the latent variable X is often chosen as a continuous random variable, such as a
Gaussian distribution, and Y is a discrete distribution of real data, e.g. the ImageNet dataset. By
learning a map f, sampling from the generative model boils down to simply drawing a sample from
X and then applying f to that sample.
Mapping with optimality Among the potentially many maps f verifying f (X) ∼ Y , it may be of
interest to ﬁnd a map which satisﬁes some optimality criterion. Given a cost of moving mass from
one point to another, one would naturally look for a map which minimizes the total cost of trans-
porting the mass from X to Y . This is the original formulation of Monge (1781), which initiated

1

