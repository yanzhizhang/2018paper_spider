Published as a conference paper at ICLR 2018

EMPIRICAL RISK LANDSCAPE ANALYSIS FOR UNDER-
STANDING DEEP NEURAL NETWORKS

Pan Zhou & Jiashi Feng
Department of Electrical and Computer Engineering
National University of Singapore
Singapore, 117583
{pzhou@u.nus.edu, elefjia@nus.edu.sg}

ABSTRACT

√

l

O(r2l(cid:112)

This work aims to provide comprehensive landscape analysis of empirical risk
in deep neural networks (DNNs), including the convergence behavior of its gra-
dient, its stationary points and the empirical risk itself to their corresponding
population counterparts, which reveals how various network parameters deter-
mine the convergence performance.
In particular, for an l-layer linear neural
network consisting of di neurons in the i-th layer, we prove the gradient of its
empirical risk uniformly converges to the one of its population risk, at the rate of
maxi dis log(d/l)/n). Here d is the total weight dimension, s is the
number of nonzero entries of all the weights and the magnitude of weights per
layer is upper bounded by r. Moreover, we prove the one-to-one correspondence
of the non-degenerate stationary points between the empirical and population risks
and provide convergence guarantee for each pair. We also establish the uniform
convergence of the empirical risk to its population counterpart and further derive
the stability and generalization bounds for the empirical risk. In addition, we ana-
lyze these properties for deep nonlinear neural networks with sigmoid activation
functions. We prove similar results for convergence behavior of their empirical risk
gradients, non-degenerate stationary points as well as the empirical risk itself.
To our best knowledge, this work is the ﬁrst one theoretically characterizing the
uniform convergence of the gradient and stationary points of the empirical risk
of DNN models, which beneﬁts the theoretical understanding on how the neural
network depth l, the layer width di, the network size d, the sparsity in weight and
the parameter magnitude r determine the neural network landscape.

1

INTRODUCTION

Deep learning has achieved remarkable success in many ﬁelds, such as computer vision (Hinton
et al., 2006; Szegedy et al., 2015; He et al., 2016), natural language processing (Collobert & Weston,
2008; Bakshi & Stephanopoulos, 1993), and speech recognition (Hinton et al., 2012; Graves et al.,
2013). However, theoretical understanding on the properties of deep learning models still lags
behind their practical achievements (Shalev-Shwartz et al., 2017; Kawaguchi, 2016) due to their
high non-convexity and internal complexity. In practice, parameters of deep learning models are
learned by minimizing the empirical risk via (stochastic-)gradient descent. Therefore, some recent
works (Bartlett & Maass, 2003; Neyshabur et al., 2015) analyzed the convergence of the empirical
risk to the population risk, which are however still far from fully understanding the landscape of the
empirical risk in deep learning models. Beyond the convergence properties of the empirical risk itself,
the convergence and distribution properties of its gradient and stationary points are also essential
in landscape analysis. A comprehensive landscape analysis can reveal important information on
the optimization behavior and practical performance of deep neural networks, and will be helpful
to designing better network architectures. Thus, in this work we aim to provide comprehensive
landscape analysis by looking into the gradients and stationary points of the empirical risk.
Formally, we consider a DNN model f (w; x, y) : Rd0 × Rdl → R parameterized by w ∈ Rd
consisting of l layers (l ≥ 2) that is trained by minimizing the commonly used squared loss function

1

