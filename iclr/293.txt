LEARNING A GENERATIVE MODEL FOR VALIDITY IN
COMPLEX DISCRETE STRUCTURES

David Janz
University of Cambridge
dj343@cam.ac.uk

Jos van der Westhuizen
University of Cambridge
jv365@cam.ac.uk

Brooks Paige
Alan Turing Institute
University of Cambridge
bpaige@turing.ac.uk

Matt J. Kusner
Alan Turing Institute
University of Warwick
mkusner@turing.ac.uk

Jos´e Miguel Hern´andez-Lobato
Alan Turing Institute
University of Cambridge
jmh233@cam.ac.uk

ABSTRACT

Deep generative models have been successfully used to learn representations for
high-dimensional discrete spaces by representing discrete objects as sequences and
employing powerful sequence-based deep models. Unfortunately, these sequence-
based models often produce invalid sequences: sequences which do not represent
any underlying discrete structure; invalid sequences hinder the utility of such mod-
els. As a step towards solving this problem, we propose to learn a deep recurrent
validator model, which can estimate whether a partial sequence can function as
the beginning of a full, valid sequence. This validator provides insight as to how
individual sequence elements inﬂuence the validity of the overall sequence, and can
be used to constrain sequence based models to generate valid sequences – and thus
faithfully model discrete objects. Our approach is inspired by reinforcement learn-
ing, where an oracle which can evaluate validity of complete sequences provides a
sparse reward signal. We demonstrate its effectiveness as a generative model of
Python 3 source code for mathematical expressions, and in improving the ability
of a variational autoencoder trained on SMILES strings to decode valid molecular
structures.

1

INTRODUCTION

Deep generative modeling has seen many successful recent developments, such as producing realistic
images from noise (Radford et al., 2015) and creating artwork (Gatys et al., 2016). We ﬁnd particularly
promising the opportunity to leverage deep generative models for search in high-dimensional discrete
spaces (G´omez-Bombarelli et al., 2016b; Kusner et al., 2017). Discrete search is at the heart of
problems in drug discovery (G´omez-Bombarelli et al., 2016a), natural language processing (Bowman
et al., 2016; Guimaraes et al., 2017), and symbolic regression (Kusner et al., 2017).
The application of deep modeling to search involves ‘lifting’ the search from the discrete space to a
continuous space, via an autoencoder (Rumelhart et al., 1985). An autoencoder learns two mappings:
1) a mapping from discrete space to continuous space called an encoder; and 2) a reverse mapping
from continuous space back to discrete space called a decoder. The discrete space is presented
to the autoencoder as a sequence in some formal language — for example, in G´omez-Bombarelli
et al. (2016b) molecules are encoded as SMILES strings — and powerful sequential models (e.g.,
LSTMs (Hochreiter & Schmidhuber, 1997) GRUs (Cho et al., 2014), DCNNs (Kalchbrenner et al.,
2014)) are applied to the string representation. When employing these models as encoders and
decoders, generation of invalid sequences is however possible, and using current techniques this
happens frequently. Kusner et al. (2017) aimed to ﬁx this by basing the sequential models on parse
tree representations of the discrete structures, where externally speciﬁed grammatical rules assist the
model in the decoding process. This work boosted the ability of the model to produce valid sequences

1

