Published as a conference paper at ICLR 2018

META-LEARNING AND UNIVERSALITY:
DEEP REPRESENTATIONS AND GRADIENT DESCENT CAN
APPROXIMATE ANY LEARNING ALGORITHM

Chelsea Finn & Sergey Levine
University of California, Berkeley
{cbfinn,svlevine}@eecs.berkeley.edu

ABSTRACT

Learning to learn is a powerful paradigm for enabling models to learn from data
more effectively and efﬁciently. A popular approach to meta-learning is to train
a recurrent model to read in a training dataset as input and output the parame-
ters of a learned model, or output predictions for new test inputs. Alternatively, a
more recent approach to meta-learning aims to acquire deep representations that
can be effectively ﬁne-tuned, via standard gradient descent, to new tasks. In this
paper, we consider the meta-learning problem from the perspective of universal-
ity, formalizing the notion of learning algorithm approximation and comparing
the expressive power of the aforementioned recurrent models to the more recent
approaches that embed gradient descent into the meta-learner. In particular, we
seek to answer the following question: does deep representation combined with
standard gradient descent have sufﬁcient capacity to approximate any learning al-
gorithm? We ﬁnd that this is indeed true, and further ﬁnd, in our experiments, that
gradient-based meta-learning consistently leads to learning strategies that gener-
alize more widely compared to those represented by recurrent models.

INTRODUCTION

1
Deep neural networks that optimize for effective representations have enjoyed tremendous success
over human-engineered representations. Meta-learning takes this one step further by optimizing
for a learning algorithm that can effectively acquire representations. A common approach to meta-
learning is to train a recurrent or memory-augmented model such as a recurrent neural network to
take a training dataset as input and then output the parameters of a learner model (Schmidhuber,
1987; Bengio et al., 1992; Li & Malik, 2017a; Andrychowicz et al., 2016). Alternatively, some
approaches pass the dataset and test input into the model, which then outputs a corresponding pre-
diction for the test example (Santoro et al., 2016; Duan et al., 2016; Wang et al., 2016; Mishra et al.,
2018). Such recurrent models are universal learning procedure approximators, in that they have the
capacity to approximately represent any mapping from dataset and test datapoint to label. However,
depending on the form of the model, it may lack statistical efﬁciency.
In contrast to the aforementioned approaches, more recent work has proposed methods that include
the structure of optimization problems into the meta-learner (Ravi & Larochelle, 2017; Finn et al.,
2017a; Husken & Goerick, 2000). In particular, model-agnostic meta-learning (MAML) optimizes
only for the initial parameters of the learner model, using standard gradient descent as the learner’s
update rule (Finn et al., 2017a). Then, at meta-test time, the learner is trained via gradient descent.
By incorporating prior knowledge about gradient-based learning, MAML improves on the statistical
efﬁciency of black-box meta-learners and has successfully been applied to a range of meta-learning
problems (Finn et al., 2017a;b; Li et al., 2017). But, does it do so at a cost? A natural question that
arises with purely gradient-based meta-learners such as MAML is whether it is indeed sufﬁcient
to only learn an initialization, or whether representational power is in fact lost from not learning
the update rule. Intuitively, we might surmise that learning an update rule is more expressive than
simply learning an initialization for gradient descent. In this paper, we seek to answer the follow-
ing question: does simply learning the initial parameters of a deep neural network have the same
representational power as arbitrarily expressive meta-learners that directly ingest the training data
at meta-test time? Or, more concisely, does representation combined with standard gradient descent
have sufﬁcient capacity to constitute any learning algorithm?

1

