Published as a conference paper at ICLR 2018

SEMANTIC INTERPOLATION IN IMPLICIT MODELS

Yannic Kilcher, Aur´elien Lucchi, Thomas Hofmann
Department of Computer Science
ETH Zurich
{yannic.kilcher,aurelien.lucchi,thomas.hofmann}@inf.ethz.ch

ABSTRACT

In implicit models, one often interpolates between sampled points in latent space.
As we show in this paper, care needs to be taken to match-up the distributional
assumptions on code vectors with the geometry of the interpolating paths. Oth-
erwise, typical assumptions about the quality and semantics of in-between points
may not be justiﬁed. Based on our analysis we propose to modify the prior code
distribution to put signiﬁcantly more probability mass closer to the origin. As a
result, linear interpolation paths are not only shortest paths, but they are also guar-
anteed to pass through high-density regions, irrespective of the dimensionality of
the latent space. Experiments on standard benchmark image datasets demonstrate
clear visual improvements in the quality of the generated samples and exhibit more
meaningful interpolation paths.

1

INTRODUCTION

Continuous latent variable models have been developed and studied in statistics for almost a cen-
tury, with factor analysis (Young (1941); Bartholomew (1987)) being the most paradigmatic and
widespread model family. In the neural network community, autoencoders have been used to ﬁnd
low-dimensional codes with low reconstruction error (Baldi & Hornik (1989); DeMers & Cottrell
(1993)). Recently there has been an increased interest in implict models, where a complex gener-
ative mechanism is driven by a source of randomness (cf. MacKay (1995)). This includes popular
architectures known as Variational Autoencoders (VAEs, Kingma & Welling (2013); Rezende et al.
(2014)) as well as Generative Adversarial Networks (GANs, Goodfellow et al. (2014)). Typically,
one deﬁnes a deterministic mechanism or generator Gφ : Rd → Rm, z (cid:55)→ Gφ(z) = x, parametrized
by φ and often implemented as a deep neural network (DNN). This map is then hooked up to a code
distribution z ∼ Pz, to induce a distribution x ∼ Px.
It is known that under mild regularity
conditions, by a suitable choice of generator, any Px can be obtained from an arbitrary ﬁxed Pz
(cf. Kallenberg (2006)). Relying on the representational power and ﬂexibility of DNNs, this has led
to the view that code distributions should be simple, e.g. most commonly Pz = N (0, I). Implicit
models are essentially sampling devices that can be trained and used without explicit access to the
densities they deﬁne. They have shown great promise in producing samples that are perceptually in-
distinguishable from samples generated by nature (Radford et al. (2015)) and are currently a subject
of extensive investigation.
Earlier work on embedding models such as the word embeddings of Mikolov et al. (2013a), has
shown how semantic relations and analogies are naturally captured by the afﬁne structure of embed-
dings (Mikolov et al. (2013c); Levy & Goldberg (2014)). This has inspired the use of afﬁne vector
arithmetic and linear interpolation in implicit models such as GANs, where it has shown to lead to
semantic interpolations in image space (cf. Radford et al. (2015); White (2016)). These traversal
experiments have also been used to justify that deep generative models do not only memorize the
training data, but do learn models that generalize to unseen data. However, as pointed out by White
(2016), the commonly used linear interpolation has one major ﬂaw in that it ignores the manifold
structure of the latent space. Indeed, traversing the latent space along straight lines may lead through
low-density regions, where – by deﬁnition – the generator has not been trained (well). This is easy
to understand as for z ∼ N (0, I) we get that

(cid:20)(cid:107)z(cid:107)2

(cid:21)

(cid:107)z(cid:107)2 ∼ χ2(d),

and hence E(cid:107)z(cid:107)2 = d, Var

1

=

2
d

.

(1)

d

