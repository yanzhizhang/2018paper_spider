Published as a conference paper at ICLR 2018

A DEEP REINFORCED MODEL FOR ABSTRACTIVE
SUMMARIZATION

Romain Paulus, Caiming Xiong∗& Richard Socher
Salesforce Research
575 High Street
Palo Alto, CA 94301, USA
{rpaulus,cxiong,rsocher}@salesforce.com

ABSTRACT

Attentional, RNN-based encoder-decoder models for abstractive summarization
have achieved good performance on short input and output sequences. For longer
documents and summaries however these models often include repetitive and
incoherent phrases. We introduce a neural network model with a novel intra-
attention that attends over the input and continuously generated output separately,
and a new training method that combines standard supervised word prediction and
reinforcement learning (RL). Models trained only with supervised learning often
exhibit “exposure bias” – they assume ground truth is provided at each step during
training. However, when standard word prediction is combined with the global se-
quence prediction training of RL the resulting summaries become more readable.
We evaluate this model on the CNN/Daily Mail and New York Times datasets.
Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an
improvement over previous state-of-the-art models. Human evaluation also shows
that our model produces higher quality summaries.

1

INTRODUCTION

Text summarization is the process of automatically generating natural language summaries from an
input document while retaining the important points. By condensing large quantities of information
into short, informative summaries, summarization can aid many downstream applications such as
creating news digests, search, and report generation.
There are two prominent types of summarization algorithms. First, extractive summarization sys-
tems form summaries by copying parts of the input (Dorr et al., 2003; Nallapati et al., 2017). Second,
abstractive summarization systems generate new phrases, possibly rephrasing or using words that
were not in the original text (Chopra et al., 2016; Nallapati et al., 2016).
Neural network models (Nallapati et al., 2016) based on the attentional encoder-decoder model for
machine translation (Bahdanau et al., 2015) were able to generate abstractive summaries with high
ROUGE scores. However, these systems have typically been used for summarizing short input
sequences (one or two sentences) to generate even shorter summaries. For example, the summaries
on the DUC-2004 dataset generated by the state-of-the-art system by Zeng et al. (2016) are limited
to 75 characters.
Nallapati et al. (2016) also applied their abstractive summarization model on the CNN/Daily Mail
dataset (Hermann et al., 2015), which contains input sequences of up to 800 tokens and multi-
sentence summaries of up to 100 tokens. But their analysis illustrates a key problem with attentional
encoder-decoder models: they often generate unnatural summaries consisting of repeated phrases.
We present a new abstractive summarization model that achieves state-of-the-art results on the
CNN/Daily Mail and similarly good results on the New York Times dataset (NYT) (Sandhaus,
2008). To our knowledge, this is the ﬁrst end-to-end model for abstractive summarization on the
NYT dataset. We introduce a key attention mechanism and a new learning objective to address the

∗Corresponding author.

1

