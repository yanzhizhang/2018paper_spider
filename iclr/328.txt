Published as a conference paper at ICLR 2018

SPATIALLY TRANSFORMED ADVERSARIAL EXAMPLES

Chaowei Xiao1 ∗, Jun-Yan Zhu2 ∗, Bo Li3, Warren He3, Mingyan Liu1, Dawn Song3
1University of Michigan, Ann Arbor, USA
2Massachusetts Institute of Technology, MA, USA
3University of California, Berkeley, USA

ABSTRACT

Recent studies show that widely used deep neural networks (DNNs) are vulner-
able to carefully crafted adversarial examples. Many advanced algorithms have
been proposed to generate adversarial examples by leveraging the Lp distance for
penalizing perturbations. Researchers have explored different defense methods to
defend against such adversarial attacks. While the effectiveness of Lp distance
as a metric of perceptual quality remains an active research area, in this paper
we will instead focus on a different type of perturbation, namely spatial transfor-
mation, as opposed to manipulating the pixel values directly as in prior works.
Perturbations generated through spatial transformation could result in large Lp
distance measures, but our extensive experiments show that such spatially trans-
formed adversarial examples are perceptually realistic and more difﬁcult to defend
against with existing defense systems. This potentially provides a new direction
in adversarial example generation and the design of corresponding defenses. We
visualize the spatial transformation based perturbation for different examples and
show that our technique can produce realistic adversarial examples with smooth
image deformation. Finally, we visualize the attention of deep networks with dif-
ferent types of adversarial examples to better understand how these examples are
interpreted.

1

INTRODUCTION

Deep neural networks (DNNs) have demonstrated their outstanding performance in different do-
mains, ranging from image processing (Krizhevsky et al., 2012; He et al., 2016), text analysis (Col-
lobert & Weston, 2008) to speech recognition (Hinton et al., 2012). Though deep networks have
exhibited high performance for these tasks, recently they have been shown to be particularly vul-
nerable to adversarial perturbations added to the input images (Szegedy et al., 2013; Goodfellow
et al., 2015). These perturbed instances are called adversarial examples, which can lead to un-
desirable consequences in many practical applications based on DNNs. For example, adversarial
examples can be used to subvert malware detection, fraud detection, or even potentially mislead au-
tonomous navigation systems (Papernot et al., 2016b; Evtimov et al., 2017; Grosse et al., 2016) and
therefore pose security risks when applied to security-related applications. A comprehensive study
about adversarial examples is required to motivate effective defenses. Different methods have been
proposed to generate adversarial examples such as fast gradient sign methods (FGSM) (Goodfel-
low et al., 2015), which can produce adversarial instances rapidly, and optimization-based methods
(C&W) (Carlini & Wagner, 2017a), which search for adversarial examples with smaller magnitude
of perturbation.
One important criterion for adversarial examples is that the perturbed images should “look like”
the original instances. The traditional attack strategies adopt L2 (or other Lp) norm distance as a
perceptual similarity metric to evaluate the distortion (Gu & Rigazio, 2014). However, this is not
an ideal metric (Johnson et al., 2016; Isola et al., 2017), as L2 similarity is sensitive to lighting and
viewpoint change of a pictured object. For instance, an image can be shifted by one pixel, which will
lead to large L2 distance, while the translated image actually appear “the same” to human perception.
Motivated by this example, in this paper we aim to look for other types of adversarial examples
and propose to create perceptually realistic examples by changing the positions of pixels instead

∗indicates equal contributions

1

