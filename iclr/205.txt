Under review as a conference paper at ICLR 2018

COUNTERING ADVERSARIAL IMAGES
USING INPUT TRANSFORMATIONS

Anonymous authors
Paper under double-blind review

ABSTRACT

This paper investigates strategies that defend against adversarial-example attacks
on image-classiﬁcation systems by transforming the inputs before feeding them
to the system. Speciﬁcally, we study applying image transformations such as
bit-depth reduction, JPEG compression, total variance minimization, and image
quilting before feeding the image to a convolutional network classiﬁer. Our ex-
periments on ImageNet show that total variance minimization and image quilting
are very effective defenses in practice, in particular, when the network is trained on
transformed images. The strength of those defenses lies in their non-differentiable
nature and their inherent randomness, which makes it difﬁcult for an adversary to
circumvent the defenses. Our best defense eliminates 60% of strong gray-box and
90% of strong black-box attacks by a variety of major attack methods.

1

INTRODUCTION

As the use of machine intelligence increases in security-sensitive applications (Bojarski et al., 2016;
Amodei et al., 2015), robustness has become a critical feature to guarantee the reliability of deployed
machine-learning systems. Unfortunately, recent research has demonstrated that existing models are
not robust to small, adversarially designed perturbations of the input (Biggio et al., 2013; Szegedy
et al., 2014; Goodfellow et al., 2015; Kurakin et al., 2016a; Cisse et al., 2017a). Adversarially
perturbed examples have been deployed to attack image classiﬁcation services (Liu et al., 2016),
speech recognition systems (Cisse et al., 2017a), and robot vision (Melis et al., 2017). The existence
of these adversarial examples has motivated proposals for approaches that increase the robustness of
learning systems to such examples (Papernot et al., 2016; Kurakin et al., 2016a; Cisse et al., 2017b).
The robustness of machine learning models to adversarial examples depends both on the properties
of the model (i.e., Lipschitzness) and on the nature of the problem considered, e.g., on the input di-
mensionality and the Bayes error of the problem (Fawzi et al., 2015; 2016). Consequently, defenses
that aim to increase robustness against adversarial examples fall in one of two main categories. The
ﬁrst category comprises model-speciﬁc strategies that enforce model properties such as invariance
and smoothness via the learning algorithm or regularization scheme (Shaham et al., 2015; Kurakin
et al., 2016a; Cisse et al., 2017b), potentially exploiting knowledge about the adversary’s attack
strategy (Goodfellow et al., 2015). The second category of defenses are model-agnostic: they try
to remove adversarial perturbations from the input. For example, in the context of image classiﬁca-
tion, adversarial perturbations can be partly removed via JPEG compression (Dziugaite et al., 2016)
or image re-scaling (Lu et al., 2017). Hitherto, none of these defenses has been shown to be very
effective. Speciﬁcally, model-agnostic defenses appear too simple to sufﬁciently remove adversar-
ial perturbations from input images. By contrast, model-speciﬁc defenses make strong assumptions
about the nature of the adversary (e.g., on the norm that the adversary minimizes or on the number of
iterations it uses to generate the perturbation). Consequently, they do not satisfy Kerckhoffs (1883)
principle: the adversary can alter its attack to circumvent such model-speciﬁc defenses.
In this paper, we focus on increasing the effectiveness of model-agnostic defense strategies by de-
veloping approaches that (1) remove the adversarial perturbations from input images, (2) maintain
sufﬁcient information in input images to correctly classify them, and (3) are still effective in settings
in which the adversary has information on the defense strategy being used. We explore transforma-
tions based on image cropping and rescaling (Graese et al., 2016), bit-depth reduction (Xu et al.,
2017), JPEG compression (Dziugaite et al., 2016), total variance minimization (Rudin et al., 1992),

1

