TRAINING GANS WITH OPTIMISM

Constantinos Daskalakis∗
MIT, EECS
costis@mit.edu

Andrew Ilyas∗
MIT, EECS
ailyas@mit.edu

Vasilis Syrgkanis∗
Microsoft Research
vasy@microsoft.com

Haoyang Zeng∗
MIT, EECS
haoyangz@mit.edu

ABSTRACT

We address the issue of limit cycling behavior in training Generative Adversarial
Networks and propose the use of Optimistic Mirror Decent (OMD) for training
Wasserstein GANs. Recent theoretical results have shown that optimistic mirror
decent (OMD) can enjoy faster regret rates in the context of zero-sum games.
WGANs is exactly a context of solving a zero-sum game with simultaneous no-
regret dynamics. Moreover, we show that optimistic mirror decent addresses the
limit cycling problem in training WGANs. We formally show that in the case of
bi-linear zero-sum games the last iterate of OMD dynamics converges to an equi-
librium, in contrast to GD dynamics which are bound to cycle. We also portray the
huge qualitative difference between GD and OMD dynamics with toy examples,
even when GD is modiﬁed with many adaptations proposed in the recent litera-
ture, such as gradient penalty or momentum. We apply OMD WGAN training to
a bioinformatics problem of generating DNA sequences. We observe that mod-
els trained with OMD achieve consistently smaller KL divergence with respect to
the true underlying distribution, than models trained with GD variants. Finally,
we introduce a new algorithm, Optimistic Adam, which is an optimistic variant
of Adam. We apply it to WGAN training on CIFAR10 and observe improved
performance in terms of inception score as compared to Adam.

1

INTRODUCTION

Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have proven a very successful
approach for ﬁtting generative models in complex structured spaces, such as distributions over im-
ages. GANs frame the question of ﬁtting a generative model from a data set of samples from some
distribution as a zero-sum game between a Generator (G) and a discriminator (D). The Generator is
represented as a deep neural network which takes as input random noise and outputs a sample in the
same space of the sampled data set, trying to approximate a sample from the underlying distribution
of data. The discriminator, also modeled as a deep neural network is attempting to discriminate be-
tween a true sample and a sample generated by the generator. The hope is that at the equilibrium of
this zero-sum game the generator will learn to generate samples in a manner that is indistinguishable
from the true samples and hence has essentially learned the underlying data distribution.
Despite their success at generating visually appealing samples when applied to image generation
tasks, GANs are very ﬁnicky to train. One particular problem, raised for instance in a recent survey
as a major issue (Goodfellow, 2017) is the instability of the training process. Typically training of
GANs is achieved by solving the zero-sum game via running simultaneously a variant of a Stochastic
Gradient Descent algorithm for both players (potentially training the discriminator more frequently
than the generator).

training

1Code for our models is available at https://github.com/vsyrgkanis/optimistic_GAN_
∗These authors contribute equally to this work.

1

