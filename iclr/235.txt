Published as a conference paper at ICLR 2018

DEEP AUTOENCODING GAUSSIAN MIXTURE MODEL
FOR UNSUPERVISED ANOMALY DETECTION

Bo Zong†, Qi Song‡, Martin Renqiang Min†, Wei Cheng†
Cristian Lumezanu†, Daeki Cho†, Haifeng Chen†
†NEC Laboratories America
‡Washington State University, Pullman
{bzong, renqiang, weicheng, lume, dkcho, haifeng}@nec-labs.com
qsong@eecs.wsu.edu

ABSTRACT

Unsupervised anomaly detection on multi- or high-dimensional data is of great
importance in both fundamental machine learning research and industrial applica-
tions, for which density estimation lies at the core. Although previous approaches
based on dimensionality reduction followed by density estimation have made
fruitful progress, they mainly suffer from decoupled model learning with incon-
sistent optimization goals and incapability of preserving essential information
in the low-dimensional space. In this paper, we present a Deep Autoencoding
Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our
model utilizes a deep autoencoder to generate a low-dimensional representation and
reconstruction error for each input data point, which is further fed into a Gaussian
Mixture Model (GMM). Instead of using decoupled two-stage training and the
standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes
the parameters of the deep autoencoder and the mixture model simultaneously in
an end-to-end fashion, leveraging a separate estimation network to facilitate the
parameter learning of the mixture model. The joint optimization, which well bal-
ances autoencoding reconstruction, density estimation of latent representation, and
regularization, helps the autoencoder escape from less attractive local optima and
further reduce reconstruction errors, avoiding the need of pre-training. Experimen-
tal results on several public benchmark datasets show that, DAGMM signiﬁcantly
outperforms state-of-the-art anomaly detection techniques, and achieves up to 14%
improvement based on the standard F1 score.

1

INTRODUCTION

Unsupervised anomaly detection is a fundamental problem in machine learning, with critical applica-
tions in many areas, such as cybersecurity (Tan et al. (2011)), complex system management (Liu et al.
(2008)), medical care (Keller et al. (2012)), and so on. At the core of anomaly detection is density
estimation: given a lot of input samples, anomalies are those ones residing in low probability density
areas.
Although fruitful progress has been made in the last several years, conducting robust anomaly
detection on multi- or high-dimensional data without human supervision remains a challenging
task. Especially, when the dimensionality of input data becomes higher, it is more difﬁcult to
perform density estimation in the original feature space, as any input sample could be a rare event
with low probability to observe (Chandola et al. (2009)). To address this issue caused by the
curse of dimensionality, two-step approaches are widely adopted (Cand`es et al. (2011)), in which
dimensionality reduction is ﬁrst conducted, and then density estimation is performed in the latent
low-dimensional space. However, these approaches could easily lead to suboptimal performance,
because dimensionality reduction in the ﬁrst step is unaware of the subsequent density estimation
task, and the key information for anomaly detection could be removed in the ﬁrst place. Therefore, it
is desirable to combine the force of dimensionality reduction and density estimation, although a joint
optimization accounting for these two components is usually computationally difﬁcult. Several recent

1

