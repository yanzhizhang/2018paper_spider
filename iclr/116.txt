FUSIONNET: FUSING VIA FULLY-AWARE ATTENTION
WITH APPLICATION TO MACHINE COMPREHENSION

Hsin-Yuan Huang*1,2, Chenguang Zhu1, Yelong Shen1, Weizhu Chen1
1Microsoft Business AI and Research
2National Taiwan University
momohuang@gmail.com, {chezhu,yeshen,wzchen}@microsoft.com

ABSTRACT

This paper introduces a new neural structure called FusionNet, which extends ex-
isting attention approaches from three perspectives. First, it puts forward a novel
concept of “history of word” to characterize attention information from the lowest
word-level embedding up to the highest semantic-level representation. Second, it
identiﬁes an attention scoring function that better utilizes the “history of word”
concept. Third, it proposes a fully-aware multi-level attention mechanism to cap-
ture the complete information in one text (such as a question) and exploit it in its
counterpart (such as context or passage) layer by layer. We apply FusionNet to the
Stanford Question Answering Dataset (SQuAD) and it achieves the ﬁrst position
for both single and ensemble model on the ofﬁcial SQuAD leaderboard at the time
of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of Fusion-
Net with two adversarial SQuAD datasets and it sets up the new state-of-the-art
on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6%
to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to
60.7%.

1

INTRODUCTION

Teaching machines to read, process and comprehend
text and then answer questions is one of key prob-
lems in artiﬁcial intelligence. Figure 1 gives an ex-
ample of the machine reading comprehension task.
It feeds a machine with a piece of context and a ques-
tion and teaches it to ﬁnd a correct answer to the
question. This requires the machine to possess high
capabilities in comprehension, inference and reason-
ing. This is considered a challenging task in artiﬁcial
intelligence and has already attracted numerous re-
search efforts from the neural network and natural
language processing communities. Many neural net-
work models have been proposed for this challenge
and they generally frame this problem as a machine
reading comprehension (MRC) task (Hochreiter &
Schmidhuber, 1997; Wang et al., 2017; Seo et al.,
2017; Shen et al., 2017; Xiong et al., 2017; Weis-
senborn et al., 2017; Chen et al., 2017a).
The key innovation in recent models lies in how to ingest information in the question and character-
ize it in the context, in order to provide an accurate answer to the question. This is often modeled
as attention in the neural network community, which is a mechanism to attend the question into the
context so as to ﬁnd the answer related to the question. Some (Chen et al., 2017a; Weissenborn et al.,
2017) attend the word-level embedding from the question to context, while some (Wang et al., 2017)
attend the high-level representation in the question to augment the context. However we observed

Figure 1: Question-answer pair for a passage
discussing Alpine Rhine.

*Most of the work was done during internship at Microsoft, Redmond.

1

Context: The Alpine Rhine is part of the Rhine, a famous European river. The Alpine Rhine begins in the most western part of the Swiss canton of Graubünden, and later forms the border between Switzerland to the West and Liechtenstein and later Austria to the East. On the other hand, the Danube separates Romania and Bulgaria.Question: What is the other country the Rhine separates Switzerland to?Answer: Liechtenstein