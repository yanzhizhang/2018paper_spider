Published as a conference paper at ICLR 2018

IMPROVING GAN TRAINING VIA
BINARIZED REPRESENTATION ENTROPY (BRE)
REGULARIZATION

Yanshuai Cao, Gavin Weiguang Ding, Kry Yik-Chau Lui, Ruitong Huang
Borealis AI
Canada

ABSTRACT

We propose a novel regularizer to improve the training of Generative Adversarial
Networks (GANs). The motivation is that when the discriminator D spreads out its
model capacity in the right way, the learning signals given to the generator G are
more informative and diverse. These in turn help G to explore better and discover
the real data manifold while avoiding large unstable jumps due to the erroneous
extrapolation made by D . Our regularizer guides the rectiﬁer discriminator D to
better allocate its model capacity, by encouraging the binary activation patterns on
selected internal layers of D to have a high joint entropy. Experimental results on
both synthetic data and real datasets demonstrate improvements in stability and
convergence speed of the GAN training, as well as higher sample quality. The ap-
proach also leads to higher classiﬁcation accuracies in semi-supervised learning.

1

INTRODUCTION

Generative Adversarial Network (GAN) (Goodfellow et al., 2014) has been a new promising ap-
proach to unsupervised learning of complex high dimensional data in the last two years, with suc-
cessful applications on image data (Isola et al., 2016; Shrivastava et al., 2016), and high potential
for predictive representation learning (Mathieu et al., 2015) as well as reinforcement learning (Finn
et al., 2016; Henderson et al., 2017). In a nutshell, GANs learn from unlabeled data by engaging the
generative model (G ) in an adversarial game with a discriminator (D ). D learns to tell apart fake
data generated by G from real data, while G learns to fool D , having access to D ’s input gradient.

Despite its success in generating high-quality data, such adversarial game setting also raises chal-
lenges for the training of GANs. Many architectures and techniques have been proposed (Radford
et al., 2015; Salimans et al., 2016; Gulrajani et al., 2017) to reduce extreme failures and improve
the sample quality of generated data. However, many theoretical and practical open problems still
remain, which have impeded the ease-of-use of GANs in new problems. In particular, G often fails
to capture certain variation or modes in the real data distribution, while D fails to exploit this failure
to provide better training signal for G, leading to subtle mode collapse. Recently Arora et al. (2017)
showed that the capacity of D plays an essential role in giving G sufﬁcient learning guidances to
model the complex real data distribution. With insufﬁcient capacity, D could fail to distinguish real
and generated data distributions even when their Jensen-Shannon divergence or Wasserstein distance
is not small.

In this work, we demonstrate that even with sufﬁcient maximum capacity, D might not allocate its
capacity in a desirable way that facilitates convergence to a good equilibrium. We then propose
a novel regularizer to guide D to have a better model capacity allocation. Our regularizer is con-
structed to encourage D ’s hidden binary activation patterns to have high joint entropy, based on a
connection between the model capacity of a rectiﬁer net and its internal binary activation patterns.
Our experiments show that such high entropy representation leads to faster convergences, improved
sample quality, as well as lower errors in semi-supervised learning.

1

