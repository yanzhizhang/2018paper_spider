Published as a conference paper at ICLR 2018

DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES

Jaehoon Lee∗†, Yasaman Bahri∗†, Roman Novak , Samuel S. Schoenholz,
Jeffrey Pennington, Jascha Sohl-Dickstein

Google Brain
{jaehlee, yasamanb, romann, schsam, jpennin, jaschasd}@google.com

ABSTRACT

It has long been known that a single-layer fully-connected neural network with an
i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit
of inﬁnite network width. This correspondence enables exact Bayesian inference
for inﬁnite width neural networks on regression tasks by means of evaluating the
corresponding GP. Recently, kernel functions which mimic multi-layer random
neural networks have been developed, but only outside of a Bayesian framework.
As such, previous work has not identiﬁed that these kernels can be used as co-
variance functions for GPs and allow fully Bayesian prediction with a deep neural
network.
In this work, we derive the exact equivalence between inﬁnitely wide deep net-
works and GPs. We further develop a computationally efﬁcient pipeline to com-
pute the covariance function for these GPs. We then use the resulting GPs to per-
form Bayesian inference for wide deep neural networks on MNIST and CIFAR-
10. We observe that trained neural network accuracy approaches that of the corre-
sponding GP with increasing layer width, and that the GP uncertainty is strongly
correlated with trained network prediction error. We further ﬁnd that test perfor-
mance increases as ﬁnite-width trained networks are made wider and more similar
to a GP, and thus that GP predictions typically outperform those of ﬁnite-width
networks. Finally we connect the performance of these GPs to the recent theory
of signal propagation in random neural networks.

1

INTRODUCTION

Deep neural networks have emerged in recent years as ﬂexible parametric models which can ﬁt
complex patterns in data. As a contrasting approach, Gaussian processes have long served as a
traditional nonparametric tool for modeling. An equivalence between these two approaches was
derived in Neal (1994a), for the case of one layer networks in the limit of inﬁnite width. Neal
(1994a) further suggested that a similar correspondence might hold for deeper networks.
Consider a deep fully-connected neural network with i.i.d. random parameters. Each scalar output
of the network, an afﬁne transformation of the ﬁnal hidden layer, will be a sum of i.i.d. terms. As we
will discuss in detail below, in the limit of inﬁnite width the Central Limit Theorem1 implies that the
function computed by the neural network (NN) is a function drawn from a Gaussian process (GP).
In the case of single hidden-layer networks, the form of the kernel of this GP is well known (Neal
(1994a); Williams (1997)).
This correspondence implies that if we choose the hypothesis space to be the class of inﬁnitely
wide neural networks, an i.i.d. prior over weights and biases can be replaced with a corresponding
GP prior over functions. As noted by (Williams, 1997), this substitution enables exact Bayesian
inference for regression using neural networks. The computation requires building the necessary
covariance matrices over the training and test sets and straightforward linear algebra computations.

∗Both authors contributed equally to this work.
†Work done as a member of the Google AI Residency program (g.co/airesidency).
1Throughout this paper, we assume the conditions on the parameter distributions and nonlinearities are such
that the Central Limit Theorem will hold; for instance, that the weight variance is scaled inversely proportional
to the layer width.

1

