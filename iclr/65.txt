Published as a conference paper at ICLR 2018

REINFORCEMENT LEARNING ALGORITHM SELECTION

Romain Laroche1 and Raphaël Féraud2

1 Microsoft Research, Montréal, Canada

2 Orange Labs, Lannion, France

ABSTRACT

This paper formalises the problem of online algorithm selection in the context of
Reinforcement Learning (RL). The setup is as follows: given an episodic task and
a ﬁnite number of off-policy RL algorithms, a meta-algorithm has to decide which
RL algorithm is in control during the next episode so as to maximize the expected
return. The article presents a novel meta-algorithm, called Epochal Stochastic
Bandit Algorithm Selection (ESBAS). Its principle is to freeze the policy updates
at each epoch, and to leave a rebooted stochastic bandit in charge of the algorithm
selection. Under some assumptions, a thorough theoretical analysis demonstrates
its near-optimality considering the structural sampling budget limitations. ESBAS
is ﬁrst empirically evaluated on a dialogue task where it is shown to outperform
each individual algorithm in most conﬁgurations. ESBAS is then adapted to a true
online setting where algorithms update their policies after each transition, which
we call SSBAS. SSBAS is evaluated on a fruit collection task where it is shown to
adapt the stepsize parameter more efﬁciently than the classical hyperbolic decay,
and on an Atari game, where it improves the performance by a wide margin.

1

INTRODUCTION

Reinforcement Learning (RL, Sutton & Barto (1998)) is a machine learning framework for optimising
the behaviour of an agent interacting with an unknown environment. For the most practical problems,
such as dialogue or robotics, trajectory collection is costly and sample efﬁciency is the main key
performance indicator. Consequently, when applying RL to a new problem, one must carefully choose
in advance a model, a representation, an optimisation technique and their parameters. Facing the
complexity of choice, RL and domain expertise is not sufﬁcient. Confronted to the cost of data, the
popular trial and error approach shows its limits.
We develop an online learning version (Gagliolo & Schmidhuber, 2006; 2010) of Algorithm Selection
(AS, Rice (1976); Smith-Miles (2009); Kotthoff (2012)). It consists in testing several algorithms on
the task and in selecting the best one at a given time. For clarity, throughout the whole article, the
algorithm selector is called a meta-algorithm, and the set of algorithms available to the meta-algorithm
is called a portfolio. The meta-algorithm maximises an objective function such as the RL return.
Beyond the sample efﬁciency objective, the online AS approach besides addresses four practical
problems for online RL-based systems. First, it improves robustness: if an algorithm fails to terminate,
or outputs to an aberrant policy, it will be dismissed and others will be selected instead. Second,
convergence guarantees and empirical efﬁciency may be united by covering the empirically efﬁcient
algorithms with slower algorithms that have convergence guarantees. Third, it enables curriculum
learning: shallow models control the policy in the early stages, while deep models discover the best
solution in late stages. And four, it allows to deﬁne an objective function that is not an RL return.
A fair algorithm selection implies a fair budget allocation between the algorithms, so that they can
be equitably evaluated and compared. In order to comply with this requirement, the reinforcement
algorithms in the portfolio are assumed to be off-policy, and are trained on every trajectory, regardless
which algorithm controls it. Section 2 provides a unifying view of RL algorithms, that allows infor-
mation sharing between algorithms, whatever their state representations and optimisation techniques.
It also formalises the problem of online selection of off-policy RL algorithms.

1

