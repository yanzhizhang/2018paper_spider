Published as a conference paper at ICLR 2018

SCALABLE PRIVATE LEARNING WITH PATE

Nicolas Papernot∗
Pennsylvania State University
ngp5056@cse.psu.edu

Shuang Song∗
University of California San Diego
shs037@eng.ucsd.edu

Ilya Mironov, Ananth Raghunathan, Kunal Talwar & Úlfar Erlingsson
Google Brain
{mironov,pseudorandom,kunal,ulfar}@google.com

ABSTRACT

The rapid adoption of machine learning has increased concerns about the privacy
implications of machine learning models trained on sensitive data, such as medical
records or other personal information. To address those concerns, one promising
approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers
to a “student” model the knowledge of an ensemble of “teacher” models, with
intuitive privacy provided by training teachers on disjoint data and strong privacy
guaranteed by noisy aggregation of teachers’ answers. However, PATE has so far
been evaluated only on simple classiﬁcation tasks like MNIST, leaving unclear its
utility when applied to larger-scale learning tasks and real-world datasets.
In this work, we show how PATE can scale to learning tasks with large numbers
of output classes and uncurated, imbalanced training data with errors. For this, we
introduce new noisy aggregation mechanisms for teacher ensembles that are more
selective and add less noise, and prove their tighter differential-privacy guarantees.
Our new mechanisms build on two insights: the chance of teacher consensus is
increased by using more concentrated noise and, lacking consensus, no answer
need be given to a student. The consensus answers used are more likely to be
correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our
evaluation shows our mechanisms improve on the original PATE on all measures,
and scale to larger tasks with both high utility and very strong privacy (ε < 1.0).

1

INTRODUCTION

Many attractive applications of modern machine-learning techniques involve training models using
highly sensitive data. For example, models trained on people’s personal messages or detailed med-
ical information can offer invaluable insights into real-world language usage or the diagnoses and
treatment of human diseases (McMahan et al., 2017; Liu et al., 2017). A key challenge in such
applications is to prevent models from revealing inappropriate details of the sensitive data—a non-
trivial task, since models are known to implicitly memorize such details during training and also to
inadvertently reveal them during inference (Zhang et al., 2017; Shokri et al., 2017).
Recently, two promising, new model-training approaches have offered the hope that practical, high-
utility machine learning may be compatible with strong privacy-protection guarantees for sensitive
training data (Abadi et al., 2017). This paper revisits one of these approaches, Private Aggrega-
tion of Teacher Ensembles, or PATE (Papernot et al., 2017), and develops techniques that improve
its scalability and practical applicability. PATE has the advantage of being able to learn from the
aggregated consensus of separate “teacher” models trained on disjoint data, in a manner that both
provides intuitive privacy guarantees and is agnostic to the underlying machine-learning techniques
(cf. the approach of differentially-private stochastic gradient descent (Abadi et al., 2016)). In the
PATE approach multiple teachers are trained on disjoint sensitive data (e.g., different users’ data),
and uses the teachers’ aggregate consensus answers in a black-box fashion to supervise the training
of a “student” model. By publishing only the student model (keeping the teachers private) and by
adding carefully-calibrated Laplacian noise to the aggregate answers used to train the student, the
∗Equal contributions, authors ordered alphabetically. Work done while the authors were at Google Brain.

1

