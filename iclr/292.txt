Published as a conference paper at ICLR 2018

TRUNCATED HORIZON POLICY SEARCH: COMBINING
REINFORCEMENT LEARNING & IMITATION LEARNING

Wen Sun
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA, USA
wensun@cs.cmu.edu

J. Andrew Bagnell
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA, USA
dbagnell@cs.cmu.edu

Byron Boots
School of Interactive Computing
Georgia Institute of Technology
Atlanta, GA, USA
bboots@cc.gatech.edu

ABSTRACT

In this paper, we propose to combine imitation and reinforcement learning via the
idea of reward shaping using an oracle. We study the effectiveness of the near-
optimal cost-to-go oracle on the planning horizon and demonstrate that the cost-
to-go oracle shortens the learner’s planning horizon as function of its accuracy: a
globally optimal oracle can shorten the planning horizon to one, leading to a one-
step greedy Markov Decision Process which is much easier to optimize, while an
oracle that is far away from the optimality requires planning over a longer horizon
to achieve near-optimal performance. Hence our new insight bridges the gap and
interpolates between imitation learning and reinforcement learning. Motivated
by the above mentioned insights, we propose Truncated HORizon Policy Search
(THOR), a method that focuses on searching for policies that maximize the total
reshaped reward over a ﬁnite planning horizon when the oracle is sub-optimal. We
experimentally demonstrate that a gradient-based implementation of THOR can
achieve superior performance compared to RL baselines and IL baselines even
when the oracle is sub-optimal.

1

INTRODUCTION

Reinforcement Learning (RL), equipped with modern deep learning techniques, has dramatically ad-
vanced the state-of-the-art in challenging sequential decision problems including high-dimensional
robotics control tasks as well as video and board games (Mnih et al., 2015; Silver et al., 2016). How-
ever, these approaches typically require a large amount of training data and computational resources
to succeed. In response to these challenges, researchers have explored strategies for making RL
more efﬁcient by leveraging additional information to guide the learning process. Imitation learning
(IL) is one such approach. In IL, the learner can reference expert demonstrations (Abbeel & Ng,
2004), or can access a cost-to-go oracle (Ross & Bagnell, 2014), providing additional information
about the long-term effects of learner decisions. Through these strategies, imitation learning lowers
sample complexity by reducing random global exploration. For example, Sun et al. (2017) shows
that, with access to an optimal expert, imitation learning can exponentially lower sample complexity
compared to pure RL approaches. Experimentally, researchers also have demonstrated sample efﬁ-
ciency by leveraging expert demonstrations by adding demonstrations into a replay buffer (Veˇcer´ık
et al., 2017; Nair et al., 2017), or mixing the policy gradient with a behavioral cloning-related gra-
dient (Rajeswaran et al., 2017).
Although imitating experts can speed up the learning process in RL tasks, the performance of the
learned policies are generally limited to the performance of the expert, which is often sub-optimal
in practice. Previous imitation learning approaches with strong theoretical guarantees such as Data
Aggregation (DAgger) (Ross et al., 2011) and Aggregation with Values (AGGREVATE) (Ross &
Bagnell, 2014) can only guarantee a policy which performs as well as the expert policy or a one-step
deviation improvement over the expert policy.1 Unfortunately, this implies that imitation learning
with a sub-optimal expert will often return a sub-optimal policy. Ideally, we want the best of both

1AGGREVATE achieves one-step deviation improvement over the expert under the assumption that the pol-

icy class is rich enough.

1

