Published as a conference paper at ICLR 2018

A NEURAL REPRESENTATION OF SKETCH DRAWINGS

David Ha
Google Brain
hadavid@google.com

Douglas Eck
Google Brain
deck@google.com

ABSTRACT

We present sketch-rnn, a recurrent neural network (RNN) able to construct
stroke-based drawings of common objects. The model is trained on a dataset of
human-drawn images representing many different classes. We outline a framework
for conditional and unconditional sketch generation, and describe new robust
training methods for generating coherent sketch drawings in a vector format.

1

INTRODUCTION

Recently, there have been major advancements in generative modelling of images using neural
networks as a generative tool. Generative Adversarial Networks (GANs) (Goodfellow, 2016),
Variational Inference (VI) (Kingma & Welling, 2013), and Autoregressive (AR) (Reed et al., 2017)
models have become popular tools in this fast growing area. Most of the work thus far has been
targeted towards modelling low resolution, pixel images. Humans, however, do not understand the
world as a grid of pixels, but rather develop abstract concepts to represent what we see. From a young
age, we develop the ability to communicate what we see by drawing on paper with a pencil or crayon.
In this way we learn to express a sequential, vector representation of an image as a short sequence of
strokes. In this paper we investigate an alternative to traditional pixel image modelling approaches,
and propose a generative model for vector images.

Figure 1: Latent space interpolation of various vector images produced by our model (left).

Interpolation of two different Kanji characters (→B) as sequence of strokes (right).

Our goal is to train machines to draw and generalize abstract concepts in a manner similar to humans.
In this work, as a ﬁrst step towards this goal, we train our model on a dataset of hand-drawn sketches,
each represented as a sequence of motor actions controlling a pen: which direction to move, when to
lift the pen up, and when to stop drawing. In doing so, we created a model that potentially has many
applications, from assisting the creative process of an artist, to helping teach students how to draw.
This paper makes the following contributions: We outline a framework for both unconditional and
conditional generation of vector images composed of a sequence of lines. Our recurrent neural
network-based generative model is capable of producing sketches of common objects in a vector
format. We develop a training procedure unique to vector images to make the training more robust. In
the conditional generation model, we explore the latent space developed by the model to represent a
vector image. We also discuss creative applications of our methodology. We make available a dataset
of 50 million hand drawn vector images to encourage further development of generative modelling
for vector images, and also release an implementation of our model as an open source project.1

1The code and dataset is available at https://magenta.tensorflow.org/sketch_rnn.

1

