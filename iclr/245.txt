Published as a conference paper at ICLR 2018

GLOBAL OPTIMALITY CONDITIONS FOR DEEP NEURAL
NETWORKS

Chulhee Yun, Suvrit Sra & Ali Jadbabaie
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{chulheey,suvrit,jadbabai}@mit.edu

ABSTRACT

We study the error landscape of deep linear and nonlinear neural networks with
the squared error loss. Minimizing the loss of a deep linear neural network is a
nonconvex problem, and despite recent progress, our understanding of this loss
surface is still incomplete. For deep linear networks, we present necessary and
sufﬁcient conditions for a critical point of the risk function to be a global mini-
mum. Surprisingly, our conditions provide an efﬁciently checkable test for global
optimality, while such tests are typically intractable in nonconvex optimization.
We further extend these results to deep nonlinear neural networks and prove sim-
ilar sufﬁcient conditions for global optimality, albeit in a more limited function
space setting.

INTRODUCTION

1
Since the advent of AlexNet (Krizhevsky et al., 2012), deep neural networks have surged in pop-
ularity, and have redeﬁned the state-of-the-art across many application areas of machine learning
and artiﬁcial intelligence, such as computer vision, speech recognition, and natural language pro-
cessing. However, a concrete theoretical understanding of why deep neural networks work well in
practice remains elusive. From the perspective of optimization, a signiﬁcant barrier is imposed by
the nonconvexity of training neural networks. Moreover, it was proved by Blum & Rivest (1988)
that training even a 3-node neural network to global optimality is NP-Hard in the worst case, so
there is little hope that neural networks have properties that make global optimization tractable.
Despite the difﬁculties of optimizing weights in neural networks, the empirical successes suggest
that the local minima of their loss surfaces could be close to global minima; and several papers have
recently appeared in the literature attempting to provide a theoretical justiﬁcation for the success
of these models. For example, by relating neural networks to spherical spin-glass models from
statistical physics, Choromanska et al. (2015) provided some empirical evidence that the increase of
size of neural networks makes local minima close to global minima.
Another line of results (Yu & Chen, 1995; Soudry & Carmon, 2016; Xie et al., 2016; Nguyen & Hein,
2017) provides conditions under which a critical point of the empirical risk is a global minimum.
Such results roughly involve proving that if full rank conditions of certain matrices (as well as some
additional technical conditions) are satisﬁed, derivative of the risk being zero implies loss being
zero. However, these results are obtained under restrictive assumptions; for example, Nguyen &
Hein (2017) require the width of one of the hidden layers to be as large as the number of training
examples. Soudry & Carmon (2016) and Xie et al. (2016) require the product of widths of two
adjacent layers to be at least as large as the number of training examples, meaning that the number
of parameters in the model must grow rapidly as we have more training data available. Another
recent paper (Haeffele & Vidal, 2017) provides a sufﬁcient condition for global optimality when the
neural network is composed of subnetworks with identical architectures connected in parallel and a
regularizer is designed to control the number of parallel architectures.
Towards obtaining a more precise characterization of the loss-surfaces, a valuable conceptual simpli-
ﬁcation of deep nonlinear networks is deep linear neural networks, in which all activation functions
are linear and the output of the entire network is a chained product of weight matrices with the
input vector. Although at ﬁrst sight a deep linear model may appear overly simplistic, even its opti-

1

