Published as a conference paper at ICLR 2018

PROGRESSIVE REINFORCEMENT LEARNING
WITH DISTILLATION
FOR MULTI-SKILLED MOTION CONTROL

Glen Berseth*, Cheng Xie*, Paul Cernek, Michiel Van de Panne
gberseth@cs.ubc.ca,cheng.k.xie@gmail.com,pcernek@cs.ubc.ca,
van@cs.ubc.ca
University of British Colubia

ABSTRACT

Deep reinforcement learning has demonstrated increasing capabilities for con-
tinuous control problems, including agents that can move with skill and agility
through their environment. An open problem in this setting is that of develop-
ing good strategies for integrating or merging policies for multiple skills, where
each individual skill is a specialist in a speciﬁc skill and its associated state dis-
tribution. We extend policy distillation methods to the continuous action setting
and leverage this technique to combine expert policies, as evaluated in the do-
main of simulated bipedal locomotion across different classes of terrain. We also
introduce an input injection method for augmenting an existing policy network
to exploit new input features. Lastly, our method uses transfer learning to assist
in the efﬁcient acquisition of new skills. The combination of these methods al-
lows a policy to be incrementally augmented with new skills. We compare our
progressive learning and integration via distillation (PLAID) method against three
alternative baselines.

1

INTRODUCTION

As they gain experience, humans develop rich repertoires of motion skills that are useful in differ-
ent contexts and environments. Recent advances in reinforcement learning provide an opportunity
to understand how motion repertoires can best be learned, recalled, and augmented. Inspired by
studies on the development and recall of movement patterns useful for different locomotion con-
texts (Roemmich & Bastian, 2015), we develop and evaluate an approach for learning multi-skilled
movement repertoires. In what follows, we refer to the proposed method as PLAID: Progressive
Learning and Integration via Distillation.
For long lived applications of complex control tasks a learning system may need to acquire and
integrate additional skills. Accordingly, our problem is deﬁned by the sequential acquisition and
integration of new skills. Given an existing controller that is capable of one-or-more skills, we wish
to: (a) efﬁciently learn a new skill or movement pattern in a way that is informed by the existing
control policy, and (b) to reintegrate that into a single controller that is capable of the full motion
repertoire. This process can then be repeated as necessary. We view PLAID as a continual learning
method, in that we consider a context where all tasks are not known in advance and we wish to
learn any new task in an efﬁcient manner. However, it is also proves surprisingly effective as a
multitask solution, given the three speciﬁc benchmarks that we compare against. In the process of
acquiring a new skill, we also allow for a control policy to be augmented with additional inputs,
without adversely impacting its performance. This is a process we refer to as input injection.
Understanding the time course of sensorimotor learning in human motor control is an open research
problem (Wolpert & Flanagan, 2016) that exists concurrently with recent advances in deep rein-
forcement learning. Issues of generalization, context-dependent recall, transfer or ”savings” in fast

* These authors contributed equally to this work.

1

