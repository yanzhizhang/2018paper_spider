Submitted as a conference paper to ICLR 2018

A HIERARCHICAL MODEL FOR DEVICE PLACEMENT

Azalia Mirhoseini*, Anna Goldie∗, Hieu Pham, Benoit Steiner, Quoc V. Le and Jeff Dean
{azalia,agoldie,hyhieu,bsteiner,qvl,jeff}@google.com

ABSTRACT

We introduce a hierarchical model for efﬁcient placement of computational graphs
onto hardware devices, especially in heterogeneous environments with a mixture of
CPUs, GPUs, and other computational devices. Our method learns to assign graph
operations to groups and to allocate those groups to available devices. The grouping
and device allocations are learned jointly. The proposed method is trained with
policy gradient and requires no human intervention. Experiments with widely-used
computer vision and natural language models show that our algorithm can ﬁnd
optimized, non-trivial placements for TensorFlow computational graphs with over
80,000 operations. In addition, our approach outperforms placements by human
experts as well as a previous state-of-the-art placement method based on deep
reinforcement learning. Our method achieves runtime reductions of up to 60.6%
per training step when applied to models such as Neural Machine Translation.

1

INTRODUCTION & RELATED WORK

Deep neural networks have been successfully applied to many practical problems, such as image
classiﬁcation (LeCun et al., 1998; Krizhevsky et al., 2012; Taigman et al., 2014; Szegedy et al., 2015),
speech recognition (Hinton et al., 2012; Hannun et al., 2014), and machine translation (Sutskever
et al., 2014; Bahdanau et al., 2015; Wu et al., 2016b). These successes have lead to a surge in
demand for the computational resources needed to train and infer with neural networks. A common
approach to addressing this demand is to use a distributed environment with a combination of CPUs
and GPUs. In this environment, it is typical for a machine learning practitioner to explicitly place
the operations of their neural network onto particular computing devices for model parallelism and
data parallelism. For example, one might distribute the computation of the ﬁrst layer in a translation
network onto the ﬁrst GPU and the computation of the second layer onto the second GPU (Sutskever
et al., 2014; Wu et al., 2016b). Although these decisions can be made by a human practitioner, such
an approach does not scale well or produce optimal results, especially in the case of more complicated
networks (Szegedy et al., 2016b;a). Given the growing diversity of hardware devices (e.g., Google
TPUs, Intel Nervana, etc.) and recent trends toward automated neural architecture search (Zoph & Le,
2017; Real et al., 2017; Baker et al., 2016), where new models are generated, trained and evaluated
in an entirely end-to-end fashion, it seems natural to move toward more automated solutions for
efﬁciently distributing computation.
Device placement can be framed as the problem of learning to partition a graph across available
devices. Given that graph partitioning is a well-studied subject in computer science (Fiduccia
& Mattheyses, 1988; Karypis & Kumar, 1995b; Pellegrini, 2009b), traditional graph partitioning
methods represent a natural baseline for automated device placement. We ran experiments using
Scotch (Pellegrini, 2009b), a well-established open source library for graph partitioning, which
includes optimizations such as k-way Fiduccia-Mattheyses (Fiduccia & Mattheyses, 1988), Multilevel
methods (Barnard & Simon, 1994; Hendrickson & Leland, 1993; Karypis & Kumar, 1995a), the
Band Method (Chevalier & Pellegrini, 2006), the Diffusion Method (Pellegrini, 2007), and Dual
Recursive Bipartitioning Mapping (Pellegrini & Roman, 1996). The objective was to balance the
computational load across a set of connected processing nodes, while colocating neighboring nodes
to minimize communication cost. Despite its promise, this approach yielded disappointing results,
likely due to the non-stationarity of the cost function. We target a distributed environment where we
use a shared cluster of CPUs and GPUs, and our CPUs may also serve other jobs at the same time.
Thus, while cost-based models such as (Matthias Boehm & Tian, 2014) provide a strong baseline

∗Equal Contribution.

1

