Published as a conference paper at ICLR 2018

WHAI: WEIBULL HYBRID AUTOENCODING
INFERENCE FOR DEEP TOPIC MODELING

Hao Zhang, Bo Chen∗ & Dandan Guo
National Laboraory of Radar Signal Processing,
Collaborative Innovation Center of Information Sensing and Understanding,
Xidian University, Xi’an, China.
zhanghao_xidian@163.com
gdd_xidian@126.com

bchen@mail.xidian.edu.cn

Mingyuan Zhou
McCombs School of Business,
The University of Texas at Austin, Austin, TX 78712, USA.
Mingyuan.Zhou@mccombs.utexas.edu

ABSTRACT

To train an inference network jointly with a deep generative topic model, making
it both scalable to big corpora and fast in out-of-sample prediction, we develop
Weibull hybrid autoencoding inference (WHAI) for deep latent Dirichlet alloca-
tion, which infers posterior samples via a hybrid of stochastic-gradient MCMC
and autoencoding variational Bayes. The generative network of WHAI has a hier-
archy of gamma distributions, while the inference network of WHAI is a Weibull
upward-downward variational autoencoder, which integrates a deterministic-
upward deep neural network, and a stochastic-downward deep generative model
based on a hierarchy of Weibull distributions. The Weibull distribution can be
used to well approximate a gamma distribution with an analytic Kullback-Leibler
divergence, and has a simple reparameterization via the uniform noise, which help
efﬁciently compute the gradients of the evidence lower bound with respect to the
parameters of the inference network. The effectiveness and efﬁciency of WHAI
are illustrated with experiments on big corpora.

1

INTRODUCTION

There is a surge of research interest in multilayer representation learning for documents. To analyze
the term-document count matrix of a text corpus, Srivastava et al. (2013) extend the deep Boltzmann
machine (DBM) with the replicated softmax topic model of Salakhutdinov & Hinton (2009) to infer
a multilayer representation with binary hidden units, but its inference network is not trained to match
the true posterior (Mnih & Gregor, 2014) and the higher-layer neurons learned by DBM are difﬁcult
to visualize. The deep Poisson factor models of Gan et al. (2015) are introduced to generalize
Poisson factor analysis (Zhou et al., 2012), with a deep structure restricted to model binary topic
usage patterns. Deep exponential families (DEF) of Ranganath et al. (2015) construct more general
probabilistic deep networks with non-binary hidden units, in which a count matrix can be factorized
under the Poisson likelihood, with the gamma distributed hidden units of adjacent layers linked via
the gamma scale parameters. The Poisson gamma belief network (PGBN) (Zhou et al., 2015; 2016)
also factorizes a count matrix under the Poisson likelihood, but factorizes the shape parameters of
the gamma distributed hidden units of each layer into the product of a connection weight matrix and
the gamma hidden units of the next layer, resulting in strong nonlinearity and readily interpretable
multilayer latent representations.
Those multilayer probabilistic models are often characterized by a top-down generative structure,
with the distribution of a hidden layer typically acting as a prior for the layer below. Despite be-
ing able to infer a multilayer representation of a text corpus with scalable inference (Patterson &

∗Corresponding author

1

