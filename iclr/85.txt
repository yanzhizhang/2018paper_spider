Published as a conference paper at ICLR 2018

ON THE DISCRIMINATION-GENERALIZATION TRADE-
OFF IN GANS

Pengchuan Zhang
Microsoft Research, Redmond
penzhan@microsoft.com

Dengyong Zhou
Google
dennyzhou@google.com

Xiaodong He
Microsoft Research, Redmond
xiaohe@microsoft.com

Qiang Liu
Computer Science, Dartmouth College
qiang.liu@dartmouth.edu

Tao Xu
Computer Science, Lehigh University
tax313@lehigh.edu

ABSTRACT

Generative adversarial training can be generally understood as minimizing certain
moment matching loss deﬁned by a set of discriminator functions, typically neu-
ral networks. The discriminator set should be large enough to be able to uniquely
identify the true distribution (discriminative), and also be small enough to go be-
yond memorizing samples (generalizable). In this paper, we show that a discrim-
inator set is guaranteed to be discriminative whenever its linear span is dense in
the set of bounded continuous functions. This is a very mild condition satisﬁed
even by neural networks with a single neuron. Further, we develop generaliza-
tion bounds between the learned distribution and true distribution under different
evaluation metrics. When evaluated with neural distance, our bounds show that
generalization is guaranteed as long as the discriminator set is small enough, re-
gardless of the size of the generator or hypothesis set. When evaluated with KL
divergence, our bound provides an explanation on the counter-intuitive behaviors
of testing likelihood in GAN training. Our analysis sheds lights on understanding
the practical performance of GANs.

1

INTRODUCTION

Generative adversarial networks (GANs) (Goodfellow et al., 2014) and their variants can be gen-
erally understood as minimizing certain moment matching loss deﬁned by a set of discriminator
functions. Mathematically, GANs minimize the integral probability metric (IPM) (M¨uller, 1997),
that is,

(cid:8) Ex∼ˆµm [f (x)] − Ex∼ν[f (x)](cid:9)(cid:27)

,

(cid:26)

min
ν∈G

dF (ˆµm, ν) := sup
f∈F

corresponding to the Wasserstain-1 distance.

(1)
where ˆµm is the empirical measure of the observed data, and F and G are the sets of discriminators
and generators, respectively.
1. Wasserstain GAN (W-GAN) (Arjovsky et al., 2017). F = Lip1(X) := {f : ||f||Lip ≤ 1},
2. MMD-GAN (Li et al., 2015; Dziugaite et al., 2015; Li et al., 2017a). F is taken as the unit ball in
a Reproducing Kernel Hilbert Space (RKHS), corresponding to the Maximum Mean Discrepency
(MMD).
3. Energy-based GANs (Zhao et al., 2016). F is taken as the set of continuous functions bounded
between 0 and M for some constant M > 0, corresponding to the total variation distance (Ar-
jovsky et al., 2017).

1

