Published as a conference paper at ICLR 2018

POLAR TRANSFORMER NETWORKS

Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, Kostas Daniilidis
GRASP Laboratory, University of Pennsylvania
{machc, allec, xiaowz, kostas}@seas.upenn.edu

ABSTRACT

Convolutional neural networks (CNNs) are inherently equivariant to translation.
Efforts to embed other forms of equivariance have concentrated solely on rota-
tion. We expand the notion of equivariance in CNNs through the Polar Trans-
former Network (PTN). PTN combines ideas from the Spatial Transformer Net-
work (STN) and canonical coordinate representations. The result is a network
invariant to translation and equivariant to both rotation and scale. PTN is trained
end-to-end and composed of three distinct stages: a polar origin predictor, the
newly introduced polar transformer module and a classiﬁer. PTN achieves state-
of-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an
MNIST variation obtained by adding clutter and perturbing digits with translation,
rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate
through the Cylindrical Transformer Network.

1

INTRODUCTION

Whether at the global pattern or local feature level (Granlund, 1978), the quest for (in/equi)variant
representations is as old as the ﬁeld of computer vision and pattern recognition itself. State-of-the-art
in “hand-crafted” approaches is typiﬁed by SIFT (Lowe, 2004). These detector/descriptors identify
the intrinsic scale or rotation of a region (Lindeberg, 1994; Chomat et al., 2000) and produce an
equivariant descriptor which is normalized for scale and/or rotation invariance. The burden of these
methods is in the computation of the orbit (i.e. a sampling the transformation space) which is neces-
sary to achieve equivariance. This motivated steerable ﬁltering which guarantees transformed ﬁlter
responses can be interpolated from a ﬁnite number of ﬁlter responses. Steerability was proved for
rotations of Gaussian derivatives (Freeman et al., 1991) and extended to scale and translations in the
shiftable pyramid (Simoncelli et al., 1992). Use of the orbit and SVD to create a ﬁlter basis was
proposed by Perona (1995)and in parallel, Segman et al. (1992) proved for certain classes of trans-
formations there exists canonical coordinates where deformation of the input presents as translation
of the output. Following this work, Nordberg & Granlund (1996) and Hel-Or & Teo (1996); Teo
& Hel-Or (1998) proposed a methodology for computing the bases of equivariant spaces given the
Lie generators of a transformation. and most recently, Sifre & Mallat (2013) proposed the scattering
transform which offers representations invariant to translation, scaling, and rotations.
The current consensus is representations should be learned not designed. Equivariance to trans-
lations by convolution and invariance to local deformations by pooling are now textbook (LeCun
et al. (2015), p.335) but approaches to equivariance of more general deformations are still maturing.
The main veins are: Spatial Transformer Network (STN) (Jaderberg et al., 2015) which similarly to
SIFT learn a canonical pose and produce an invariant representation through warping, work which
constrains the structure of convolutional ﬁlters (Worrall et al., 2016) and work which uses the ﬁlter
orbit (Cohen & Welling, 2016b) to enforce an equivariance to a speciﬁc transformation group.
In this paper, we propose the Polar Transformer Network (PTN), which combines the ideas of STN
and canonical coordinate representations to achieve equivariance to translations, rotations, and dila-
tions. The three stage network learns to identify the object center then transforms the input into log-
polar coordinates. In this coordinate system, planar convolutions correspond to group-convolutions
in rotation and scale. PTN produces a representation equivariant to rotations and dilations without

http://github.com/daniilidis-group//polar-transformer-networks

1

