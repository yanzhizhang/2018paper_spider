Published as a conference paper at ICLR 2018

DEEP LEARNING AND QUANTUM ENTANGLEMENT:
FUNDAMENTAL CONNECTIONS WITH IMPLICATIONS
TO NETWORK DESIGN

Yoav Levine, David Yakira, Nadav Cohen & Amnon Shashua
The Hebrew University of Jerusalem
{yoavlevine,davidyakira,cohennadav,shashua}@cs.huji.ac.il

ABSTRACT

Formal understanding of the inductive bias behind deep convolutional networks,
i.e. the relation between the network’s architectural features and the functions it
is able to model, is limited. In this work, we establish a fundamental connection
between the ﬁelds of quantum physics and deep learning, and use it for obtain-
ing novel theoretical observations regarding the inductive bias of convolutional
networks. Speciﬁcally, we show a structural equivalence between the function re-
alized by a convolutional arithmetic circuit (ConvAC) and a quantum many-body
wave function, which facilitates the use of quantum entanglement measures as
quantiﬁers of a deep network’s expressive ability to model correlations. Further-
more, the construction of a deep ConvAC in terms of a quantum Tensor Network
is enabled. This allows us to perform a graph-theoretic analysis of a convolutional
network, tying its expressiveness to a min-cut in its underlying graph. We demon-
strate a practical outcome in the form of a direct control over the inductive bias
via the number of channels (width) of each layer. We empirically validate our
ﬁndings on standard convolutional networks which involve ReLU activations and
max pooling. The description of a deep convolutional network in well-deﬁned
graph-theoretic tools and the structural connection to quantum entanglement, are
two interdisciplinary bridges that are brought forth by this work.

INTRODUCTION

1
A central factor in the application of machine learning to a given task is the restriction of the hypoth-
esis space of learned functions known as inductive bias. In deep convolutional networks, inductive
bias manifests itself in architectural features such as number of layers, number of channels per layer,
and more (LeCun et al., 2015). Formal understanding of the inductive bias behind convolutional net-
works is limited – the assumptions encoded into these models, which seem to form an excellent prior
knowledge for different types of data (e.g. Krizhevsky et al. (2012); He et al. (2016); van den Oord
et al. (2016)), are for the most part a mystery.
An important aspect of the inﬂuence that a certain architectural feature has on the inductive bias, is its
effect on the network’s ability to model correlations between regions of its input. In this regard, one
typically considers partitions that divide input regions into disjoint sets, and asks how far the function
realized by the network is from being separable with respect to these partitions(Cohen and Shashua,
2017; Levine et al., 2017). For example, Cohen and Shashua (2017) show that when separability is
measured through the algebraic notion of separation-rank, deep Convolutional Arithmetic Circuits
(ConvACs) (Cohen et al., 2016b) support exponential (in network size) separation-ranks for certain
input partitions, while being limited to polynomial separation-ranks for others. ConvACs are a
special class of convolutional networks, characterized by linear activations and product pooling,
which served a key role in theoretical analyses of convolutional networks, in virtue of their algebraic
structure.
In this work, we draw upon formal similarities between how physicists describe a system of many-
particles as a quantum mechanical wave function, and how machine learning practitioners map a
high-dimensional input (e.g. image) to a set of output labels through a deep network. In particular,
we show that there is a structural equivalence between a function modeled by a ConvAC and a
many-body quantum wave function, which relies on their underlying tensorial structure. This allows
employment of the well-established physical notion of quantum entanglement measures (Plenio and
Virmani, 2007), which subsumes other algebraic notions of separability such as the separation-rank
mentioned above, for the analysis of correlations modeled by deep convolutional networks.

1

