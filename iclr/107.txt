Published as a conference paper at ICLR 2018

LARGE SCALE DISTRIBUTED NEURAL NETWORK
TRAINING THROUGH ONLINE DISTILLATION

Rohan Anil
Google
rohananil@google.com

Gabriel Pereyra ∗
Google DeepMind
pereyra@google.com

Alexandre Passos
Google Brain
apassos@google.com

Robert Ormandi
Google
ormandi@google.com

George E. Dahl
Google Brain
gdahl@google.com

Geoffrey E. Hinton
Google Brain
geoffhinton@google.com

ABSTRACT

Techniques such as ensembling and distillation promise model quality improve-
ments when paired with almost any base model. However, due to increased test-
time cost (for ensembles) and increased complexity of the training pipeline (for
distillation), these techniques are challenging to use in industrial settings. In this
paper we explore a variant of distillation which is relatively straightforward to use
as it does not require a complicated multi-stage setup or many new hyperparam-
eters. Our ﬁrst claim is that online distillation enables us to use extra parallelism
to ﬁt very large datasets about twice as fast. Crucially, we can still speed up train-
ing even after we have already reached the point at which additional parallelism
provides no beneﬁt for synchronous or asynchronous stochastic gradient descent.
Two neural networks trained on disjoint subsets of the data can share knowledge
by encouraging each model to agree with the predictions the other model would
have made. These predictions can come from a stale version of the other model so
they can be safely computed using weights that only rarely get transmitted. Our
second claim is that online distillation is a cost-effective way to make the exact
predictions of a model dramatically more reproducible. We support our claims
using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the
largest to-date dataset used for neural language modeling, containing 6 × 1011
tokens and based on the Common Crawl repository of web data.

1

INTRODUCTION

For large-scale, commercially valuable neural net training problems, practitioners would be will-
ing to devote many more machines to training if it sped up training time dramatically or improved
the quality of the ﬁnal model. Currently, distributed stochastic gradient descent (SGD), in both its
synchronous and asynchronous forms (Chen et al., 2016), is the dominant algorithm for large-scale
neural network training across multiple interconnected machines. Unfortunately, as the number of
machines increases, there are diminishing improvements to the time needed to train a high quality
model, to a point where adding workers does not further improve training time. A combination of
infrastructure limitations and optimization barriers constrain the scalability of distributed minibatch
SGD. The overhead of communicating weight updates and the long tail of the machine and network
latency distributions slow down execution and produce thorny engineering challenges. For the syn-
chronous algorithm, there are rapidly diminishing returns from increasing the effective batch size
(LeCun et al., 2012; Keskar et al., 2017). For the asynchronous algorithm, gradient interference
from inconsistent weights can cause updates to thrash and even, in some cases, result in worse ﬁnal
accuracy or completely stall learning progress. The precise scalability limit for distributed SGD will
depend on implementation details of the algorithm, speciﬁcs of the infrastructure, and the capabili-
ties of the hardware, but in our experience it can be very difﬁcult to scale effectively much beyond

∗Work completed while G. Pereyra was a Google Brain resident.

1

