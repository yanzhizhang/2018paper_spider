Published as a conference paper at ICLR 2018

SMOOTH LOSS FUNCTIONS FOR DEEP TOP-K
CLASSIFICATION

Leonard Berrada1, Andrew Zisserman1 and M. Pawan Kumar1,2
1Department of Engineering Science
University of Oxford
2Alan Turing Institute
{lberrada,az,pawan}@robots.ox.ac.uk

ABSTRACT

The top-k error is a common measure of performance in machine learning and
computer vision. In practice, top-k classiﬁcation is typically performed with deep
neural networks trained with the cross-entropy loss. Theoretical results indeed
suggest that cross-entropy is an optimal learning objective for such a task in the limit
of inﬁnite data. In the context of limited and noisy data however, the use of a loss
function that is speciﬁcally designed for top-k classiﬁcation can bring signiﬁcant
improvements. Our empirical evidence suggests that the loss function must be
smooth and have non-sparse gradients in order to work well with deep neural
networks. Consequently, we introduce a family of smoothed loss functions that are
suited to top-k optimization via deep learning. The widely used cross-entropy is a
special case of our family. Evaluating our smooth loss functions is computationally

challenging: a na¨ıve algorithm would require O((cid:0)n

number of classes. Thanks to a connection to polynomial algebra and a divide-
and-conquer approach, we provide an algorithm with a time complexity of O(kn).
Furthermore, we present a novel approximation to obtain fast and stable algorithms
on GPUs with single ﬂoating point precision. We compare the performance of the
cross-entropy loss and our margin-based losses in various regimes of noise and
data size, for the predominant use case of k = 5. Our investigation reveals that our
loss is more robust to noise and overﬁtting than cross-entropy.

(cid:1)) operations, where n is the

k

1

INTRODUCTION

In machine learning many classiﬁcation tasks present inherent label confusion. The confusion
can originate from a variety of factors, such as incorrect labeling, incomplete annotation, or some
fundamental ambiguities that obfuscate the ground truth label even to a human expert. For example,
consider the images from the ImageNet data set (Russakovsky et al., 2015) in Figure 1, which
illustrate the aforementioned factors. To mitigate these issues, one may require the model to predict
the k most likely labels, where k is typically very small compared to the total number of labels. Then
the prediction is considered incorrect if all of its k labels differ from the ground truth, and correct
otherwise. This is commonly referred to as the top-k error. Learning such models is a longstanding
task in machine learning, and many loss functions for top-k error have been suggested in the literature.
In the context of correctly labeled large data, deep neural networks trained with cross-entropy have
shown exemplary capacity to accurately approximate the data distribution. An illustration of this
phenomenon is the performance attained by deep convolutional neural networks on the ImageNet
challenge. Speciﬁcally, state-of-the-art models trained with cross-entropy yield remarkable success on
the top-5 error, although cross-entropy is not tailored for top-5 error minimization. This phenomenon
can be explained by the fact that cross-entropy is top-k calibrated for any k (Lapin et al., 2016), an
asymptotic property which is veriﬁed in practice in the large data setting. However, in cases where
only a limited amount of data is available, learning large models with cross-entropy can be prone to
over-ﬁtting on incomplete or noisy labels.
To alleviate the deﬁciency of cross-entropy, we present a new family of top-k classiﬁcation loss
functions for deep neural networks. Taking inspiration from multi-class SVMs, our loss creates a

1

