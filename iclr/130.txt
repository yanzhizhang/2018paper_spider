Published as a conference paper at ICLR 2018

TD OR NOT TD: ANALYZING THE ROLE OF TEMPORAL
DIFFERENCING IN DEEP REINFORCEMENT LEARNING

Artemij Amiranashvili Alexey Dosovitskiy Vladlen Koltun Thomas Brox
University of Freiburg

Intel Labs

Intel Labs

University of Freiburg

ABSTRACT

Our understanding of reinforcement learning (RL) has been shaped by theoretical
and empirical results that were obtained decades ago using tabular representations
and linear function approximators. These results suggest that RL methods that use
temporal differencing (TD) are superior to direct Monte Carlo estimation (MC).
How do these results hold up in deep RL, which deals with perceptually complex
environments and deep nonlinear models? In this paper, we re-examine the role
of TD in modern deep RL, using specially designed environments that control for
speciﬁc factors that affect performance, such as reward sparsity, reward delay, and
the perceptual complexity of the task. When comparing TD with inﬁnite-horizon
MC, we are able to reproduce classic results in modern settings. Yet we also
ﬁnd that ﬁnite-horizon MC is not inferior to TD, even when rewards are sparse or
delayed. This makes MC a viable alternative to TD in deep RL.

1

INTRODUCTION

The use of deep networks as function approximators has signiﬁcantly expanded the range of prob-
lems that can be successfully tackled with reinforcement learning. However, there is little under-
standing of when and why certain deep RL algorithms work well. Theoretical results are mainly
based on tabular environments or linear function approximators (Sutton & Barto, 2017). Their as-
sumptions do not cover the typical application domains of deep RL, which feature extremely high
input dimensionality (typically in the tens of thousands) and the use of nonlinear function approx-
imators. Thus, our understanding of deep RL is based primarily on empirical results, and these
empirical results guide the design of deep RL algorithms.
One design decision shared by the vast majority of existing value-based deep RL methods is the use
of temporal difference (TD) learning – training predictive models by bootstrapping based on their
own predictions. This design decision is primarily based on evidence from the pre-deep-RL era (Sut-
ton, 1988; 1995). The results of those experimental studies are well-known and clearly demonstrate
that simple supervised learning, also known as Monte Carlo prediction (MC), is outperformed by
pure TD learning, which, in turn, is outperformed by TD(λ) – a method that can be seen as a mixture
of TD and MC (Sutton, 1988).
However, recent research has shown that an algorithm based on Monte Carlo prediction can out-
perform TD-based methods on complex sensorimotor control tasks in three-dimensional, partially
observable environments (Dosovitskiy & Koltun, 2017). These results suggest that the classic un-
derstanding of the relative performance of TD and MC may not hold in modern settings. This evi-
dence is not conclusive: the algorithm proposed by Dosovitskiy & Koltun (2017) involves custom
components such as parametrized goals and decomposed rewards, and therefore cannot be directly
compared to TD-based baselines.
In this paper, we perform a controlled experimental study aiming at better understanding the role of
temporal differencing in modern deep reinforcement learning, which is characterized by essentially
inﬁnite-dimensional state spaces, extremely high observation dimensionality, partial observability,
and deep nonlinear models used as function approximators. We focus on environments with vi-
sual inputs and discrete action sets, and algorithms that involve prediction of value or action-value
functions. This is in contrast to value-free policy optimization algorithms (Schulman et al., 2015;
Levine & Koltun, 2013) and tasks with continuous action spaces and low-dimensional vectorial state
representations that have been extensively benchmarked by Duan et al. (2016) and Henderson et al.

1

