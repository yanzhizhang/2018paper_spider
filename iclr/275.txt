Published as a conference paper at ICLR 2018

QANET: COMBINING LOCAL CONVOLUTION WITH
GLOBAL SELF-ATTENTION FOR READING COMPRE-
HENSION

Adams Wei Yu1∗, David Dohan2†, Minh-Thang Luong2†
{weiyu}@cs.cmu.edu, {ddohan,thangluong}@google.com
1Carnegie Mellon University, 2Google Brain

Rui Zhao, Kai Chen, Mohammad Norouzi, Quoc V. Le
Google Brain

ABSTRACT

Current end-to-end machine reading and question answering (Q&A) models are
primarily based on recurrent neural networks (RNNs) with attention. Despite their
success, these models are often slow for both training and inference due to the se-
quential nature of RNNs. We propose a new Q&A architecture called QANet,
which does not require recurrent networks: Its encoder consists exclusively of
convolution and self-attention, where convolution models local interactions and
self-attention models global interactions. On the SQuAD dataset, our model is 3x
to 13x faster in training and 4x to 9x faster in inference, while achieving equiva-
lent accuracy to recurrent models. The speed-up gain allows us to train the model
with much more data. We hence combine our model with data generated by back-
translation from a neural machine translation model. On the SQuAD dataset, our
single model, trained with augmented data, achieves 84.6 F1 score1 on the test set,
which is signiﬁcantly better than the best published F1 score of 81.8.

1

INTRODUCTION

There is growing interest in the tasks of machine reading comprehension and automated question
answering. Over the past few years, signiﬁcant progress has been made with end-to-end models
showing promising results on many challenging datasets. The most successful models generally
employ two key ingredients: (1) a recurrent model to process sequential inputs, and (2) an attention
component to cope with long term interactions. A successful combination of these two ingredients is
the Bidirectional Attention Flow (BiDAF) model by Seo et al. (2016), which achieve strong results
on the SQuAD dataset (Rajpurkar et al., 2016). A weakness of these models is that they are often
slow for both training and inference due to their recurrent nature, especially for long texts. The
expensive training not only leads to high turnaround time for experimentation and limits researchers
from rapid iteration but also prevents the models from being used for larger dataset. Meanwhile
the slow inference prevents the machine comprehension systems from being deployed in real-time
applications.
In this paper, aiming to make the machine comprehension fast, we propose to remove the recurrent
nature of these models. We instead exclusively use convolutions and self-attentions as the building
blocks of encoders that separately encodes the query and context. Then we learn the interactions
between context and question by standard attentions (Xiong et al., 2016; Seo et al., 2016; Bahdanau
et al., 2015). The resulting representation is encoded again with our recurrency-free encoder before
ﬁnally decoding to the probability of each position being the start or end of the answer span. We
call this architecture QANet, which is shown in Figure 1.

∗Work performed while Adams Wei Yu was with Google Brain.
†Equal contribution.
1While the major results presented here are those obtained in Oct 2017, our latest scores (as of Apr 23,
2018) on SQuAD leaderboard is EM/F1=82.2/88.6 for single model and EM/F1=83.9/89.7 for ensemble, both
ranking No.1. Notably, the EM of our ensemble is better than the human performance (82.3).

1

