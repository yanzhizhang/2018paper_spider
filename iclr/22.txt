Published as a conference paper at ICLR 2018

BEYOND SHARED HIERARCHIES: DEEP MULTITASK
LEARNING THROUGH SOFT LAYER ORDERING

Elliot Meyerson & Risto Miikkulainen
The University of Texas at Austin and Sentient Technologies, Inc.
{ekm, risto}@cs.utexas.edu

ABSTRACT

Existing deep multitask learning (MTL) approaches align layers shared between
tasks in a parallel ordering. Such an organization signiﬁcantly constricts the types
of shared structure that can be learned. The necessity of parallel ordering for
deep MTL is ﬁrst tested by comparing it with permuted ordering of shared layers.
The results indicate that a ﬂexible ordering can enable more effective sharing,
thus motivating the development of a soft ordering approach, which learns how
shared layers are applied in different ways for different tasks. Deep MTL with
soft ordering outperforms parallel ordering methods across a series of domains.
These results suggest that the power of deep MTL comes from learning highly
general building blocks that can be assembled to meet the demands of each task.

1

INTRODUCTION

In multitask learning (MTL) (Caruana, 1998), auxiliary data sets are harnessed to improve overall
performance by exploiting regularities present across tasks. As deep learning has yielded state-of-
the-art systems across a range of domains, there has been increased focus on developing deep MTL
techniques. Such techniques have been applied across settings such as vision (Bilen and Vedaldi,
2016; 2017; Jou and Chang, 2016; Lu et al., 2017; Misra et al., 2016; Ranjan et al., 2016; Yang and
Hospedales, 2017; Zhang et al., 2014), natural language (Collobert and Weston, 2008; Dong et al.,
2015; Hashimoto et al., 2016; Liu et al., 2015a; Luong et al., 2016), speech (Huang et al., 2013;
2015; Seltzer and Droppo, 2013; Wu et al., 2015), and reinforcement learning (Devin et al., 2016;
Fernando et al., 2017; Jaderberg et al., 2017; Rusu et al., 2016). Although they improve performance
over single-task learning in these settings, these approaches have generally been constrained to joint
training of relatively few and/or closely-related tasks.
On the other hand, from a perspective of Kolmogorov complexity, “transfer should always be use-
ful”; any pair of distributions underlying a pair of tasks must have something in common (Mahmud,
2009; Mahmud and Ray, 2008). In principle, even tasks that are “superﬁcially unrelated” such as
those in vision and NLP can beneﬁt from sharing (even without an adaptor task, such as image cap-
tioning). In other words, for a sufﬁciently expressive class of models, the inductive bias of requiring
a model to ﬁt multiple tasks simultaneously should encourage learning to converge to more realistic
representations. The expressivity and success of deep models suggest they are ideal candidates for
improvement via MTL. So, why have existing approaches to deep MTL been so restricted in scope?
MTL is based on the assumption that learned transformations can be shared across tasks. This
paper identiﬁes an additional implicit assumption underlying existing approaches to deep MTL: this
sharing takes place through parallel ordering of layers. That is, sharing between tasks occurs only
at aligned levels (layers) in the feature hierarchy implied by the model architecture. This constraint
limits the kind of sharing that can occur between tasks. It requires subsequences of task feature
hierarchies to match, which may be difﬁcult to establish as tasks become plentiful and diverse.
This paper investigates whether parallel ordering of layers is necessary for deep MTL. As an al-
ternative, it introduces methods that make deep MTL more ﬂexible. First, existing approaches are
reviewed in the context of their reliance on parallel ordering. Then, as a foil to parallel ordering,
permuted ordering is introduced, in which shared layers are applied in different orders for differ-
ent tasks. The increased ability of permuted ordering to support integration of information across
tasks is analyzed, and the results are used to develop a soft ordering approach to deep MTL. In this

1

