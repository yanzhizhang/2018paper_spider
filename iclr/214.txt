VARIANCE REDUCTION FOR POLICY GRADIENT WITH
ACTION-DEPENDENT FACTORIZED BASELINES

Cathy Wu1, Aravind Rajeswaran2, Yan Duan13, Vikash Kumar2,
Alexandre M Bayen14, Sham Kakade2, Igor Mordatch3, Pieter Abbeel13
cathywu@eecs.berkeley.edu, aravraj@cs.washington.edu,
rockyduan@eecs.berkeley.edu, vikash@cs.washington.edu,
bayen@berkeley.edu, sham@cs.washington.edu,
igor.mordatch@gmail.com, pabbeel@cs.berkeley.edu
1 Department of EECS, UC Berkeley
2 Department of CSE, University of Washington
3 OpenAI
4 Institute for Transportation Studies, UC Berkeley

ABSTRACT

Policy gradient methods have enjoyed great success in deep reinforcement learn-
ing but suffer from high variance of gradient estimates. The high variance problem
is particularly exasperated in problems with long horizons or high-dimensional
action spaces. To mitigate this issue, we derive a bias-free action-dependent base-
line for variance reduction which fully exploits the structural form of the stochas-
tic policy itself and does not make any additional assumptions about the MDP.
We demonstrate and quantify the beneﬁt of the action-dependent baseline through
both theoretical analysis as well as numerical results, including an analysis of
the suboptimality of the optimal state-dependent baseline. The result is a com-
putationally efﬁcient policy gradient algorithm, which scales to high-dimensional
control problems, as demonstrated by a synthetic 2000-dimensional target match-
ing task. Our experimental results indicate that action-dependent baselines al-
low for faster learning on standard reinforcement learning benchmarks and high-
dimensional hand manipulation and synthetic tasks. Finally, we show that the
general idea of including additional information in baselines for improved vari-
ance reduction can be extended to partially observed and multi-agent tasks.

1

INTRODUCTION

Deep reinforcement learning has achieved impressive results in recent years in domains such as
video games from raw visual inputs (Mnih et al., 2015), board games (Silver et al., 2016), sim-
ulated control tasks (Schulman et al., 2016; Lillicrap et al., 2016; Rajeswaran et al., 2017a), and
robotics (Levine et al., 2016). An important class of methods behind many of these success stories
are policy gradient methods (Williams, 1992; Sutton et al., 2000; Kakade, 2002; Schulman et al.,
2015; Mnih et al., 2016), which directly optimize parameters of a stochastic policy through local
gradient information obtained by interacting with the environment using the current policy. Policy
gradient methods operate by increasing the log probability of actions proportional to the future re-
wards inﬂuenced by these actions. On average, actions which perform better will acquire higher
probability, and the policy’s expected performance improves.
A critical challenge of policy gradient methods is the high variance of the gradient estimator. This
high variance is caused in part due to difﬁculty in credit assignment to the actions which affected the
future rewards. Such issues are further exacerbated in long horizon problems, where assigning cred-
its properly becomes even more challenging. To reduce variance, a “baseline” is often employed,
which allows us to increase or decrease the log probability of actions based on whether they perform
better or worse than the average performance when starting from the same state. This is particu-
larly useful in long horizon problems, since the baseline helps with temporal credit assignment by

1

