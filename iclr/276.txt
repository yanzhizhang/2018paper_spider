Published as a conference paper at ICLR 2018

VARIATIONAL NETWORK QUANTIZATION

Jan Achterhold1,2, Jan M. K¨ohler1, Anke Schmeink2 & Tim Genewein1,*

1Bosch Center for Artiﬁcial Intelligence
Robert Bosch GmbH
Renningen, Germany

2RWTH Aachen University
Institute for Theoretical Information Technology
Aachen, Germany
*Corresponding author: tim.genewein@de.bosch.com

ABSTRACT

In this paper, the preparation of a neural network for pruning and few-bit quanti-
zation is formulated as a variational inference problem. To this end, a quantizing
prior that leads to a multi-modal, sparse posterior distribution over weights, is in-
troduced and a differentiable Kullback-Leibler divergence approximation for this
prior is derived. After training with Variational Network Quantization, weights
can be replaced by deterministic quantization values with small to negligible loss
of task accuracy (including pruning by setting weights to 0). The method does not
require ﬁne-tuning after quantization. Results are shown for ternary quantization
on LeNet-5 (MNIST) and DenseNet (CIFAR-10).

1

INTRODUCTION

Parameters of a trained neural network commonly exhibit high degrees of redundancy (Denil et al.,
2013) which implies an over-parametrization of the network. Network compression methods im-
plicitly or explicitly aim at the systematic reduction of redundancy in neural network models while
at the same time retaining a high level of task accuracy. Besides architectural approaches, such as
SqueezeNet (Iandola et al., 2016) or MobileNets (Howard et al., 2017), many compression methods
perform some form of pruning or quantization. Pruning is the removal of irrelevant units (weights,
neurons or convolutional ﬁlters) (LeCun et al., 1990). Relevance of weights is often determined
by the absolute value (“magnitude based pruning” (Han et al., 2016; 2017; Guo et al., 2016)), but
more sophisticated methods have been known for decades, e.g., based on second-order derivatives
(Optimal Brain Damage (LeCun et al., 1990) and Optimal Brain Surgeon (Hassibi & Stork, 1993))
or ARD (automatic relevance determination, a Bayesian framework for determining the relevance
of weights, (MacKay, 1995; Neal, 1995; Karaletsos & R¨atsch, 2015)). Quantization is the reduc-
tion of the bit-precision of weights, activations or even gradients, which is particularly desirable
from a hardware perspective (Sze et al., 2017). Methods range from ﬁxed bit-width computation
(e.g., 12-bit ﬁxed point) to aggressive quantization such as binarization of weights and activations
(Courbariaux et al., 2016; Rastegari et al., 2016; Zhou et al., 2016; Hubara et al., 2016). Few-bit
quantization (2 to 6 bits) is often performed by k-means clustering of trained weights with subse-
quent ﬁne-tuning of the cluster centers (Han et al., 2016). Pruning and quantization methods have
been shown to work well in conjunction (Han et al., 2016). In so-called “ternary” networks, weights
can have one out of three possible values (negative, zero or positive) which also allows for simulta-
neous pruning and few-bit quantization (Li et al., 2016; Zhu et al., 2016).
This work is closely related to some recent Bayesian methods for network compression (Ullrich
et al., 2017; Molchanov et al., 2017; Louizos et al., 2017; Neklyudov et al., 2017) that learn a
posterior distribution over network weights under a sparsity-inducing prior. The posterior distribu-
tion over network parameters allows identifying redundancies through three means: weights with
(1) an expected value very close to zero and (2) weights with a large variance can be pruned as
they do not contribute much to the overall computation. (3) the posterior variance over non-pruned

1

