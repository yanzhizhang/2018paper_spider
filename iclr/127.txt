Published as a conference paper at ICLR 2018

PIXELNN: EXAMPLE-BASED IMAGE SYNTHESIS

Aayush Bansal
Carnegie Mellon University
{aayushb,yaser,deva}@cs.cmu.edu

Yaser Sheikh

Deva Ramanan

ABSTRACT

We present a simple nearest-neighbor (NN) approach that synthesizes high-
frequency photorealistic images from an “incomplete” signal such as a low-
resolution image, a surface normal map, or edges. Current state-of-the-art deep
generative models designed for such conditional image synthesis lack two impor-
tant things: (1) they are unable to generate a large set of diverse outputs, due
to the mode collapse problem. (2) they are not interpretable, making it difﬁcult
to control the synthesized output. We demonstrate that NN approaches poten-
tially address such limitations, but suffer in accuracy on small datasets. We design
a simple pipeline that combines the best of both worlds:
the ﬁrst stage uses a
convolutional neural network (CNN) to map the input to a (overly-smoothed) im-
age, and the second stage uses a pixel-wise nearest neighbor method to map the
smoothed output to multiple high-quality, high-frequency outputs in a control-
lable manner. Importantly, pixel-wise matching allows our method to compose
novel high-frequency content by cutting-and-pasting pixels from different train-
ing exemplars. We demonstrate our approach for various input modalities, and for
various domains ranging from human faces, pets, shoes, and handbags.

1

INTRODUCTION

We consider the task of generating high-resolution photo-realistic images from incomplete input
such as a low-resolution image, sketches, surface normal map, or label mask. Such a task has a
number of practical applications such as upsampling/colorizing legacy footage, texture synthesis for
graphics applications, and semantic image understanding for vision through analysis-by-synthesis.
These problems share a common underlying structure: a human/machine is given a signal that is
missing considerable details, and the task is to reconstruct plausible details.
Consider the edge map of cat in Figure 1-c. When we humans look at this edge map, we can easily
imagine multiple variations of whiskers, eyes, and stripes that could be viable and pleasing to the
eye. Indeed, the task of image synthesis has been well explored, not just for its practical applications
but also for its aesthetic appeal.
GANs: Current state-of-the-art approaches rely on generative adversarial networks (GANs) (Good-
fellow et al., 2014), and most relevant to us, conditional GANS that generate image conditioned on
an input signal (Denton et al., 2015; Radford et al., 2015; Isola et al., 2016). We argue that there are
two prominent limitations to such popular formalisms: (1) First and foremost, humans can imag-
ine multiple plausible output images given a incomplete input. We see this rich space of potential
outputs as a vital part of the human capacity to imagine and generate. Conditional GANs are in
principle able to generate multiple outputs through the injection of noise, but in practice suffer from
limited diversity (i.e., mode collapse) (Fig. 2). Recent approaches even remove the noise altogether,
treating conditional image synthesis as regression problem (Chen & Koltun, 2017). (2) Deep net-
works are still difﬁcult to explain or interpret, making the synthesized output difﬁcult to modify.
One implication is that users are not able to control the synthesized output. Moreover, the right
mechanism for even specifying user constraints (e.g., “generate a cat image that looks like my cat”)
is unclear. This restricts applicability, particularly for graphics tasks.
Nearest-neighbors: To address these limitations, we appeal to a classic learning architecture
that can naturally allow for multiple outputs and user-control: non-parametric models, or nearest-
neighbors (NN). Though quite a classic approach (Efros & Leung, 1999; Efros & Freeman, 2001;

1

