Published as a conference paper at ICLR 2018

FEW-SHOT LEARNING WITH GRAPH NEURAL NET-
WORKS

Victor Garcia∗
Amsterdam Machine Learning Lab
University of Amsterdam
Amsterdam, 1098 XH, NL
v.garciasatorras@uva.nl

Joan Bruna
Courant Institute of Mathematical Sciences
New York University
New York City, NY, 10010, USA
bruna@cims.nyu.edu

ABSTRACT

We propose to study the problem of few-shot learning with the prism of infer-
ence on a partially observed graphical model, constructed from a collection of
input images whose label can be either observed or not. By assimilating generic
message-passing inference algorithms with their neural-network counterparts, we
deﬁne a graph neural network architecture that generalizes several of the recently
proposed few-shot learning models. Besides providing improved numerical per-
formance, our framework is easily extended to variants of few-shot learning, such
as semi-supervised or active learning, demonstrating the ability of graph-based
models to operate well on ‘relational’ tasks.

1

INTRODUCTION

Supervised end-to-end learning has been extremely successful in computer vision, speech, or ma-
chine translation tasks, thanks to improvements in optimization technology, larger datasets and
streamlined designs of deep convolutional or recurrent architectures. Despite these successes, this
learning setup does not cover many aspects where learning is nonetheless possible and desirable.
One such instance is the ability to learn from few examples, in the so-called few-shot learning tasks.
Rather than relying on regularization to compensate for the lack of data, researchers have explored
ways to leverage a distribution of similar tasks, inspired by human learning Lake et al. (2015). This
deﬁnes a new supervised learning setup (also called ‘meta-learning’) in which the input-output pairs
are no longer given by iid samples of images and their associated labels, but by iid samples of
collections of images and their associated label similarity.
A recent and highly-successful research program has exploited this meta-learning paradigm on the
few-shot image classiﬁcation task Lake et al. (2015); Koch et al. (2015); Vinyals et al. (2016); Mishra
et al. (2017); Snell et al. (2017). In essence, these works learn a contextual, task-speciﬁc similarity
measure, that ﬁrst embeds input images using a CNN, and then learns how to combine the embedded
images in the collection to propagate the label information towards the target image.
In particular, Vinyals et al. (2016) cast the few-shot learning problem as a supervised classiﬁcation
task mapping a support set of images into the desired label, and developed an end-to-end architec-
ture accepting those support sets as input via attention mechanisms. In this work, we build upon
this line of work, and argue that this task is naturally expressed as a supervised interpolation prob-
lem on a graph, where nodes are associated with the images in the collection, and edges are given
by a trainable similarity kernels. Leveraging recent progress on representation learning for graph-
structured data Bronstein et al. (2017); Gilmer et al. (2017), we thus propose a simple graph-based
few-shot learning model that implements a task-driven message passing algorithm. The resulting
architecture is trained end-to-end, captures the invariances of the task, such as permutations within
the input collections, and offers a good tradeoff between simplicity, generality, performance and
sample complexity.
Besides few-shot learning, a related task is the ability to learn from a mixture of labeled and unla-
beled examples — semi-supervised learning, as well as active learning, in which the learner has the

∗Work done while Victor Garcia was a visiting scholar at New York University

1

