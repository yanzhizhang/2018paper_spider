Published as a conference paper at ICLR 2018

A DIRT-T APPROACH TO UNSUPERVISED DOMAIN
ADAPTATION

Rui Shu†∗, Hung H. Bui‡, Hirokazu Narui†, & Stefano Ermon†
†Stanford University
‡DeepMind
†
{ruishu,hirokaz2,ermon}@stanford.edu
‡
{buih}@google.com

ABSTRACT

Domain adaptation refers to the problem of leveraging labeled data in a source
domain to learn an accurate model in a target domain where labels are scarce or
unavailable. A recent approach for ﬁnding a common representation of the two
domains is via domain adversarial training (Ganin & Lempitsky, 2015), which
attempts to induce a feature extractor that matches the source and target feature
distributions in some feature space. However, domain adversarial training faces
two critical limitations: 1) if the feature extraction function has high-capacity, then
feature distribution matching is a weak constraint, 2) in non-conservative domain
adaptation (where no single classiﬁer can perform well in both the source and
target domains), training the model to do well on the source domain hurts perfor-
mance on the target domain. In this paper, we address these issues through the lens
of the cluster assumption, i.e., decision boundaries should not cross high-density
data regions. We propose two novel and related models: 1) the Virtual Adver-
sarial Domain Adaptation (VADA) model, which combines domain adversarial
training with a penalty term that punishes violation of the cluster assumption; 2)
the Decision-boundary Iterative Reﬁnement Training with a Teacher (DIRT-T)1
model, which takes the VADA model as initialization and employs natural gradient
steps to further minimize the cluster assumption violation. Extensive empirical re-
sults demonstrate that the combination of these two models signiﬁcantly improve
the state-of-the-art performance on the digit, trafﬁc sign, and Wi-Fi recognition
domain adaptation benchmarks.

1

INTRODUCTION

The development of deep neural networks has enabled impressive performance in a wide variety of
machine learning tasks. However, these advancements often rely on the existence of a large amount
of labeled training data. In many cases, direct access to vast quantities of labeled data for the task
of interest (the target domain) is either costly or otherwise absent, but labels are readily available
for related training sets (the source domain). A notable example of this scenario occurs when the
source domain consists of richly-annotated synthetic or semi-synthetic data, but the target domain
consists of unannotated real-world data (Sun & Saenko, 2014; Vazquez et al., 2014). However, the
source data distribution is often dissimilar to the target data distribution, and the resulting signiﬁcant
covariate shift is detrimental to the performance of the source-trained model when applied to the
target domain (Shimodaira, 2000).
Solving the covariate shift problem of this nature is an instance of domain adaptation (Ben-David
et al., 2010b). In this paper, we consider a challenging setting of domain adaptation where 1) we
are provided with fully-labeled source samples and completely-unlabeled target samples, and 2) the
existence of a classiﬁer in the hypothesis space with low generalization error in both source and
target domains is not guaranteed. Borrowing approximately the terminology from Ben-David et al.
(2010b), we refer to this setting as unsupervised, non-conservative domain adaptation. We note

∗Work was done during ﬁrst author’s internship at Adobe Research.
1Pronounce as “dirty.” Implementation available at https://github.com/RuiShu/dirt-t

1

