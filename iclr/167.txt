Published as a conference paper at ICLR 2018

CONTINUOUS ADAPTATION VIA META-LEARNING IN
NONSTATIONARY AND COMPETITIVE ENVIRONMENTS

Maruan Al-Shedivat∗
CMU

Trapit Bansal
UMass Amherst

Yura Burda
OpenAI

Ilya Sutskever
OpenAI

Igor Mordatch
OpenAI

Pieter Abbeel
UC Berkeley

ABSTRACT

The ability to continuously learn and adapt from limited experience in nonstationary
environments is an important milestone on the path towards general intelligence. In
this paper, we cast the problem of continuous adaptation into the learning-to-learn
framework. We develop a simple gradient-based meta-learning algorithm suitable
for adaptation in dynamically changing and adversarial scenarios. Additionally,
we design a new multi-agent competitive environment, RoboSumo, and deﬁne
iterated adaptation games for testing various aspects of continuous adaptation. We
demonstrate that meta-learning enables signiﬁcantly more efﬁcient adaptation than
reactive baselines in the few-shot regime. Our experiments with a population of
agents that learn and compete suggest that meta-learners are the ﬁttest.

1

INTRODUCTION

Recent progress in reinforcement learning (RL) has achieved very impressive results ranging from
playing games (Mnih et al., 2015; Silver et al., 2016), to applications in dialogue systems (Li et al.,
2016), to robotics (Levine et al., 2016). Despite the progress, the learning algorithms for solving
many of these tasks are designed to deal with stationary environments. On the other hand, real-world
is often nonstationary either due to complexity (Sutton et al., 2007), changes in the dynamics or the
objectives in the environment over the life-time of a system (Thrun, 1998), or presence of multiple
learning actors (Lowe et al., 2017; Foerster et al., 2017a). Nonstationarity breaks the standard
assumptions and requires agents to continuously adapt, both at training and execution time, in order
to succeed.
Learning under nonstationary conditions is challenging. The classical approaches to dealing with
nonstationarity are usually based on context detection (Da Silva et al., 2006) and tracking (Sutton
et al., 2007), i.e., reacting to the already happened changes in the environment by continuously
ﬁne-tuning the policy. Unfortunately, modern deep RL algorithms, while able to achieve super-human
performance on certain tasks, are known to be sample inefﬁcient. Nevertheless, nonstationarity allows
only for limited interaction before the properties of the environment change. Thus, it immediately
puts learning into the few-shot regime and often renders simple ﬁne-tuning methods impractical.
A nonstationary environment can be seen as a sequence of stationary tasks, and hence we propose to
tackle it as a multi-task learning problem (Caruana, 1998). The learning-to-learn (or meta-learning)
approaches (Schmidhuber, 1987; Thrun & Pratt, 1998) are particularly appealing in the few-shot
regime, as they produce ﬂexible learning rules that can generalize from only a handful of examples.
Meta-learning has shown promising results in the supervised domain and have gained a lot of attention
from the research community recently (e.g., Santoro et al., 2016; Ravi & Larochelle, 2016). In this
paper, we develop a gradient-based meta-learning algorithm similar to (Finn et al., 2017b) and suitable
for continuous adaptation of RL agents in nonstationary environments. More concretely, our agents
meta-learn to anticipate the changes in the environment and update their policies accordingly.
While virtually any changes in an environment could induce nonstationarity (e.g., changes in the
physics or characteristics of the agent), environments with multiple agents are particularly challenging

∗Correspondence: maruan.alshedivat.com. Work done while MA and TB interned at OpenAI.

1

