Published as a conference paper at ICLR 2018

STABILIZING ADVERSARIAL NETS WITH PREDICTION
METHODS

Abhay Yadav∗, Sohil Shah∗, Zheng Xu, David Jacobs, & Tom Goldstein
University of Maryland,
College Park, MD 20740, USA.
{jaiabhay, xuzh, tomg}@cs.umd.edu, sohilas@umd.edu,
djacobs@umiacs.umd.edu

ABSTRACT

Adversarial neural networks solve many important problems in data science, but
are notoriously difﬁcult to train. These difﬁculties come from the fact that optimal
weights for adversarial nets correspond to saddle points, and not minimizers, of
the loss function. The alternating stochastic gradient methods typically used for
such problems do not reliably converge to saddle points, and when convergence
does happen it is often highly sensitive to learning rates. We propose a simple
modiﬁcation of stochastic gradient descent that stabilizes adversarial networks. We
show, both in theory and practice, that the proposed method reliably converges
to saddle points, and is stable with a wider range of training parameters than a
non-prediction method. This makes adversarial networks less likely to “collapse,”
and enables faster training with larger learning rates.

1

INTRODUCTION

Adversarial networks play an important role in a variety of applications, including image genera-
tion (Zhang et al., 2017; Wang & Gupta, 2016), style transfer (Brock et al., 2017; Taigman et al.,
2017; Wang & Gupta, 2016; Isola et al., 2017), domain adaptation (Taigman et al., 2017; Tzeng et al.,
2017; Ganin & Lempitsky, 2015), imitation learning (Ho et al., 2016), privacy (Edwards & Storkey,
2016; Abadi & Andersen, 2016), fair representation (Mathieu et al., 2016; Edwards & Storkey, 2016),
etc. One particularly motivating application of adversarial nets is their ability to form generative
models, as opposed to the classical discriminative models (Goodfellow et al., 2014; Radford et al.,
2016; Denton et al., 2015; Mirza & Osindero, 2014).
While adversarial networks have the power to attack a wide range of previously unsolved problems,
they suffer from a major ﬂaw: they are difﬁcult to train. This is because adversarial nets try to
accomplish two objectives simultaneously; weights are adjusted to maximize performance on one
task while minimizing performance on another. Mathematically, this corresponds to ﬁnding a saddle
point of a loss function - a point that is minimal with respect to one set of weights, and maximal with
respect to another.
Conventional neural networks are trained by marching down a loss function until a minimizer is
reached (Figure 1a). In contrast, adversarial training methods search for saddle points rather than a
minimizer, which introduces the possibility that the training path “slides off” the objective functions
and the loss goes to −∞ (Figure 1b), resulting in “collapse” of the adversarial network. As a result,
many authors suggest using early stopping, gradients/weight clipping (Arjovsky et al., 2017), or
specialized objective functions (Goodfellow et al., 2014; Zhao et al., 2017; Arjovsky et al., 2017) to
maintain stability.
In this paper, we present a simple “prediction” step that is easily added to many training algorithms
for adversarial nets. We present theoretical analysis showing that the proposed prediction method is
asymptotically stable for a class of saddle point problems. Finally, we use a wide range of experiments
to show that prediction enables faster training of adversarial networks using large learning rates
without the instability problems that plague conventional training schemes.

∗Equal contribution

1

