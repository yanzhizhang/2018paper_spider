Published as a conference paper at ICLR 2018

MGAN: TRAINING GENERATIVE ADVERSARIAL NETS WITH
MULTIPLE GENERATORS

Quan Hoang
University of Massachusetts-Amherst
Amherst, MA, USA
qhoang@umass.edu

Tu Dinh Nguyen, Trung Le, Dinh Phung
PRaDA Centre, Deakin University
Geelong, Australia
{tu.nguyen,trung.l,dinh.phung}
@deakin.edu.au

ABSTRACT

We propose in this paper a new approach to train the Generative Adversarial Nets
(GANs) with a mixture of generators to overcome the mode collapsing problem.
The main intuition is to employ multiple generators, instead of using a single one
as in the original GAN. The idea is simple, yet proven to be extremely effective at
covering diverse data modes, easily overcoming the mode collapsing problem and
delivering state-of-the-art results. A minimax formulation was able to establish
among a classiﬁer, a discriminator, and a set of generators in a similar spirit with
GAN. Generators create samples that are intended to come from the same distribu-
tion as the training data, whilst the discriminator determines whether samples are
true data or generated by generators, and the classiﬁer speciﬁes which generator a
sample comes from. The distinguishing feature is that internal samples are created
from multiple generators, and then one of them will be randomly selected as ﬁnal
output similar to the mechanism of a probabilistic mixture model. We term our
method Mixture Generative Adversarial Nets (MGAN). We develop theoretical
analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD)
between the mixture of generators’ distributions and the empirical data distribu-
tion is minimal, whilst the JSD among generators’ distributions is maximal, hence
effectively avoiding the mode collapsing problem. By utilizing parameter sharing,
our proposed model adds minimal computational cost to the standard GAN, and
thus can also efﬁciently scale to large-scale datasets. We conduct extensive exper-
iments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and
ImageNet) to demonstrate the superior performance of our MGAN in achieving
state-of-the-art Inception scores over latest baselines, generating diverse and ap-
pealing recognizable objects at different resolutions, and specializing in capturing
different types of objects by the generators.

1

INTRODUCTION

Generative Adversarial Nets (GANs) (Goodfellow et al., 2014) are a recent novel class of deep
generative models that are successfully applied to a large variety of applications such as image, video
generation, image inpainting, semantic segmentation, image-to-image translation, and text-to-image
synthesis, to name a few (Goodfellow, 2016). From the game theory metaphor, the model consists of
a discriminator and a generator playing a two-player minimax game, wherein the generator aims to
generate samples that resemble those in the training data whilst the discriminator tries to distinguish
between the two as narrated in (Goodfellow et al., 2014). Training GAN, however, is challenging as
it can be easily trapped into the mode collapsing problem where the generator only concentrates on
producing samples lying on a few modes instead of the whole data space (Goodfellow, 2016).

1

