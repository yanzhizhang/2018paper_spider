Published as a conference paper at ICLR 2018

MODULAR CONTINUAL LEARNING IN A UNIFIED
VISUAL ENVIRONMENT

Blue Sheffer
Department of Psychology
Stanford University
Stanford, CA 94305
bsheffer@stanford.edu

Kevin T. Feigelis
Department of Physics
Stanford Neurosciences Institute
Stanford University
Stanford, CA 94305
feigelis@stanford.edu

Daniel L. K. Yamins
Departments of Psychology and Computer Science
Stanford Neurosciences Institute
Stanford University
Stanford, CA 94305
yamins@stanford.edu

ABSTRACT

A core aspect of human intelligence is the ability to learn new tasks quickly and
switch between them ﬂexibly. Here, we describe a modular continual reinforcement
learning paradigm inspired by these abilities. We ﬁrst introduce a visual interaction
environment that allows many types of tasks to be uniﬁed in a single framework.
We then describe a reward map prediction scheme that learns new tasks robustly
in the very large state and action spaces required by such an environment. We
investigate how properties of module architecture inﬂuence efﬁciency of task
learning, showing that a module motif incorporating speciﬁc design principles (e.g.
early bottlenecks, low-order polynomial nonlinearities, and symmetry) signiﬁcantly
outperforms more standard neural network motifs, needing fewer training examples
and fewer neurons to achieve high levels of performance. Finally, we present a
meta-controller architecture for task switching based on a dynamic neural voting
scheme, which allows new modules to use information learned from previously-
seen tasks to substantially improve their own learning efﬁciency.

INTRODUCTION

In the course of everyday functioning, people are constantly faced with real-world environ-
ments in which they are required to shift unpredictably between multiple, sometimes unfamiliar,
tasks (Botvinick & Cohen, 2014). They are nonetheless able to ﬂexibly adapt existing decision
schemas or build new ones in response to these challenges (Arbib, 1992). How humans support
such ﬂexible learning and task switching is largely unknown, both neuroscientiﬁcally and algorithmi-
cally (Wagner et al., 1998; Cole et al., 2013).
We investigate solving this problem with a neural module approach in which simple, task-specialized
decision modules are dynamically allocated on top of a largely-ﬁxed underlying sensory system (An-
dreas et al., 2015; Hu et al., 2017). The sensory system computes a general-purpose visual representa-
tion from which the decision modules read. While this sensory backbone can be large, complex, and
learned comparatively slowly with signiﬁcant amounts of training data, the task modules that deploy
information from the base representation must, in contrast, be lightweight, quick to be learned, and
easy to switch between. In the case of visually-driven tasks, results from neuroscience and computer
vision suggest the role of the ﬁxed general purpose visual representation may be played by the ventral
visual stream, modeled as a deep convolutional neural network (Yamins & DiCarlo, 2016; Razavian
et al., 2014). However, the algorithmic basis for how to efﬁciently learn and dynamically deploy
visual decision modules remains far from obvious.

1

