Published as a conference paper at ICLR 2018

ACTIVE NEURAL LOCALIZATION

Devendra Singh Chaplot, Emilio Parisotto, Ruslan Salakhutdinov
Machine Learning Department
School of Computer Science
Carnegie Mellon University
{chaplot,eparisot,rsalakhu}@cs.cmu.edu

ABSTRACT

Localization is the problem of estimating the location of an autonomous agent from
an observation and a map of the environment. Traditional methods of localization,
which ﬁlter the belief based on the observations, are sub-optimal in the number of
steps required, as they do not decide the actions taken by the agent. We propose
“Active Neural Localizer”, a fully differentiable neural network that learns to
localize accurately and efﬁciently. The proposed model incorporates ideas of
traditional ﬁltering-based localization methods, by using a structured belief of
the state with multiplicative interactions to propagate belief, and combines it
with a policy model to localize accurately while minimizing the number of steps
required for localization. Active Neural Localizer is trained end-to-end with
reinforcement learning. We use a variety of simulation environments for our
experiments which include random 2D mazes, random mazes in the Doom game
engine and a photo-realistic environment in the Unreal game engine. The results
on the 2D environments show the effectiveness of the learned policy in an idealistic
setting while results on the 3D environments demonstrate the model’s capability
of learning the policy and perceptual model jointly from raw-pixel based RGB
observations. We also show that a model trained on random textures in the Doom
environment generalizes well to a photo-realistic ofﬁce space environment in the
Unreal engine.
INTRODUCTION

1
Localization is the problem of estimating the position of an autonomous agent given a map of
the environment and agent observations. The ability to localize under uncertainity is required by
autonomous agents to perform various downstream tasks such as planning, exploration and target-
navigation. Localization is considered as one of the most fundamental problems in mobile robotics
(Cox & Wilfong, 1990; Borenstein et al., 1996). Localization is useful in many real-world applications
such as autonomous vehicles, factory robots and delivery drones.
In this paper we tackle the global localization problem where the initial position of the agent is
unknown. Despite the long history of research, global localization is still an open problem, and
there are not many methods developed which can be learnt from data in an end-to-end manner,
instead typically requiring signiﬁcant hand-tuning and feature selection by domain experts. Another
limitation of majority of localization approaches till date is that they are passive, meaning that they
passively estimate the position of the agent from the stream of incoming observations, and do not
have the ability to decide the actions taken by the agent. The ability to decide the actions can result in
faster as well as more accurate localization as the agent can learn to navigate quickly to unambiguous
locations in the environment.
We propose “Active Neural Localizer”, a neural network model capable of active localization using
raw pixel-based observations and a map of the environment12. Based on the Bayesian ﬁltering
algorithm for localization (Fox et al., 2003), the proposed model contains a perceptual model to
estimate the likelihood of the agent’s observations, a structured component for representing the belief,
multiplicative interactions to propagate the belief based on observations and a policy model over the
current belief to localize accurately while minimizing the number of steps required for localization.
The entire model is fully differentiable and trained using reinforcement learning, allowing the
perceptual model and the policy model to be learnt simultaneously in an end-to-end fashion. A variety

1 Demo videos: https://devendrachaplot.github.io/projects/Neural-Localization
2 The code is available at https://github.com/devendrachaplot/Neural-Localization

1

