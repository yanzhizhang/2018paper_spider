Published as a conference paper at ICLR 2018

TOWARDS DEEP LEARNING MODELS RESISTANT TO
ADVERSARIAL ATTACKS

Aleksander M ˛adry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu∗
Department of Electrical Engineering and Computer Science
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{madry,amakelov,ludwigs,tsipras,avladu}@mit.edu

ABSTRACT

Recent work has demonstrated that neural networks are vulnerable to adversarial
examples, i.e., inputs that are almost indistinguishable from natural data and yet
classiﬁed incorrectly by the network. To address this problem, we study the
adversarial robustness of neural networks through the lens of robust optimization.
This approach provides us with a broad and unifying view on much prior work on
this topic. Its principled nature also enables us to identify methods for both training
and attacking neural networks that are reliable and, in a certain sense, universal.
In particular, they specify a concrete security guarantee that would protect against
a well-deﬁned class of adversaries. These methods let us train networks with
signiﬁcantly improved resistance to a wide range of adversarial attacks. They also
suggest robustness against a ﬁrst-order adversary as a natural security guarantee.
We believe that robustness against such well-deﬁned classes of adversaries is an
important stepping stone towards fully resistant deep learning models.

1

INTRODUCTION

Recent breakthroughs in computer vision and speech recognition are bringing trained classiﬁers
into the center of security-critical systems. Important examples include vision for autonomous cars,
face recognition, and malware detection. These developments make security aspects of machine
learning increasingly important. In particular, resistance to adversarially chosen inputs is becoming
a crucial design goal. While trained models tend to be very effective in classifying benign inputs,
recent work (Dalvi et al., 2004; Szegedy et al., 2013; Goodfellow et al., 2014; Nguyen et al., 2015;
Sharif et al., 2016) shows that an adversary is often able to manipulate the input so that the model
produces an incorrect output.
This phenomenon has received particular attention in the context of deep neural networks, and there is
now a quickly growing body of work on this topic (Fawzi et al., 2015; Kurakin et al., 2016; Papernot
& McDaniel, 2016; Rozsa et al., 2016; Torkamani, 2016; Sokolic et al., 2016; Tramèr et al., 2017b).
Computer vision presents a particularly striking challenge: very small changes to the input image
can fool state-of-the-art neural networks with high probability (Szegedy et al., 2013; Goodfellow
et al., 2014; Nguyen et al., 2015; Sharif et al., 2016; Moosavi-Dezfooli et al., 2016). This holds even
when the benign example was classiﬁed correctly, and the change is imperceptible to a human. Apart
from the security implications, this phenomenon also demonstrates that our current models are not
learning the underlying concepts in a robust manner. All these ﬁndings raise a fundamental question:

How can we learn models robust to adversarial inputs?

There are now many proposed defense mechanisms for the adversarial setting. Examples include
defensive distillation (Papernot et al., 2016a; Papernot & McDaniel, 2016), feature squeezing (Xu
et al., 2017), and several detection approaches for adversarial inputs (see Carlini & Wagner (2017) for
references). While these works constitute important ﬁrst steps in exploring the realm of possibilities,
they do not offer a good understanding of the guarantees they provide. We can never be certain

∗Authors ordered alphabetically.

1

