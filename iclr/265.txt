Published as a conference paper at ICLR 2018

DEEP ACTIVE LEARNING FOR NAMED ENTITY
RECOGNITION

Yanyao Shen
UT Austin
Austin, TX 78712
shenyanyao@utexas.edu

Hyokun Yun
Amazon Web Services
Seattle, WA 98101
yunhyoku@amazon.com

Zachary C. Lipton
Amazon Web Services
Seattle, WA 98101
liptoz@amazon.com

Yakov Kronrod
Amazon Web Services
Seattle, WA 98101
kronrod@amazon.com

Animashree Anandkumar
Amazon Web Services
Seattle, WA 98101
anima@amazon.com

ABSTRACT

Deep learning has yielded state-of-the-art performance on many natural language
processing tasks including named entity recognition (NER). However, this typically
requires large amounts of labeled data. In this work, we demonstrate that the
amount of labeled training data can be drastically reduced when deep learning is
combined with active learning. While active learning is sample-efﬁcient, it can
be computationally expensive since it requires iterative retraining. To speed this
up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM
model consisting of convolutional character and word encoders and a long short
term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art
performance on standard datasets for the task while being computationally much
more efﬁcient than best performing models. We carry out incremental active
learning, during the training process, and are able to nearly match state-of-the-art
performance with just 25% of the original training data.

1

INTRODUCTION

Over the past few years, papers applying deep neural networks (DNNs) to the task of named entity
recognition (NER) have successively advanced the state-of-the-art (Collobert et al., 2011; Huang
et al., 2015; Lample et al., 2016; Chiu & Nichols, 2016; Yang et al., 2016). However, under typical
training procedures, the advantages of deep learning diminish when working with small datasets.
For instance, on the OntoNotes-5.0 English dataset, whose training set contains 1,088,503 words,
a DNN model outperforms the best shallow model by 2.24% as measured by F1 score (Chiu &
Nichols, 2016). However, on the comparatively small CoNLL-2003 English dataset, whose training
set contains 203,621 words, the best DNN model enjoys only a 0.4% advantage. To make deep
learning more broadly useful, it is crucial to reduce its training data requirements.
Generally, the annotation budget for labeling is far less than the total number of available (unlabeled)
samples. For NER, getting unlabeled data is practically free, owing to the large amount of content
that can be efﬁciently scraped off the web. On the other hand, it is especially expensive to obtain
annotated data for NER since it requires multi-stage pipelines with sufﬁciently well-trained annotators
(Kilicoglu et al., 2016; Bontcheva et al., 2017). In such cases, active learning offers a promising
approach to efﬁciently select the set of samples for labeling. Unlike the supervised learning setting,
in which examples are drawn and labeled at random, in the active learning setting, the algorithm can
choose which examples to label.
Active learning aims to select a more informative set of examples in contrast to supervised learning,
which is trained on a set of randomly drawn examples. A central challenge in active learning is to
determine what constitutes more informative and how the active learner can recognize this based on
what it already knows. The most common approach is uncertainty sampling, in which the model
preferentially selects examples for which it’s current prediction is least conﬁdent. Other approaches

1

