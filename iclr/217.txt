Published as a conference paper at ICLR 2018

NEURAL SKETCH LEARNING FOR CONDITIONAL
PROGRAM GENERATION

Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine
Department of Computer Science
Rice University
Houston, TX 77005, USA.
{vijay, letao.qi, swarat, cmj4}@rice.edu

ABSTRACT

We study the problem of generating source code in a strongly typed, Java-like
programming language, given a label (for example a set of API calls or types) car-
rying a small amount of information about the code that is desired. The generated
programs are expected to respect a “realistic” relationship between programs and
labels, as exempliﬁed by a corpus of labeled programs available during training.
Two challenges in such conditional program generation are that the generated pro-
grams must satisfy a rich set of syntactic and semantic constraints, and that source
code contains many low-level features that impede learning. We address these
problems by training a neural generator not on code but on program sketches, or
models of program syntax that abstract out names and operations that do not gen-
eralize across programs. During generation, we infer a posterior distribution over
sketches, then concretize samples from this distribution into type-safe programs
using combinatorial techniques. We implement our ideas in a system for generat-
ing API-heavy Java code, and show that it can often predict the entire body of a
method given just a few API calls or data types that appear in the method.

1

INTRODUCTION

Neural networks have been successfully applied to many generative modeling tasks in the recent
past (Oord et al., 2016; Ha & Eck, 2017; Vinyals et al., 2015). However, the use of these mod-
els in generating highly structured text remains relatively understudied. In this paper, we present a
method, combining neural and combinatorial techniques, for the condition generation of an impor-
tant category of such text: the source code of programs in Java-like programming languages.
The speciﬁc problem we consider is one of supervised learning. During training, we are given a set
of programs, each program annotated with a label, which may contain information such as the set
of API calls or the types used in the code. Our goal is to learn a function g such that for a test case
of the form (X, Prog) (where Prog is a program and X is a label), g(X) is a compilable, type-safe
program that is equivalent to Prog.
This problem has immediate applications in helping humans solve programming tasks (Hindle et al.,
2012; Raychev et al., 2014). In the usage scenario that we envision, a human programmer uses a
label to specify a small amount of information about a program that they have in mind. Based on
this information, our generator seeks to produce a program equivalent to the “target” program, thus
performing a particularly powerful form of code completion.
Conditional program generation is a special case of program synthesis (Manna & Waldinger, 1971;
Summers, 1977), the classic problem of generating a program given a constraint on its behavior.
This problem has received signiﬁcant interest in recent years (Alur et al., 2013; Gulwani et al.,
2017). In particular, several neural approaches to program synthesis driven by input-output examples
have emerged (Balog et al., 2017; Parisotto et al., 2016; Devlin et al., 2017). Fundamentally, these
approaches are tasked with associating a program’s syntax with its semantics. As doing so in general
is extremely hard, these methods choose to only generate programs in highly controlled domain-
speciﬁc languages. For example, Balog et al. (2017) consider a functional language in which the

1

