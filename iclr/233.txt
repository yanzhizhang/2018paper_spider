Published as a conference paper at ICLR 2018

VARIATIONAL MESSAGE PASSING WITH STRUCTURED
INFERENCE NETWORKS

Wu Lin∗, Nicolas Hubacher∗, Mohammad Emtiyaz Khan ∗
RIKEN Center for Adavanced Intelligene Project, Tokyo, Japan
wlin2018@cs.ubc.ca, nicolas.hubacher@outlook.com, emtiyaz@gmail.com

ABSTRACT

Recent efforts on combining deep models with probabilistic graphical models are
promising in providing ﬂexible models that are also easy to interpret. We propose
a variational message-passing algorithm for variational inference in such models.
We make three contributions. First, we propose structured inference networks
that incorporate the structure of the graphical model in the inference network of
variational auto-encoders (VAE). Second, we establish conditions under which
such inference networks enable fast amortized inference similar to VAE. Finally,
we derive a variational message passing algorithm to perform efﬁcient natural-
gradient inference while retaining the efﬁciency of the amortized inference. By
simultaneously enabling structured, amortized, and natural-gradient inference for
deep structured models, our method simpliﬁes and generalizes existing methods.

1

INTRODUCTION

To analyze real-world data, machine learning relies on models that can extract useful patterns. Deep
Neural Networks (DNNs) are a popular choice for this purpose because they can learn ﬂexible
representations. Another popular choice are probabilistic graphical models (PGMs) which can ﬁnd
interpretable structures in the data. Recent work on combining these two types of models hopes to
exploit their complimentary strengths and provide powerful models that are also easy to interpret
(Johnson et al., 2016; Krishnan et al., 2015; Archer et al., 2015; Fraccaro et al., 2016).
To apply such hybrid models to real-world problems, we need efﬁcient algorithms that can extract
useful structure from the data. However, the two ﬁelds of deep learning and PGMs traditionally
use different types of algorithms. For deep learning, stochastic-gradient methods are the most
popular choice, e.g., those based on back-propagation. These algorithms are not only widely ap-
plicable, but can also employ amortized inference to enable fast inference at test time (Rezende
et al., 2014; Kingma & Welling, 2013). On the other hand, most popular algorithms for PGMs
exploit the model’s graphical conjugacy structure to gain computational efﬁciency, e.g., variational
message passing (VMP) (Winn & Bishop, 2005), expectation propagation (Minka, 2001), Kalman
ﬁltering (Ghahramani & Hinton, 1996; 2000), and more recently natural-gradient variational infer-
ence (Honkela et al., 2011) and stochastic variational inference (Hoffman et al., 2013). In short, the
two ﬁelds of deep learning and probabilistic modelling employ fundamentally different inferential
strategies and a natural question is, whether we can design algorithms that combine their respective
strengths.
There have been several attempts to design such methods in the recent years, e.g., Krishnan et al.
(2015; 2017); Fraccaro et al. (2016); Archer et al. (2015); Johnson et al. (2016); Chen et al. (2015).
Our work in this paper is inspired by the previous work of Johnson et al. (2016) that aims to combine
message-passing, natural-gradient, and amortized inference. Our proposed method in this paper
simpliﬁes and generalizes the method of Johnson et al. (2016).
To do so, we propose Structured Inference Networks (SIN) that incorporate the PGM structure in
the standard inference networks used in variational auto-encoders (VAE) (Kingma & Welling, 2013;
Rezende et al., 2014). We derive conditions under which such inference networks can enable fast
amortized inference similar to VAE. By using a recent VMP method of Khan & Lin (2017), we

∗Equal contributions. Wu Lin is now at the University of British Columbia, Vancouver, Canada.

1

