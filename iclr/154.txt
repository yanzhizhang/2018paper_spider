Published as a conference paper at ICLR 2018

MEMORY ARCHITECTURES IN RECURRENT NEURAL
NETWORK LANGUAGE MODELS

Dani Yogatama♣, Yishu Miao♠, Gabor Melis♣, Wang Ling♣, Adhiguna Kuncoro♣♠
Chris Dyer♣, Phil Blunsom♣♠
♣DeepMind and ♠University of Oxford
dyogatama@google.com, yishu.miao@cs.ox.ac.uk
{melisgl,lingwang,akuncoro,cdyer,pblunsom}@google.com

ABSTRACT

We compare and analyze sequential, random access, and stack memory architec-
tures for recurrent neural network language models. Our experiments on the Penn
Treebank and Wikitext-2 datasets show that stack-based memory architectures
consistently achieve the best performance in terms of held out perplexity. We also
propose a generalization to existing continuous stack models (Joulin & Mikolov,
2015; Grefenstette et al., 2015) to allow a variable number of pop operations more
naturally that further improves performance. We further evaluate these language
models in terms of their ability to capture non-local syntactic dependencies on a
subject-verb agreement dataset (Linzen et al., 2016) and establish new state of the
art results using memory augmented language models. Our results demonstrate
the value of stack-structured memory for explaining the distribution of words in
natural language, in line with linguistic theories claiming a context-free backbone
for natural language.

1

INTRODUCTION

Sequential recurrent neural networks such as LSTMs (Hochreiter & Schmidhuber, 1997) are the
basis of state-of-the-art models of natural language in various tasks. They effectively learn to capture
dependencies between events separated in time by learning to store and retrieve information in a
hidden state. However, the ability of these methods to discover long-term dependencies is limited by
the capacity of their hidden state and the difﬁculty of propagating reliable gradients. For example,
LSTM language models have been shown to struggle to capture non-sequential syntactic dependencies
in complex sentences without explicit supervision (Linzen et al., 2016). As an illustration of the
kind of dependencies they have difﬁculty learning, in the sentence, the loss of basic needs providers
emigrating from impoverished countries has a damaging effect, correctly predicting singular has
rather than its plural form have requires that the LSTM have learned that it depends on the subject, in
this case the ﬁrst noun (loss) rather than any of the intervening non-subject nouns, such as countries.
Linzen et al. (2016) show that LSTM language models fail to capture this kind of dependencies,
especially as the number of attractors (underlined) increases.
Attempts to improve language models’ ability to capture non-local dependencies have recently
been undertaken by introducing an external memory components. These include (i) a soft attention
mechanism (Daniluk et al., 2017) and (ii) an explicit memory block or cache model (Tran et al., 2016;
Grave et al., 2017). However, since very local context is often most highly informative for predicting
the next word, existing memory-augmented RNN LMs use memory just to store information about
local context (Daniluk et al., 2017).
In this work, we compare several memory architectures for recurrent neural network language models.
Since our goal is to evaluate how well these types of memory architectures learn long term and
syntactic dependencies, we focus on language models that are static as opposed to non-static models
such as neural cache (Grave et al., 2017) and dynamic evaluation (Krause et al., 2017) that can update
their distribution at test time. We consider increasing the capacity of a purely sequential memory
model by increasing the capacity of an LSTM, a random access memory model as typiﬁed by an
attention-based LSTM, and a new variant of a stack augmented recurrent neural network.

1

