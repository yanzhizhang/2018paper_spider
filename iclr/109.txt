Published as a conference paper at ICLR 2018

COMPRESSING WORD EMBEDDINGS VIA DEEP
COMPOSITIONAL CODE LEARNING

Raphael Shu
The University of Tokyo
shu@nlab.ci.i.u-tokyo.ac.jp

Hideki Nakayama
The University of Tokyo
nakayama@ci.i.u-tokyo.ac.jp

ABSTRACT

Natural language processing (NLP) models often require a massive number of pa-
rameters for word embeddings, resulting in a large storage or memory footprint.
Deploying neural NLP models to mobile devices requires compressing the word
embeddings without any signiﬁcant sacriﬁces in performance. For this purpose,
we propose to construct the embeddings with few basis vectors. For each word,
the composition of basis vectors is determined by a hash code. To maximize
the compression rate, we adopt the multi-codebook quantization approach instead
of binary coding scheme. Each code is composed of multiple discrete numbers,
such as (3, 2, 1, 8), where the value of each component is limited to a ﬁxed range.
We propose to directly learn the discrete codes in an end-to-end neural network
by applying the Gumbel-softmax trick. Experiments show the compression rate
achieves 98% in a sentiment analysis task and 94% ∼ 99% in machine translation
tasks without performance loss. In both tasks, the proposed method can improve
the model performance by slightly lowering the compression rate. Compared to
other approaches such as character-level segmentation, the proposed method is
language-independent and does not require modiﬁcations to the network architec-
ture.

1

INTRODUCTION

Word embeddings play an important role in neural-based natural language processing (NLP) models.
Neural word embeddings encapsulate the linguistic information of words in continuous vectors.
However, as each word is assigned an independent embedding vector, the number of parameters
in the embedding matrix can be huge. For example, when each embedding has 500 dimensions,
the network has to hold 100M embedding parameters to represent 200K words. In practice, for a
simple sentiment analysis model, the word embedding parameters account for 98.8% of the total
parameters.
As only a small portion of the word embeddings is selected in the forward pass, the giant embedding
matrix usually does not cause a speed issue. However, the massive number of parameters in the
neural network results in a large storage or memory footprint. When other components of the neural
network are also large, the model may fail to ﬁt into GPU memory during training. Moreover, as
the demand for low-latency neural computation for mobile platforms increases, some neural-based
models are expected to run on mobile devices. Thus, it is becoming more important to compress the
size of NLP models for deployment to devices with limited memory or storage capacity.
In this study, we attempt to reduce the number of parameters used in word embeddings without
hurting the model performance. Neural networks are known for the signiﬁcant redundancy in the
connections (Denil et al., 2013). In this work, we further hypothesize that learning independent
embeddings causes more redundancy in the embedding vectors, as the inter-similarity among words
is ignored. Some words are very similar regarding the semantics. For example, “dog” and “dogs”
have almost the same meaning, except one is plural. To efﬁciently represent these two words, it
is desirable to share information between the two embeddings. However, a small portion in both
vectors still has to be trained independently to capture the syntactic difference.
Following the intuition of creating partially shared embeddings, instead of assigning each word a
unique ID, we represent each word w with a code Cw = (C 1
w is

w ). Each component C i

w, C 2

w, ..., CM

1

