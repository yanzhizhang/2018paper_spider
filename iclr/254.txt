Published as a conference paper at ICLR 2018

LEARNING APPROXIMATE INFERENCE NETWORKS
FOR STRUCTURED PREDICTION

Kevin Gimpel

Lifu Tu
Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA
{lifu,kgimpel}@ttic.edu

ABSTRACT

Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use
neural network architectures to deﬁne energy functions that can capture arbitrary
dependencies among parts of structured outputs. Prior work used gradient descent
for inference, relaxing the structured output to a set of continuous variables and then
optimizing the energy with respect to them. We replace this use of gradient descent
with a neural network trained to approximate structured argmax inference. This
“inference network” outputs continuous values that we treat as the output structure.
We develop large-margin training criteria for joint training of the structured energy
function and inference network. On multi-label classiﬁcation we report speed-ups
of 10-60x compared to (Belanger et al., 2017) while also improving accuracy.
For sequence labeling with simple structured energies, our approach performs
comparably to exact inference while being much faster at test time. We then
demonstrate improved accuracy by augmenting the energy with a “label language
model” that scores entire output label sequences, showing it can improve handling
of long-distance dependencies in part-of-speech tagging. Finally, we show how
inference networks can replace dynamic programming for test-time inference in
conditional random ﬁelds, suggestive for their general use for fast inference in
structured settings.

1

INTRODUCTION

Energy-based modeling (LeCun et al., 2006) associates a scalar measure of compatibility to each
conﬁguration of input and output variables. Given an input x, the predicted output ˆy is chosen
by minimizing an energy function E(x, ˆy). For structured prediction, the parameterization of the
energy function can leverage domain knowledge about the structured output space. However, learning
and prediction become complex.
Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use an energy function
to score structured outputs, and perform inference by using gradient descent to iteratively optimize
the energy with respect to the outputs. Belanger et al. (2017) develop an “end-to-end” method that
unrolls an approximate energy minimization algorithm into a ﬁxed-size computation graph that is
trainable by gradient descent. After learning the energy function, however, they still must use gradient
descent for test-time inference.
We replace the gradient descent approach with a neural network trained to do inference, which we
call an inference network. It can have any architecture such that it takes an input x and returns an
output interpretable as a y. As in prior work, we relax y from discrete to continuous. For multi-label
classiﬁcation, we use a feed-forward network that outputs a vector. We assign a single label to each
dimension of the vector, interpreting its value as the probability of predicting that label. For sequence
labeling, we output a distribution over predicted labels at each position in the sequence. We adapt
the energy functions such that they can operate with both discrete ground truth outputs and outputs
generated by our inference networks.
We deﬁne large-margin training objectives to jointly train energy functions and inference networks.
Our training objectives resemble the alternating optimization framework of generative adversarial
networks (GANs; Goodfellow et al. 2014): the inference network is analogous to the generator and
the energy function is analogous to the discriminator. Our approach avoids argmax computations,

1

