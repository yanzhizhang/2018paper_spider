Published as a conference paper at ICLR 2018

APPRENTICE: USING KNOWLEDGE DISTILLATION
TECHNIQUES TO IMPROVE LOW-PRECISION NET-
WORK ACCURACY

Asit Mishra & Debbie Marr
Accelerator Architecture Lab
Intel Labs
{asit.k.mishra,debbie.marr}@intel.com

ABSTRACT

Deep learning networks have achieved state-of-the-art accuracies on computer vi-
sion workloads like image classiﬁcation and object detection. The performant
systems, however, typically involve big models with numerous parameters. Once
trained, a challenging aspect for such top performing models is deployment on re-
source constrained inference systems — the models (often deep networks or wide
networks or both) are compute and memory intensive. Low-precision numerics
and model compression using knowledge distillation are popular techniques to
lower both the compute requirements and memory footprint of these deployed
models. In this paper, we study combination of these two techniques and show
that the performance of low-precision networks can be signiﬁcantly improved
by using knowledge distillation techniques. Our approach, Apprentice, achieves
state-of-the-art accuracies using ternary precision and 4-bit precision for variants
of ResNet architecture on ImageNet dataset. We present three schemes using
which one can apply knowledge distillation techniques to various stages of the
train-and-deploy pipeline.

1

INTRODUCTION

Background: Today’s high performing deep neural networks (DNNs) for computer vision applica-
tions comprise of multiple layers and involve numerous parameters. These networks have O(Giga-
FLOPS) compute requirements and generate models which are O(Mega-Bytes) in storage (Canziani
et al., 2016). Further, the memory and compute requirements during training and inference are quite
different (Mishra et al., 2017). Training is performed on big datasets with large batch-sizes where
memory footprint of activations dominates the model memory footprint. On the other hand, batch-
size during inference is typically small and the model’s memory footprint dominates the runtime
memory requirements.
Because of complexity in compute, memory and storage requirements, training phase of the net-
works is performed on CPU and/or GPU clusters in a distributed computing environment. Once
trained, a challenging aspect is deployment of trained models on resource constrained inference
systems such as portable devices or sensor networks, and for applications in which real-time predic-
tions are required. Performing inference on edge-devices comes with severe constraints on memory,
compute and power. Additionally, ensemble based methods, which one can potentially use to get
improved accuracy predictions, become prohibitive in resource constrained systems.
Quantization using low-precision numerics (Vanhoucke et al., 2011; Zhou et al., 2016; Lin et al.,
2015; Miyashita et al., 2016; Gupta et al., 2015; Zhu et al., 2016; Rastegari et al., 2016; Courbariaux
et al., 2015; Umuroglu et al., 2016; Mishra et al., 2017) and model compression (Buciluˇa et al.,
2006; Hinton et al., 2015; Romero et al., 2014) have emerged as popular solutions for resource
constrained deployment scenarios. With quantization, a low-precision version of network model is
generated and deployed on the device. Operating in lower precision mode reduces compute as well
as data movement and storage requirements. However, majority of existing works in low-precision
DNNs sacriﬁce accuracy over baseline full-precision networks. With model compression, a smaller

1

