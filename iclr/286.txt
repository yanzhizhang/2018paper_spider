Published as a conference paper at ICLR 2018

ZERO-SHOT VISUAL IMITATION

Deepak Pathak∗, Parsa Mahmoudieh∗, Guanghao Luo∗, Pulkit Agrawal∗, Dian Chen,
Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, Trevor Darrell

UC Berkeley
{pathak,parsa.m,michaelluo,pulkitag,dianchen,
fredshentu,shelhamer,malik,efros,trevor}@cs.berkeley.edu

ABSTRACT

The current dominant paradigm for imitation learning relies on strong supervision
of expert actions to learn both what and how to imitate. We pursue an alternative
paradigm wherein an agent ﬁrst explores the world without any expert supervision
and then distills its experience into a goal-conditioned skill policy with a novel
forward consistency loss.
In our framework, the role of the expert is only to
communicate the goals (i.e., what to imitate) during inference. The learned policy
is then employed to mimic the expert (i.e., how to imitate) after seeing just a
sequence of images demonstrating the desired task. Our method is “zero-shot”
in the sense that the agent never has access to expert actions during training
or for the task demonstration at inference. We evaluate our zero-shot imitator
in two real-world settings: complex rope manipulation with a Baxter robot and
navigation in previously unseen ofﬁce environments with a TurtleBot. Through
further experiments in VizDoom simulation, we provide evidence that better
mechanisms for exploration lead to learning a more capable policy which in turn
improves end task performance. Videos, models, and more details are available
at https://pathak22.github.io/zeroshot-imitation/.

1

INTRODUCTION

Imitating expert demonstration is a powerful mechanism for learning to perform tasks from raw
sensory observations. The current dominant paradigm in learning from demonstration (LfD) (Ar-
gall et al., 2009; Ng & Russell, 2000; Pomerleau, 1989; Schaal, 1999) requires the expert to either
manually move the robot joints (i.e., kinesthetic teaching) or teleoperate the robot to execute the
desired task. The expert typically provides multiple demonstrations of a task at training time, and
this generates data in the form of observation-action pairs from the agent’s point of view. The agent
then distills this data into a policy for performing the task of interest. Such a heavily supervised
approach, where it is necessary to provide demonstrations by controlling the robot, is incredibly te-
dious for the human expert. Moreover, for every new task that the robot needs to execute, the expert
is required to provide a new set of demonstrations.
Instead of communicating how to perform a task via observation-action pairs, a more general formu-
lation allows the expert to communicate only what needs to be done by providing the observations of
the desired world states via a video or a sparse sequence of images. This way, the agent is required to
infer how to perform the task (i.e., actions) by itself. In psychology, this is known as observational
learning (Bandura & Walters, 1977). While this is a harder learning problem, it is a more interesting
setting, because the expert can demonstrate multiple tasks quickly and easily.
An agent without any prior knowledge will ﬁnd it extremely hard to imitate a task by simply watch-
ing a visual demonstration in all but the simplest of cases. Thus, the natural question is: in order
to imitate, what form of prior knowledge must the agent possess? A large body of work (Breazeal
& Scassellati, 2002; Dillmann, 2004; Ikeuchi & Suehiro, 1994; Kuniyoshi et al., 1989; 1994; Yang
et al., 2015) has sought to capture prior knowledge by manually pre-deﬁning the state that must be

∗Denotes equal contribution.

1

