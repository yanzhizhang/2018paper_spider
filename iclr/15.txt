Published as a conference paper at ICLR 2018

TREEQN AND ATREEC:
DIFFERENTIABLE TREE-STRUCTURED MODELS FOR
DEEP REINFORCEMENT LEARNING

Gregory Farquhar1
gregory.farquhar@cs.ox.ac.uk

Tim Rockt¨aschel1
tim.rocktaschel@cs.ox.ac.uk

Maximilian Igl1
maximilian.igl@cs.ox.ac.uk

Shimon Whiteson1
shimon.whiteson@cs.ox.ac.uk

1University of Oxford, United Kingdom

ABSTRACT

Combining deep model-free reinforcement learning with on-line planning is a
promising approach to building on the successes of deep RL. On-line planning with
look-ahead trees has proven successful in environments where transition models are
known a priori. However, in complex environments where transition models need
to be learned from data, the deﬁciencies of learned models have limited their utility
for planning. To address these challenges, we propose TreeQN, a differentiable,
recursive, tree-structured model that serves as a drop-in replacement for any value
function network in deep RL with discrete actions. TreeQN dynamically constructs
a tree by recursively applying a transition model in a learned abstract state space
and then aggregating predicted rewards and state-values using a tree backup to
estimate Q-values. We also propose ATreeC, an actor-critic variant that augments
TreeQN with a softmax layer to form a stochastic policy network. Both approaches
are trained end-to-end, such that the learned model is optimised for its actual use
in the tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C
on a box-pushing task, as well as n-step DQN and value prediction networks (Oh
et al., 2017) on multiple Atari games. Furthermore, we present ablation studies that
demonstrate the effect of different auxiliary losses on learning transition models.

1

INTRODUCTION

A promising approach to improving model-free deep reinforcement learning (RL) is to combine it
with on-line planning. The model-free value function can be viewed as a rough global estimate which
is then locally reﬁned on the ﬂy for the current state by the on-line planner. Crucially, this does not
require new samples from the environment but only additional computation, which is often available.
One strategy for on-line planning is to use look-ahead tree search (Knuth & Moore, 1975; Browne
et al., 2012). Traditionally, such methods have been limited to domains where perfect environment
simulators are available, such as board or card games (Coulom, 2006; Sturtevant, 2008). However, in
general, models for complex environments with high dimensional observation spaces and complex
dynamics must be learned from agent experience. Unfortunately, to date, it has proven difﬁcult to
learn models for such domains with sufﬁcient ﬁdelity to realise the beneﬁts of look-ahead planning
(Oh et al., 2015; Talvitie, 2017).
A simple approach to learning environment models is to maximise a similarity metric between model
predictions and ground truth in the observation space. This approach has been applied with some
success in cases where model ﬁdelity is less important, e.g., for improving exploration (Chiappa et al.,
2017; Oh et al., 2015). However, this objective causes signiﬁcant model capacity to be devoted to
predicting irrelevant aspects of the environment dynamics, such as noisy backgrounds, at the expense
of value-critical features that may occupy only a small part of the observation space (Pathak et al.,

1

