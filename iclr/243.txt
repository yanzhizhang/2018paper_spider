Published as a conference paper at ICLR 2018

SOBOLEV GAN

Youssef Mroueh†, Chun-Liang Li◦,(cid:63), Tom Sercu†,(cid:63), Anant Raj♦,(cid:63) & Yu Cheng†
† IBM Research AI
◦ Carnegie Mellon University
♦ Max Planck Institute for Intelligent Systems
(cid:63) denotes Equal Contribution
{mroueh,chengyu}@us.ibm.com, chunlial@cs.cmu.edu,
tom.sercu1@ibm.com,anant.raj@tuebingen.mpg.de

ABSTRACT

We propose a new Integral Probability Metric (IPM) between distributions: the
Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distribu-
tions for functions (critic) restricted to a Sobolev ball deﬁned with respect to a
dominant measure µ. We show that the Sobolev IPM compares two distributions
in high dimensions based on weighted conditional Cumulative Distribution Func-
tions (CDF) of each coordinate on a leave one out basis. The Dominant measure
µ plays a crucial role as it deﬁnes the support on which conditional CDFs are
compared. Sobolev IPM can be seen as an extension of the one dimensional Von-
Mises Cram´er statistics to high dimensional distributions. We show how Sobolev
IPM can be used to train Generative Adversarial Networks (GANs). We then ex-
ploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally
we show that a variant of Sobolev GAN achieves competitive results in semi-
supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic
by Sobolev GAN which relates to Laplacian regularization. 1

1

INTRODUCTION

In order to learn Generative Adversarial Networks (Goodfellow et al., 2014), it is now well estab-
lished that the generator should mimic the distribution of real data, in the sense of a certain dis-
crepancy measure. Discrepancies between distributions that measure the goodness of the ﬁt of the
neural generator to the real data distribution has been the subject of many recent studies (Arjovsky
& Bottou, 2017; Nowozin et al., 2016; Kaae Sønderby et al., 2017; Mao et al., 2017; Arjovsky et al.,
2017; Gulrajani et al., 2017; Mroueh et al., 2017; Mroueh & Sercu, 2017; Li et al., 2017), most of
which focus on training stability.
In terms of data modalities, most success was booked in plausible natural image generation after the
introduction of Deep Convolutional Generative Adversarial Networks (DCGAN) (Radford et al.,
2015). This success is not only due to advances in training generative adversarial networks in terms
of loss functions (Arjovsky et al., 2017) and stable algorithms, but also to the representation power
of convolutional neural networks in modeling images and in ﬁnding sufﬁcient statistics that capture
the continuous density function of natural images. When moving to neural generators of discrete
sequences generative adversarial networks theory and practice are still not very well understood.
Maximum likelihood pre-training or augmentation, in conjunction with the use of reinforcement
learning techniques were proposed in many recent works for training GAN for discrete sequences
generation (Yu et al., 2016; Che et al., 2017; Hjelm et al., 2017; Rajeswar et al., 2017). Other
methods included using the Gumbel Softmax trick (Kusner & Hern´andez-Lobato, 2016) and the
use of auto-encoders to generate adversarially discrete sequences from a continuous space (Zhao
et al., 2017). End to end training of GANs for discrete sequence generation is still an open prob-
lem (Press et al., 2017). Empirical successes of end to end training have been reported within the
framework of WGAN-GP (Gulrajani et al., 2017), using a proxy for the Wasserstein distance via a

1 Code for semi-supervised learning experiments is available on https://github.com/tomsercu/

SobolevGAN-SSL

1

