Published as a conference paper at ICLR 2018

HIERARCHICAL AND INTERPRETABLE SKILL ACQUI-
SITION IN MULTI-TASK REINFORCEMENT LEARNING

Tianmin Shu∗
University of California, Los Angeles
tianmin.shu@ucla.edu

Caiming Xiong†& Richard Socher
Salesforce Research
{cxiong, rsocher}@salesforce.com

ABSTRACT

Learning policies for complex tasks that require multiple different skills is a major
challenge in reinforcement learning (RL). It is also a requirement for its deploy-
ment in real-world scenarios. This paper proposes a novel framework for efﬁcient
multi-task reinforcement learning. Our framework trains agents to employ hier-
archical policies that decide when to use a previously learned policy and when to
learn a new skill. This enables agents to continually acquire new skills during dif-
ferent stages of training. Each learned task corresponds to a human language de-
scription. Because agents can only access previously learned skills through these
descriptions, the agent can always provide a human-interpretable description of
its choices. In order to help the agent learn the complex temporal dependencies
necessary for the hierarchical policy, we provide it with a stochastic temporal
grammar that modulates when to rely on previously learned skills and when to
execute new skills. We validate our approach on Minecraft games designed to
explicitly test the ability to reuse previously learned skills while simultaneously
learning new skills.

1

INTRODUCTION

Deep reinforcement learning has demonstrated success in policy search for tasks in domains like
game playing (Mnih et al., 2015; Silver et al., 2016; 2017; Kempka et al., 2016; Mirowski et al.,
2017) and robotic control (Levine et al., 2016a;b; Pinto & Gupta, 2016). However, it is very difﬁcult
to accumulate multiple skills using just one policy network Teh et al. (2017). Knowledge transfer
techniques like distillation (Bengio, 2012; Rusu et al., 2016; Parisotto et al., 2016; Teh et al., 2017)
have been applied to train a policy network both to learn new skills while preserving previously
learned skill as well as to combine single-task policies into a multi-task policy. Existing approaches
usually treat all tasks independently. This often prevents full exploration of the underlying relations
between different tasks. They also typically assume that all policies share the same state space and
action space. This precludes transfer of previously learned simple skills to a new policy deﬁned over
a space with differing states or actions.
When humans learn new skills, we often take advantage of our existing skills and build new ca-
pacities by composing or combining simpler ones. For instance, learning multi-digit multiplication
relies on the knowledge of single-digit multiplication; learning how to properly prepare individual
ingredients facilitates cooking dishes based on complex recipes.
Inspired by this observation, we propose a hierarchical policy network which can reuse previously
learned skills alongside and as subcomponents of new skills. It achieves this by discovering the
underlying relations between skills.
To represent the skills and their relations in an interpretable way, we also encode all tasks using
human instructions such as “put down.” This allows the agent to communicate its policy and generate
plans using human language. Figure 1 illustrates an example: given the instruction “Stack blue,” our
hierarchical policy learns to compose instructions and take multiple actions through a multi-level
hierarchy in order to stack two blue blocks together. Steps from the top-level policy π3 (i.e., the red

∗This work was done when the author was an intern at Salesforce Research.
†Corresponding author

1

