Published as a conference paper at ICLR 2018

DISTRIBUTED PRIORITIZED EXPERIENCE REPLAY

Dan Horgan
DeepMind
horgan@google.com

John Quan
DeepMind
johnquan@google.com

David Budden
DeepMind
budden@google.com

Gabriel Barth-Maron
DeepMind
gabrielbm@google.com

Matteo Hessel
DeepMind
mtthss@google.com

Hado van Hasselt
DeepMind
hado@google.com

David Silver
DeepMind
davidsilver@google.com

ABSTRACT

We propose a distributed architecture for deep reinforcement learning at scale,
that enables agents to learn effectively from orders of magnitude more data than
previously possible. The algorithm decouples acting from learning: the actors
interact with their own instances of the environment by selecting actions according
to a shared neural network, and accumulate the resulting experience in a shared
experience replay memory; the learner replays samples of experience and updates
the neural network. The architecture relies on prioritized experience replay to
focus only on the most signiﬁcant data generated by the actors. Our architecture
substantially improves the state of the art on the Arcade Learning Environment,
achieving better ﬁnal performance in a fraction of the wall-clock training time.

1

INTRODUCTION

A broad trend in deep learning is that combining more computation (Dean et al., 2012) with more
powerful models (Kaiser et al., 2017) and larger datasets (Deng et al., 2009) yields more impressive
results. It is reasonable to hope that a similar principle holds for deep reinforcement learning. There
are a growing number of examples to justify this optimism: effective use of greater computational
resources has been a critical factor in the success of such algorithms as Gorila (Nair et al., 2015),
A3C (Mnih et al., 2016), GPU Advantage Actor Critic (Babaeizadeh et al., 2017), Distributed PPO
(Heess et al., 2017) and AlphaGo (Silver et al., 2016).
Deep learning frameworks such as TensorFlow (Abadi et al., 2016) support distributed training,
making large scale machine learning systems easier to implement and deploy. Despite this, much
current research in deep reinforcement learning concerns itself with improving performance within
the computational budget of a single machine, and the question of how to best harness more re-
sources is comparatively underexplored.
In this paper we describe an approach to scaling up deep reinforcement learning by generating more
data and selecting from it in a prioritized fashion (Schaul et al., 2016). Standard approaches to
distributed training of neural networks focus on parallelizing the computation of gradients, to more
rapidly optimize the parameters (Dean et al., 2012). In contrast, we distribute the generation and
selection of experience data, and ﬁnd that this alone sufﬁces to improve results. This is complemen-
tary to distributing gradient computation, and the two approaches can be combined, but in this work
we focus purely on data-generation.
We use this distributed architecture to scale up variants of Deep Q-Networks (DQN) and Deep
Deterministic Policy Gradient (DDPG), and we evaluate these on the Arcade Learning Environment
benchmark (Bellemare et al., 2013), and on a range of continuous control tasks. Our architecture

1

