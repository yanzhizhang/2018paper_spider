Published as a conference paper at ICLR 2018

HYPERPARAMETER OPTIMIZATION:
A SPECTRAL APPROACH

Elad Hazan
Princeton University and Google Brain
ehazan@cs.princeton.edu

Yang Yuan
Department of Computer Science
Cornell University
yangyuan@cs.cornell.edu

Adam Klivans
Department of Computer Science
University of Texas at Austin
klivans@cs.utexas.edu

ABSTRACT

We give a simple, fast algorithm for hyperparameter optimization inspired by tech-
niques from the analysis of Boolean functions. We focus on the high-dimensional
regime where the canonical example is training a neural network with a large num-
ber of hyperparameters. The algorithm — an iterative application of compressed
sensing techniques for orthogonal polynomials — requires only uniform sampling
of the hyperparameters and is thus easily parallelizable.
Experiments for training deep neural networks on Cifar-10 show that compared to
state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm ﬁnds signif-
icantly improved solutions, in some cases better than what is attainable by hand-
tuning.
In terms of overall running time (i.e., time required to sample various
settings of hyperparameters plus additional computation time), we are at least an
order of magnitude faster than Hyperband and Bayesian Optimization. We also
outperform Random Search 8×.
Our method is inspired by provably-efﬁcient algorithms for learning decision
trees using the discrete Fourier transform. We obtain improved sample-complexty
bounds for learning decision trees while matching state-of-the-art bounds on run-
ning time (polynomial and quasipolynomial, respectively).

1

INTRODUCTION

Large scale machine learning and optimization systems usually involve a large number of free pa-
rameters for the user to ﬁx according to their application. A timely example is the training of deep
neural networks for a signal processing application: the ML specialist needs to decide on an ar-
chitecture, depth of the network, choice of connectivity per layer (convolutional, fully-connected,
etc.), choice of optimization algorithm and recursively choice of parameters inside the optimization
library itself (learning rate, momentum, etc.).
Given a set of hyperparameters and their potential assignments, the naive practice is to search
through the entire grid of parameter assignments and pick the one that performed the best, a.k.a.
“grid search”. As the number of hyperparameters increases, the number of possible assignments
increases exponentially and a grid search becomes quickly infeasible. It is thus crucial to ﬁnd a
method for automatic tuning of these parameters.
This auto-tuning, or ﬁnding a good setting of these parameters, is now referred to as hyperparameter
optimization (HPO), or simply automatic machine learning (auto-ML). For continuous hyperparam-
eters, gradient descent is usually the method of choice (Maclaurin et al., 2015; Luketina et al., 2015;
Fu et al., 2016). Discrete parameters, however, such as choice of architecture, number of layers,
connectivity and so forth are signiﬁcantly more challenging. More formally, let

f : {−1, 1}n (cid:55)→ [0, 1]

1

