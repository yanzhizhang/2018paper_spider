Published as a conference paper at ICLR 2018

ADVERSARIAL DROPOUT REGULARIZATION

Kuniaki Saito1, Yoshitaka Ushiku1, Tatsuya Harada1,2, and Kate Saenko3

1The University of Tokyo, 2RIKEN, 3Boston University

{k-saito,ushiku,harada}@mi.t.u-tokyo.ac.jp, saenko@bu.edu

ABSTRACT

We present a domain adaptation method for transferring neural representations
from label-rich source domains to unlabeled target domains. Recent adversarial
methods proposed for this task learn to align features across domains by “fool-
ing” a special domain classiﬁer network. However, a drawback of this approach is
that the domain classiﬁer simply labels the generated features as in-domain or not,
without considering the boundaries between classes. This means that ambiguous
target features can be generated near class boundaries, reducing target classiﬁca-
tion accuracy. We propose a novel approach, Adversarial Dropout Regularization
(ADR), which encourages the generator to output more discriminative features for
the target domain. Our key idea is to replace the traditional domain critic with a
critic that detects non-discriminative features by using dropout on the classiﬁer
network. The generator then learns to avoid these areas of the feature space and
thus creates better features. We apply our ADR approach to the problem of un-
supervised domain adaptation for image classiﬁcation and semantic segmentation
tasks, and demonstrate signiﬁcant improvements over the state of the art.

INTRODUCTION

1
Transferring knowledge learned by deep neural networks from label-rich domains to new target
domains is a challenging problem, especially when the source and target input distributions have
different characteristics. Such domain shifts occurs in many practical applications. For example,
while simulated driving images rendered by games provide a rich source of labeled data for semantic
segmentation Richter et al. (2016), deep models trained on such source data do not transfer well
to real target domains (Fig. 1(a-d)). When target-domain labels are unavailable for ﬁne-tuning,
unsupervised domain adaptation must be applied to improve the source model.
Recent methods for unsupervised domain adaptation attempt to reduce the discrepancy between the
source and target features via adversarial learning (Tzeng et al. (2014); Ganin & Lempitsky (2014)).
They divide the base network into a feature encoder G and classiﬁer C, and add a separate domain
classiﬁer (critic) network D. The critic takes the features generated by G and labels them as either
source- or target-domain. The encoder G is then trained with an additional adversarial loss that
maximizes D’s mistakes and thus aligns features across domains.
However, a major drawback of this approach is that the critic simply predicts the domain label of
the generated point and does not consider category information. Thus the generator may create
features that look like they came from the right domain, but are not discriminative. In particular,
it can generate points close to class boundaries, as shown in Fig. 1(e), which are likely to be mis-
classiﬁed by the source model. We argue that to achieve good performance on the target data, the
adaptation model must take the decision boundaries between classes into account while aligning
features across domains (Fig. 1(f)). Moreover, since our setting is unsupervised adaptation, this
must be accomplished without labels on target data.
In this paper, we propose a novel adversarial alignment technique that overcomes the above limita-
tion and preserves class boundaries. We make the following observation: if the critic could detect
points near the decision boundary, then the generator would have to avoid these areas of the feature
space in order to fool the critic. Thus the critic would force the generator to create more discrimina-
tive features. How can we obtain such a critic? If we alter the boundary of the classiﬁer C slightly
and measure the change in the posterior class probability p(y|x), where y and x denote class and

1

