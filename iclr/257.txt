Published as a conference paper at ICLR 2018

STOCHASTIC ACTIVATION PRUNING FOR
ROBUST ADVERSARIAL DEFENSE

Guneet S. Dhillon1,2, Kamyar Azizzadenesheli3, Zachary C. Lipton1,4,
Jeremy Bernstein1,5, Jean Kossaiﬁ1,6, Aran Khanna1, Anima Anandkumar1,5
1Amazon AI, 2UT Austin, 3UC Irvine, 4CMU, 5Caltech, 6Imperial College London
guneetdhillon@utexas.edu, kazizzad@uci.edu, zlipton@cmu.edu,
bernstein@caltech.edu, jean.kossaifi@imperial.ac.uk,

aran@arankhanna.com, anima@amazon.com

ABSTRACT

Neural networks are known to be vulnerable to adversarial examples. Carefully
chosen perturbations to real images, while imperceptible to humans, induce mis-
classiﬁcation and threaten the reliability of deep learning systems in the wild. To
guard against adversarial examples, we take inspiration from game theory and cast
the problem as a minimax zero-sum game between the adversary and the model. In
general, for such games, the optimal strategy for both players requires a stochas-
tic policy, also known as a mixed strategy. In this light, we propose Stochastic
Activation Pruning (SAP), a mixed strategy for adversarial defense. SAP prunes
a random subset of activations (preferentially pruning those with smaller magni-
tude) and scales up the survivors to compensate. We can apply SAP to pretrained
networks, including adversarially trained models, without ﬁne-tuning, providing ro-
bustness against adversarial examples. Experiments demonstrate that SAP confers
robustness against attacks, increasing accuracy and preserving calibration.

1

INTRODUCTION

While deep neural networks have emerged as dominant tools for supervised learning problems,
they remain vulnerable to adversarial examples (Szegedy et al., 2013). Small, carefully chosen
perturbations to input data can induce misclassiﬁcation with high probability. In the image domain,
even perturbations so small as to be imperceptible to humans can fool powerful convolutional neural
networks (Szegedy et al., 2013; Goodfellow et al., 2014). This fragility presents an obstacle to using
machine learning in the wild. For example, a vision system vulnerable to adversarial examples might
be fundamentally unsuitable for a computer security application. Even if a vision system is not
explicitly used for security, these weaknesses might be critical. Moreover, these problems seem
unnecessary. If these perturbations are not perceptible to people, why should they fool a machine?
Since this problem was ﬁrst identiﬁed, a rapid succession of papers have proposed various techniques
both for generating and for guarding against adversarial attacks. Goodfellow et al. (2014) introduced
a simple method for quickly producing adversarial examples called the fast gradient sign method
(FGSM). To produce an adversarial example using FGSM, we update the inputs by taking one step in
the direction of the sign of the gradient of the loss with respect to the input.
To defend against adversarial examples some papers propose training the neural network on adver-
sarial examples themselves, either using the same model (Goodfellow et al., 2014; Madry et al.,
2017), or using an ensemble of models (Tramèr et al., 2017a). Taking a different approach, Nayebi
& Ganguli (2017) draws inspiration from biological systems. They propose that to harden neural
networks against adversarial examples, one should learn ﬂat, compressed representations that are
sensitive to a minimal number of input dimensions.
This paper introduces Stochastic Activation Pruning (SAP), a method for guarding pretrained networks
against adversarial examples. During the forward pass, we stochastically prune a subset of the
activations in each layer, preferentially retaining activations with larger magnitudes. Following the
pruning, we scale up the surviving activations to normalize the dynamic range of the inputs to the

1

