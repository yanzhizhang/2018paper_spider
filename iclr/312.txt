Published as a conference paper at ICLR 2018

ONLINE LEARNING RATE ADAPTATION WITH
HYPERGRADIENT DESCENT

Atılım G¨unes¸ Baydin
University of Oxford
gunes@robots.ox.ac.uk

Robert Cornish
University of Oxford
rcornish@robots.ox.ac.uk

David Mart´ınez Rubio
University of Oxford
david.martinez2@wadham.ox.ac.uk

Mark Schmidt
University of British Columbia
schmidtm@cs.ubc.ca

Frank Wood
University of Oxford
fwood@robots.ox.ac.uk

ABSTRACT

We introduce a general method for improving the convergence rate of gradient-
based optimizers that is easy to implement and works well in practice. We demon-
strate the effectiveness of the method in a range of optimization problems by
applying it to stochastic gradient descent, stochastic gradient descent with Nes-
terov momentum, and Adam, showing that it signiﬁcantly reduces the need for the
manual tuning of the initial learning rate for these commonly used algorithms. Our
method works by dynamically updating the learning rate during optimization using
the gradient with respect to the learning rate of the update rule itself. Computing
this “hypergradient” needs little additional computation, requires only one extra
copy of the original gradient to be stored in memory, and relies upon nothing more
than what is provided by reverse-mode automatic differentiation.

1

INTRODUCTION

In nearly all gradient descent algorithms the choice of learning rate remains central to efﬁciency;
Bengio (2012) asserts that it is “often the single most important hyper-parameter” and that it always
should be tuned. This is because choosing to follow your gradient signal by something other than the
right amount, either too much or too little, can be very costly in terms of how fast the overall descent
procedure achieves a particular level of objective value.
Understanding that adapting the learning rate is a good thing to do, particularly on a per parameter
basis dynamically, led to the development of a family of widely-used optimizers including AdaGrad
(Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), and Adam (Kingma & Ba, 2015).
However, a persisting commonality of these methods is that they are parameterized by a “pesky” ﬁxed
global learning rate hyperparameter which still needs tuning. There have been methods proposed
that do away with needing to tune such hyperparameters altogether (Schaul et al., 2013) but their
adoption has not been widespread, owing perhaps to their complexity, applicability in practice, or
performance relative to the aforementioned family of algorithms.
Our initial conceptualization of the learning rate adaptation problem was one of automatic differen-
tiation (Baydin et al., 2018). We hypothesized that the derivative of a parameter update procedure
with respect to its global learning rate ought to be useful for improving optimizer performance. This
conceptualization is not unique, having been explored, for instance, by Maclaurin et al. (2015). While
the automatic differentiation perspective was integral to our conceptualization, the resulting algorithm
turns out to simplify elegantly and not require additional automatic differentiation machinery. In fact,
it is easily adaptable to nearly any gradient update procedure while only requiring one extra copy
of a gradient to be held in memory and very little computational overhead; just a dot product in the

1

