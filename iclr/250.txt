Published as a conference paper at ICLR 2018

ON THE CONVERGENCE OF ADAM AND BEYOND

Sashank J. Reddi, Satyen Kale & Sanjiv Kumar
Google New York
New York, NY 10011, USA
{sashank,satyenkale,sanjivk}@google.com

ABSTRACT

Several recently proposed stochastic optimization methods that have been suc-
cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA,
NADAM are based on using gradient updates scaled by square roots of exponential
moving averages of squared past gradients. In many applications, e.g. learning
with large output spaces, it has been empirically observed that these algorithms
fail to converge to an optimal solution (or a critical point in nonconvex settings).
We show that one cause for such failures is the exponential moving average used
in the algorithms. We provide an explicit example of a simple convex optimiza-
tion setting where ADAM does not converge to the optimal solution, and describe
the precise problems with the previous analysis of ADAM algorithm. Our anal-
ysis suggests that the convergence issues can be ﬁxed by endowing such algo-
rithms with “long-term memory” of past gradients, and propose new variants of
the ADAM algorithm which not only ﬁx the convergence issues but often also lead
to improved empirical performance.

1

INTRODUCTION

Stochastic gradient descent (SGD) is the dominant method to train deep networks today. This method
iteratively updates the parameters of a model by moving them in the direction of the negative gra-
dient of the loss evaluated on a minibatch. In particular, variants of SGD that scale coordinates
of the gradient by square roots of some form of averaging of the squared coordinates in the past
gradients have been particularly successful, because they automatically adjust the learning rate on
a per-feature basis. The ﬁrst popular algorithm in this line of research is ADAGRAD (Duchi et al.,
2011; McMahan & Streeter, 2010), which can achieve signiﬁcantly better performance compared to
vanilla SGD when the gradients are sparse, or in general small.
Although ADAGRAD works well for sparse settings, its performance has been observed to deteriorate
in settings where the loss functions are nonconvex and gradients are dense due to rapid decay of the
learning rate in these settings since it uses all the past gradients in the update. This problem is
especially exacerbated in high dimensional problems arising in deep learning. To tackle this issue,
several variants of ADAGRAD, such as RMSPROP (Tieleman & Hinton, 2012), ADAM (Kingma
& Ba, 2015), ADADELTA (Zeiler, 2012), NADAM (Dozat, 2016), etc, have been proposed which
mitigate the rapid decay of the learning rate using the exponential moving averages of squared past
gradients, essentially limiting the reliance of the update to only the past few gradients. While these
algorithms have been successfully employed in several practical applications, they have also been
observed to not converge in some other settings. It has been typically observed that in these settings
some minibatches provide large gradients but only quite rarely, and while these large gradients
are quite informative, their inﬂuence dies out rather quickly due to the exponential averaging, thus
leading to poor convergence.
In this paper, we analyze this situation in detail. We rigorously prove that the intuition conveyed
in the above paragraph is indeed correct; that limiting the reliance of the update on essentially only
the past few gradients can indeed cause signiﬁcant convergence issues. In particular, we make the
following key contributions:

• We elucidate how the exponential moving average in the RMSPROP and ADAM algorithms
can cause non-convergence by providing an example of simple convex optimization prob-

1

