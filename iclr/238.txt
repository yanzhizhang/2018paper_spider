Published as a conference paper at ICLR 2018

ACTION-DEPENDENT CONTROL VARIATES FOR POL-
ICY OPTIMIZATION VIA STEIN’S IDENTITY

Hao Liu∗
Computer Science
UESTC
Chengdu, China
uestcliuhao@gmail.com

Yihao Feng∗
Computer science
University of Texas at Austin
Austin, TX, 78712
yihao@cs.utexas.edu

Yi Mao
Microsoft
Redmond, WA, 98052
maoyi@microsoft.com

Dengyong Zhou
Google
Kirkland, WA, 98033
dennyzhou@google.com

Jian Peng
Computer Science
UIUC
Urbana, IL 61801
jianpeng@illinois.edu

Qiang Liu
Computer Science
University of Texas at Austin
Austin, TX, 78712
lqiang@cs.utexas.edu

ABSTRACT

Policy gradient methods have achieved remarkable successes in solving challeng-
ing reinforcement learning problems. However, it still often suffers from the large
variance issue on policy gradient estimation, which leads to poor sample efﬁciency
during training. In this work, we propose a control variate method to effectively
reduce variance for policy gradient methods. Motivated by the Stein’s identity,
our method extends the previous control variate methods used in REINFORCE
and advantage actor-critic by introducing more general action-dependent base-
line functions. Empirical studies show that our method signiﬁcantly improves the
sample efﬁciency of the state-of-the-art policy gradient approaches.

1

INTRODUCTION

Deep reinforcement learning (RL) provides a general framework for solving challenging goal-
oriented sequential decision-making problems, It has recently achieved remarkable successes in
advancing the frontier of AI technologies (Silver et al., 2017; Mnih et al., 2013; Silver et al., 2016;
Schulman et al., 2017). Policy gradient (PG) is one of the most successful model-free RL approaches
that has been widely applied to high dimensional continuous control, vision-based navigation and
video games (Schulman et al., 2016; Kakade, 2002; Schulman et al., 2015; Mnih et al., 2016).
Despite these successes, a key problem of policy gradient methods is that the gradient estimates
often have high variance. A naive solution to ﬁx this issue would be generating a large amount
of rollout samples to obtain a reliable gradient estimation in each step. Regardless of the cost of
generating large samples, in many practical applications like developing driverless cars, it may not
even be possible to generate as many samples as we want. A variety of variance reduction techniques
have been proposed for policy gradient methods (See e.g. Weaver & Tao 2001, Greensmith et al.
2004, Schulman et al. 2016 and Asadi et al. 2017).
In this work, we focus on the control variate method, one of the most widely used variance reduc-
tion techniques in policy gradient and variational inference. The idea of the control variate method
is to subtract a Monte Carlo gradient estimator by a baseline function that analytically has zero ex-
pectation. The resulted estimator does not introduction biases theoretically, but may achieve much
lower variance if the baseline function is properly chosen such that it cancels out the variance of
the original gradient estimator. Different control variates yield different variance reduction meth-
ods. For example, in REINFORCE (Williams, 1992), a constant baseline function is chosen as a
control variate; advantage actor-critic (A2C) (Sutton & Barto, 1998; Mnih et al., 2016) considers a
state-dependent baseline function as the control variate, which is often set to be an estimated value

∗Both authors contributed equally. Author ordering determined by coin ﬂip over a Google Hangout.

1

