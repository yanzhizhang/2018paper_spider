Published as a conference paper at ICLR 2018

TOWARDS REVERSE-ENGINEERING
BLACK-BOX NEURAL NETWORKS

Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz
Max-Planck Institute for Informatics, Saarland Informatics Campus, Saarbr¨ucken, Germany
{joon,maxaug,schiele,mfritz}@mpi-inf.mpg.de

ABSTRACT

Many deployed learned models are black boxes: given input, returns output. Inter-
nal information about the model, such as the architecture, optimisation procedure,
or training data, is not disclosed explicitly as it might contain proprietary infor-
mation or make the system more vulnerable. This work shows that such attributes
of neural networks can be exposed from a sequence of queries. This has multiple
implications. On the one hand, our work exposes the vulnerability of black-box
neural networks to different types of attacks – we show that the revealed internal
information helps generate more effective adversarial examples against the black
box model. On the other hand, this technique can be used for better protection
of private content from automatic recognition models using adversarial examples.
Our paper suggests that it is actually hard to draw a line between white box and
black box models. The code is available at goo.gl/MbYfsv.

1

INTRODUCTION

Black-box models take a sequence of query inputs, and return corresponding outputs, while keeping
internal states such as model architecture hidden. They are deployed as black boxes usually on
purpose – for protecting intellectual properties or privacy-sensitive training data. Our work aims at
inferring information about the internals of black box models – ultimately turning them into white
box models. Such a reverse-engineering of a black box model has many implications. On the one
hand, it has legal implications to intellectual properties (IP) involving neural networks – internal
information about the model can be proprietary and a key IP, and the training data may be privacy
sensitive. Disclosing hidden details may also render the model more susceptible to attacks from
adversaries. On the other hand, gaining information about a black-box model can be useful in other
scenarios. E.g. there has been work on utilising adversarial examples for protecting private regions
(e.g. faces) in photographs from automatic recognisers (Oh et al., 2017). In such scenarios, gaining
more knowledge on the recognisers will increase the chance of protecting one’s privacy. Either way,
it is a crucial research topic to investigate the type and amount of information that can be gained
from a black-box access to a model. We make a ﬁrst step towards understanding the connection
between white box and black box approaches – which were previously thought of as distinct classes.
We introduce the term “model attributes” to refer to various types of information about a trained
neural network model. We group them into three types: (1) architecture (e.g. type of non-linear acti-
vation), (2) optimisation process (e.g. SGD or ADAM?), and (3) training data (e.g. which dataset?).
We approach the problem as a standard supervised learning task applied over models. First, collect
a diverse set of white-box models (“meta-training set”) that are expected to be similar to the tar-
get black box at least to a certain extent. Then, over the collected meta-training set, train another
model (“metamodel”) that takes a model as input and returns the corresponding model attributes as
output. Importantly, since we want to predict attributes at test time for black-box models, the only
information available for attribute prediction is the query input-output pairs. As we will see in the
experiments, such input-output pairs allow to predict model attributes surprisingly well.
In summary, we contribute: (1) Investigation of the type and amount of internal information about
the black-box model that can be extracted from querying; (2) Novel metamodel methods that not
only reason over outputs from static query inputs, but also actively optimise query inputs that can
extract more information; (3) Study of factors like size of the meta-training set, quantity and quality

1

