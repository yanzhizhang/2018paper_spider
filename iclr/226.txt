Published as a conference paper at ICLR 2018

LEARNING FROM NOISY SINGLY-LABELED DATA

Ashish Khetan
University of Illinois at Urbana-Champaign
Urbana, IL 61801
khetan2@illinois.edu

Zachary C. Lipton
Amazon Web Services
Seattle, WA 98101
liptoz@amazon.com

Animashree Anandkumar
Amazon Web Services
Seattle, WA 98101
anima@amazon.com

ABSTRACT

Supervised learning depends on annotated examples, which are taken to be the
ground truth. But these labels often come from noisy crowdsourcing platforms,
like Amazon Mechanical Turk. Practitioners typically collect multiple labels per
example and aggregate the results to mitigate noise (the classic crowdsourcing
problem). Given a ﬁxed annotation budget and unlimited unlabeled data, redun-
dant annotation comes at the expense of fewer labeled examples. This raises two
fundamental questions: (1) How can we best learn from noisy workers? (2) How
should we allocate our labeling budget to maximize the performance of a classi-
ﬁer? We propose a new algorithm for jointly modeling labels and worker quality
from noisy crowd-sourced data. The alternating minimization proceeds in rounds,
estimating worker quality from disagreement with the current model and then
updating the model by optimizing a loss function that accounts for the current
estimate of worker quality. Unlike previous approaches, even with only one an-
notation per example, our algorithm can estimate worker quality. We establish a
generalization error bound for models learned with our algorithm and establish
theoretically that it’s better to label many examples once (vs less multiply) when
worker quality exceeds a threshold. Experiments conducted on both ImageNet
(with simulated noisy workers) and MS-COCO (using the real crowdsourced la-
bels) conﬁrm our algorithm’s beneﬁts.

1

INTRODUCTION

Recent advances in supervised learning owe, in part, to the availability of large annotated datasets.
For instance, the performance of modern image classiﬁers saturates only with millions of labeled
examples. This poses an economic problem: Assembling such datasets typically requires the labor
of human annotators. If we conﬁned the labor pool to experts, this work might be prohibitively ex-
pensive. Therefore, most practitioners turn to crowdsourcing platforms such as Amazon Mechanical
Turk (AMT), which connect employers with low-skilled workers who perform simple tasks, such as
classifying images, at low cost.
Compared to experts, crowd-workers provide noisier annotations, possibly owing to high variation
in worker skill; and a per-answer compensation structure that encourages rapid answers, even at the
expense of accuracy. To address variation in worker skill, practitioners typically collect multiple in-
dependent labels for each training example from different workers. In practice, these labels are often
aggregated by applying a simple majority vote. Academics have proposed many efﬁcient algorithms
for estimating the ground truth from noisy annotations. Research addressing the crowd-sourcing
problem goes back to the early 1970s. Dawid & Skene (1979) proposed a probabilistic model to
jointly estimate worker skills and ground truth labels and used expectation maximization (EM) to
estimate the parameters. Whitehill et al. (2009); Welinder et al. (2010); Zhou et al. (2015) proposed
generalizations of the Dawid-Skene model, e.g. by estimating the difﬁculty of each example.

1

