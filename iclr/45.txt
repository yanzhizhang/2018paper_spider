Published as a conference paper at ICLR 2018

TRAINING GENERATIVE ADVERSARIAL NETWORKS
VIA PRIMAL-DUAL SUBGRADIENT METHODS: A
LAGRANGIAN PERSPECTIVE ON GAN

†Xu Chen , ‡Jiang Wang, †Hao Ge ∗
† Department of EECS, Northwestern University, Evanston, IL, USA
‡ Google Inc.
{chenx,haoge2013}@u.northwestern.edu
wangjiangb@gmail.com

ABSTRACT

We relate the minimax game of generative adversarial networks (GANs) to ﬁnding
the saddle points of the Lagrangian function for a convex optimization problem,
where the discriminator outputs and the distribution of generator outputs play the
roles of primal variables and dual variables, respectively. This formulation shows
the connection between the standard GAN training process and the primal-dual
subgradient methods for convex optimization. The inherent connection does not
only provide a theoretical convergence proof for training GANs in the function
space, but also inspires a novel objective function for training. The modiﬁed
objective function forces the distribution of generator outputs to be updated along
the direction according to the primal-dual subgradient methods. A toy example
shows that the proposed method is able to resolve mode collapse, which in this
case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on
both Gaussian mixture synthetic data and real-world image datasets demonstrate
the performance of the proposed method on generating diverse samples.

1

INTRODUCTION

Generative adversarial networks (GANs) are a class of game theoretical methods for learning data
distributions. It trains the generative model by maintaining two deep neural networks, namely the
discriminator network D and the generator network G. The generator aims to produce samples
resembling real data samples, while the discriminator aims to distinguish the generated samples and
real data samples.
The standard GAN training procedure is formulated as the following minimax game:

min

G

max

D

Ex∼pd(x){log D(x)} + Ez∼pz(z){log(1 − D(G(z)))},

(1)

where pd(x) is the data distribution and pz(z) is the noise distribution. The generated samples G(z)
induces a generated distribution pg(x). Theoretically, the optimal solution to (1) is p∗
g = pd and
D∗(x) = 1/2 for all x in the support of data distribution.
In practice, the discriminator network and the generator network are parameterized by θθθd and θθθg,
respectively. The neural network parameters are updated iteratively according to gradient descent.
In particular, the discriminator is ﬁrst updated either with multiple gradient descent steps until
convergence or with a single gradient descent step, then the generator is updated with a single descent
step. However, the analysis of the convergence properties on the training approaches is challenging,
as noted by Ian Goodfellow in (Goodfellow, 2016), “For GANs, there is no theoretical prediction as
to whether simultaneous gradient descent should converge or not. Settling this theoretical question,
and developing algorithms guaranteed to converge, remain important open research problems.".
There have been some recent studies on the convergence behaviours of GAN training (Nowozin
et al., 2016; Li et al., 2017b; Heusel et al., 2017; Nagarajan & Kolter, 2017; Mescheder et al., 2017).

∗The ﬁrst two authors have equal contributions.

1

