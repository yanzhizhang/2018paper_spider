Published as a conference paper at ICLR 2018

MASKGAN: BETTER TEXT GENERATION VIA FILLING
IN THE

William Fedus, Ian Goodfellow and Andrew M. Dai
Google Brain
liam.fedus@gmail.com, {goodfellow, adai}@google.com

ABSTRACT

Neural text generation models are often autoregressive language models or seq2seq
models. These models generate text by sampling words sequentially, with each
word conditioned on the previous word, and are state-of-the-art for several machine
translation and summarization benchmarks. These benchmarks are often deﬁned
by validation perplexity even though this is not a direct measure of the quality
of the generated text. Additionally, these models are typically trained via maxi-
mum likelihood and teacher forcing. These methods are well-suited to optimizing
perplexity but can result in poor sample quality since generating text requires condi-
tioning on sequences of words that may have never been observed at training time.
We propose to improve sample quality using Generative Adversarial Networks
(GANs), which explicitly train the generator to produce high quality samples and
have shown a lot of success in image generation. GANs were originally designed
to output differentiable values, so discrete language generation is challenging for
them. We claim that validation perplexity alone is not indicative of the quality
of text generated by a model. We introduce an actor-critic conditional GAN that
ﬁlls in missing text conditioned on the surrounding context. We show qualita-
tively and quantitatively, evidence that this produces more realistic conditional and
unconditional text samples compared to a maximum likelihood trained model.

1

INTRODUCTION

Recurrent Neural Networks (RNNs) (Graves et al., 2012) are the most common generative model for
sequences as well as for sequence labeling tasks. They have shown impressive results in language
modeling (Mikolov et al., 2010), machine translation (Wu et al., 2016) and text classiﬁcation (Miyato
et al., 2017). Text is typically generated from these models by sampling from a distribution that is
conditioned on the previous word and a hidden state that consists of a representation of the words
generated so far. These are typically trained with maximum likelihood in an approach known as
teacher forcing, where ground-truth words are fed back into the model to be conditioned on for
generating the following parts of the sentence. This causes problems when, during sample generation,
the model is often forced to condition on sequences that were never conditioned on at training time.
This leads to unpredictable dynamics in the hidden state of the RNN. Methods such as Professor
Forcing (Lamb et al., 2016) and Scheduled Sampling (Bengio et al., 2015) have been proposed to
solve this issue. These approaches work indirectly by either causing the hidden state dynamics to
become predictable (Professor Forcing) or by randomly conditioning on sampled words at training
time, however, they do not directly specify a cost function on the output of the RNN that encourages
high sample quality. Our proposed method does so.
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are a framework for training
generative models in an adversarial setup, with a generator generating images that is trying to fool a
discriminator that is trained to discriminate between real and synthetic images. GANs have had a lot
of success in producing more realistic images than other approaches but they have only seen limited
use for text sequences. This is due to the discrete nature of text making it infeasible to propagate the
gradient from the discriminator back to the generator as in standard GAN training. We overcome this
by using Reinforcement Learning (RL) to train the generator while the discriminator is still trained via
maximum likelihood and stochastic gradient descent. GANs also commonly suffer from issues such

1

