Published as a conference paper at ICLR 2018

DIVIDE-AND-CONQUER REINFORCEMENT LEARNING

Dibya Ghosh1, Avi Singh1, Aravind Rajeswaran2, Vikash Kumar2, Sergey Levine1
1 University of California Berkeley 2 University of Washington Seattle
dibya@berkeley.edu, avisingh@cs.berkeley.edu,
{aravraj, vikash}@cs.washington.edu, svlevine@eecs.berkeley.edu

ABSTRACT

Standard model-free deep reinforcement learning (RL) algorithms sample a new
initial state for each trial, allowing them to optimize policies that can perform
well even in highly stochastic environments. However, problems that exhibit con-
siderable initial state variation typically produce high-variance gradient estimates
for model-free RL, making direct policy or value function optimization challeng-
ing. In this paper, we develop a novel algorithm that instead partitions the initial
state space into “slices”, and optimizes an ensemble of policies, each on a different
slice. The ensemble is gradually uniﬁed into a single policy that can succeed on the
whole state space. This approach, which we term divide-and-conquer RL, is able
to solve complex tasks where conventional deep RL methods are ineffective. Our
results show that divide-and-conquer RL greatly outperforms conventional policy
gradient methods on challenging grasping, manipulation, and locomotion tasks,
and exceeds the performance of a variety of prior methods. Videos of policies
learned by our algorithm can be viewed at https://sites.google.com/view/dnc-rl/.

1

INTRODUCTION

Deep reinforcement learning (RL) algorithms have demonstrated an impressive potential for tack-
ling a wide range of complex tasks, from game playing (Mnih et al., 2015) to robotic manipula-
tion (Levine et al., 2016; Kumar et al., 2016; Popov et al., 2017; Andrychowicz et al., 2017). How-
ever, many of the standard benchmark tasks in reinforcement learning, including the Atari bench-
mark suite (Mnih et al., 2013) and all of the OpenAI gym continuous control benchmarks (Brockman
et al., 2016) lack the kind of diversity that is present in realistic environments.
One of the most compelling use cases for RL algorithms is to create autonomous agents that can
interact intelligently with diverse stochastic environments. However, such environments present
a major challenge for current RL algorithms. Environments which require a lot of diversity can
be expressed in the framework of RL as having a “wide” stochastic initial state distribution for
the underlying Markov decision process. Highly stochastic initial state distributions lead to high-
variance policy gradient estimates, which in turn hamper effective learning. Similarly, diversity and
variability can also be incorporated by picking a wide distribution over goals.
In this paper, we explore RL algorithms that are especially well-suited for tasks with a high degree
of variability in both initial and goal states. We argue that a large class of practically interesting
real-world problems fall into this category, but current RL algorithms are poorly equipped to handle
them, as illustrated in our experimental evaluation. Our main observation is that, for tasks with a
high degree of initial state variability, it is often much easier to obtain effective solutions to indi-
vidual parts of the initial state space and then merge these solutions into a single policy, than to
solve the entire task as a monolithic stochastic MDP. To that end, we can autonomously partition
the state distribution into a set of distinct “slices,” and train a separate policy for each slice. For
example, if we imagine the task of picking up a block with a robotic arm, different slices might
correspond to different initial positions of the block. Similarly, for placing the block, different slices
will correspond to the different goal positions. For each slice, the algorithm might train a different
policy with a distinct strategy. As the training proceeds, we can gradually merge the distinct policies

1

