Published as a conference paper at ICLR 2018

PARAMETER SPACE NOISE FOR EXPLORATION

Matthias Plappert†‡, Rein Houthooft†, Prafulla Dhariwal†, Szymon Sidor†,
Richard Y. Chen†, Xi Chen††, Tamim Asfour‡, Pieter Abbeel††, and Marcin Andrychowicz†
† OpenAI
‡ Karlsruhe Institute of Technology (KIT)
†† University of California, Berkeley
Correspondence to matthias@openai.com

ABSTRACT

Deep reinforcement learning (RL) methods generally engage in exploratory be-
havior through noise injection in the action space. An alternative is to add noise
directly to the agent’s parameters, which can lead to more consistent exploration
and a richer set of behaviors. Methods such as evolutionary strategies use parameter
perturbations, but discard all temporal structure in the process and require signif-
icantly more samples. Combining parameter noise with traditional RL methods
allows to combine the best of both worlds. We demonstrate that both off- and
on-policy methods beneﬁt from this approach through experimental comparison
of DQN, DDPG, and TRPO on high-dimensional discrete action environments as
well as continuous control tasks.

1

INTRODUCTION

Exploration remains a key challenge in contemporary deep reinforcement learning (RL). Its main
purpose is to ensure that the agent’s behavior does not converge prematurely to a local optimum.
Enabling efﬁcient and effective exploration is, however, not trivial since it is not directed by the
reward function of the underlying Markov decision process (MDP). Although a plethora of methods
have been proposed to tackle this challenge in high-dimensional and/or continuous-action MDPs,
they often rely on complex additional structures such as counting tables (Tang et al., 2016), density
modeling of the state space (Ostrovski et al., 2017), learned dynamics models (Houthooft et al., 2016;
Achiam & Sastry, 2017; Stadie et al., 2015), or self-supervised curiosity (Pathak et al., 2017).
An orthogonal way of increasing the exploratory nature of these algorithms is through the addition
of temporally-correlated noise, for example as done in bootstrapped DQN (Osband et al., 2016a).
Along the same lines, it was shown that the addition of parameter noise leads to better exploration by
obtaining a policy that exhibits a larger variety of behaviors (Sun et al., 2009b; Salimans et al., 2017).
We discuss these related approaches in greater detail in Section 5. Their main limitation, however,
is that they are either only proposed and evaluated for the on-policy setting with relatively small
and shallow function approximators (Rückstieß et al., 2008) or disregard all temporal structure and
gradient information (Salimans et al., 2017; Kober & Peters, 2008; Sehnke et al., 2010).
This paper investigates how parameter space noise can be effectively combined with off-the-shelf deep
RL algorithms such as DQN (Mnih et al., 2015), DDPG (Lillicrap et al., 2015), and TRPO (Schulman
et al., 2015b) to improve their exploratory behavior. Experiments show that this form of exploration
is applicable to both high-dimensional discrete environments and continuous control tasks, using
on- and off-policy methods. Our results indicate that parameter noise outperforms traditional action
space noise-based baselines, especially in tasks where the reward signal is extremely sparse.

2 BACKGROUND

We consider the standard RL framework consisting of an agent interacting with an environment.
To simplify the exposition we assume that the environment is fully observable. An environment is
modeled as a Markov decision process (MDP) and is deﬁned by a set of states S, a set of actions
A, a distribution over initial states p(s0), a reward function r : S × A (cid:55)→ R, transition probabilities

1

