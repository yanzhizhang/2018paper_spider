Published as a conference paper at ICLR 2018

UNDERSTANDING SHORT-HORIZON BIAS
IN STOCHASTIC META-OPTIMIZATION

Yuhuai Wu∗, Mengye Ren∗, Renjie Liao & Roger B. Grosse
University of Toronto and Vector Institute
{ywu, mren, rjliao, rgrosse}@cs.toronto.edu

ABSTRACT

Careful tuning of the learning rate, or even schedules thereof, can be crucial to
effective neural net training. There has been much recent interest in gradient-based
meta-optimization, where one tunes hyperparameters, or even learns an optimizer,
in order to minimize the expected loss when the training procedure is unrolled.
But because the training procedure must be unrolled thousands of times, the meta-
objective must be deﬁned with an orders-of-magnitude shorter time horizon than
is typical for neural net training. We show that such short-horizon meta-objectives
cause a serious bias towards small step sizes, an effect we term short-horizon bias.
We introduce a toy problem, a noisy quadratic cost function, on which we analyze
short-horizon bias by deriving and comparing the optimal schedules for short and
long time horizons. We then run meta-optimization experiments (both ofﬂine and
online) on standard benchmark datasets, showing that meta-optimization chooses
too small a learning rate by multiple orders of magnitude, even when run with a
moderately long time horizon (100 steps) typical of work in the area. We believe
short-horizon bias is a fundamental problem that needs to be addressed if meta-
optimization is to scale to practical neural net training regimes.

1

INTRODUCTION

The learning rate is one of the most important and frustrating hyperparameters to tune in deep
learning. Too small a value causes slow progress, while too large a value causes ﬂuctuations or even
divergence. While a ﬁxed learning rate often works well for simpler problems, good performance
on the ImageNet (Russakovsky et al., 2015) benchmark requires a carefully tuned schedule. A
variety of decay schedules have been proposed for different architectures, including polynomial,
exponential, staircase, etc. Learning rate decay is also required to achieve convergence guarantee for
stochastic gradient methods under certain conditions (Bottou, 1998). Clever learning rate heuristics
have resulted in large improvements in training efﬁciency (Goyal et al., 2017; Smith, 2017). A
related hyperparameter is momentum; typically ﬁxed to a reasonable value such as 0.9, careful
tuning can also give signiﬁcant performance gains (Sutskever et al., 2013). While optimizers such
as Adam (Kingma & Ba, 2015) are often described as adapting coordinate-speciﬁc learning rates, in
fact they also have global learning rate and momentum hyperparameters analogously to SGD, and
tuning at least the learning rate can be important to good performance.
In light of this, it is not surprising that there have been many attempts to adapt learning rates,
either online during optimization (Schraudolph, 1999; Schaul et al., 2013), or ofﬂine by ﬁtting a
learning rate schedule (Maclaurin et al., 2015). More ambitiously, others have attempted to learn
an optimizer
(Andrychowicz et al., 2016; Li & Malik, 2017; Finn et al., 2017; Lv et al., 2017;
Wichrowska et al., 2017; Metz et al., 2017). All of these approaches are forms of meta-optimization,
where one deﬁnes a meta-objective (typically the expected loss after some number of optimization
steps) and tunes the hyperparameters to minimize this meta-objective. But because gradient-based
meta-optimization can require thousands of updates, each of which unrolls the entire base-level
optimization procedure, the meta-optimization is thousands of times more expensive than the base-
level optimization. Therefore, the meta-objective must be deﬁned with a much smaller time horizon

∗Equal contribution.
Code available at https://github.com/renmengye/meta-optim-public

1

