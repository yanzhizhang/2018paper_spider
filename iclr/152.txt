Published as a conference paper at ICLR 2018

ON THE STATE OF THE ART OF EVALUATION IN
NEURAL LANGUAGE MODELS

G´abor Melis†, Chris Dyer†, Phil Blunsom†‡
{melisgl,cdyer,pblunsom}@google.com
†DeepMind
‡University of Oxford

ABSTRACT

Ongoing innovations in recurrent neural network architectures have provided a
steady inﬂux of apparently state-of-the-art results on language modelling bench-
marks. However, these have been evaluated using differing codebases and limited
computational resources, which represent uncontrolled sources of experimental
variation. We reevaluate several popular architectures and regularisation meth-
ods with large-scale automatic black-box hyperparameter tuning and arrive at the
somewhat surprising conclusion that standard LSTM architectures, when properly
regularised, outperform more recent models. We establish a new state of the art
on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the
Hutter Prize dataset.

1

INTRODUCTION

The scientiﬁc process by which the deep learning research community operates is guided by em-
pirical studies that evaluate the relative quality of models. Complicating matters, the measured
performance of a model depends not only on its architecture (and data), but it can strongly depend
on hyperparameter values that affect learning, regularisation, and capacity. This hyperparameter
dependence is an often inadequately controlled source of variation in experiments, which creates a
risk that empirically unsound claims will be reported.
In this paper, we use a black-box hyperparameter optimisation technique to control for hyperpa-
rameter effects while comparing the relative performance of language modelling architectures based
on LSTMs, Recurrent Highway Networks (Zilly et al., 2016) and NAS (Zoph & Le, 2016). We
specify ﬂexible, parameterised model families with the ability to adjust embedding and recurrent
cell sizes for a given parameter budget and with ﬁne grain control over regularisation and learning
hyperparameters.
Once hyperparameters have been properly controlled for, we ﬁnd that LSTMs outperform the more
recent models, contra the published claims. Our result is therefore a demonstration that replication
failures can happen due to poorly controlled hyperparameter variation, and this paper joins other
recent papers in warning of the under-acknowledged existence of replication failure in deep learn-
ing (Henderson et al., 2017; Reimers & Gurevych, 2017). However, we do show that careful controls
are possible, albeit at considerable computational cost.
Several remarks can be made in light of these results. First, as (conditional) language models serve
as the central building block of many tasks, including machine translation, there is little reason to
expect that the problem of unreliable evaluation is unique to the tasks discussed here. However, in
machine translation, carefully controlling for hyperparameter effects would be substantially more
expensive because standard datasets are much larger. Second, the research community should strive
for more consensus about appropriate experimental methodology that balances costs of careful ex-
perimentation with the risks associated with false claims. Finally, more attention should be paid
to hyperparameter sensitivity. Models that introduce many new hyperparameters or which perform
well only in narrow ranges of hyperparameter settings should be identiﬁed as such as part of standard
publication practice.

1

