Published as a conference paper at ICLR 2018

MINIMAL-ENTROPY CORRELATION ALIGNMENT FOR
UNSUPERVISED DEEP DOMAIN ADAPTATION

Pietro Morerio1, Jacopo Cavazza1 & Vittorio Murino1,2
1 Pattern Analysis and Computer Vision (PAVIS), Istituto Italiano di Tecnologia - Genova, Italy
2 University of Verona, Department of Computer Science - Verona, Italy
{pietro.morerio,jacopo.cavazza,vittorio.murino}@iit.it

ABSTRACT

In this work, we face the problem of unsupervised domain adaptation with a novel
deep learning approach which leverages our ﬁnding that entropy minimization is
induced by the optimal alignment of second order statistics between source and
target domains. We formally demonstrate this hypothesis and, aiming at achieving
an optimal alignment in practical cases, we adopt a more principled strategy
which, differently from the current Euclidean approaches, deploys alignment along
geodesics. Our pipeline can be implemented by adding to the standard classiﬁcation
loss (on the labeled source domain), a source-to-target regularizer that is weighted
in an unsupervised and data-driven fashion. We provide extensive experiments
to assess the superiority of our framework on standard domain and modality
adaptation benchmarks.

1

INTRODUCTION

Learning visual representations that are invariant across different domains is an important task in
computer vision. Actually, data labeling is onerous and even impossible in some cases. It is thus
desirable to train a model with full supervision on a source, labeled domain and then learn how to
transfer it on a target domain, as opposed to retrain it completely from scratch. Moreover, the latter
stage is actually not possible if the target domain is totally unlabelled: this is the setting we consider
in our work. In the literature, this problem is known as unsupervised domain adaptation which can
be regarded as a special semi-supervised learning problem, where labeled and unlabeled data come
from different domains. Since no labels are available in the target domain, source-to-target adaptation
must be carried out in a fully unsupervised manner. Clearly, this is an arguably difﬁcult task since
transferring a model across domains is complicated by the so-called domain shift [Torralba & Efros
(2011)]. In fact, while switching from the source to the target, even if dealing with the same K
visual categories in both domains, different biases may arise related to several factors. For instance,
dissimilar points of view, illumination changes, background clutter, etc.
In the previous years, a broad class of approaches has leveraged on entropy optimization as a proxy
for (unsupervised) domain adaptation, borrowing this idea from semi-supervised learning [Grandvalet
& Bengio (2004)]. By either performing entropy regularization [Tzeng et al. (2015); Carlucci et al.
(2017); Saito et al. (2017)], explicit entropy minimization [Haeusser et al. (2017)], or implicit entropy
maximization through adversarial training [Ganin & Lempitsky (2015); Tzeng et al. (2017)], this
statistical tool has demonstrated to be powerful for adaptation purposes.
Alternatively, there exist methods which try to align the source to the target domain by learning an
explicit transformation between the two so that the target data distribution can be matched to the one
of the source one [Glorot et al. (2011); Kan et al. (2015); Shekhar et al. (2013); Gopalan & Li (2011);
Gong et al. (2012a)]. Within this paradigm, correlation alignment minimizes the distance between
second order statistics computed in the form of covariance representations between features from the
source a [Fernando et al. (2013); Sun et al. (2016); Sun & Saenko (2016)].
Apparently, correlation alignment and entropy minimization may seem two unrelated and approaches
in optimizing models for domain adaptation. However, in this paper, we will show that this is not the
case and, indeed, we claim that the two classes of approaches are deeply intertwined. In addition to

1

