Published as a conference paper at ICLR 2018

UNDERSTANDING DEEP NEURAL NETWORKS WITH
RECTIFIED LINEAR UNITS

Raman Arora∗ Amitabh Basu† Poorya Mianjy‡ Anirbit Mukherjee§
Johns Hopkins University

ABSTRACT

In this paper we investigate the family of functions representable by deep neural
networks (DNN) with rectiﬁed linear units (ReLU). We give an algorithm to train a
ReLU DNN with one hidden layer to global optimality with runtime polynomial in
the data size albeit exponential in the input dimension. Further, we improve on the
known lower bounds on size (from exponential to super exponential) for approx-
imating a ReLU deep net function by a shallower ReLU net. Our gap theorems
hold for smoothly parametrized families of “hard” functions, contrary to count-
able, discrete families known in the literature. An example consequence of our
gap theorems is the following: for every natural number k there exists a function
representable by a ReLU DNN with k2 hidden layers and total size k3, such that
any ReLU DNN with at most k hidden layers will require at least 1
2 kk+1 − 1 total
nodes. Finally, for the family of Rn → R DNNs with ReLU activations, we show
a new lowerbound on the number of afﬁne pieces, which is larger than previous
constructions in certain regimes of the network architecture and most distinctively
our lowerbound is demonstrated by an explicit construction of a smoothly param-
eterized family of functions attaining this scaling. Our construction utilizes the
theory of zonotopes from polyhedral theory.

1

INTRODUCTION

Deep neural networks (DNNs) provide an excellent family of hypotheses for machine learning tasks
such as classiﬁcation. Neural networks with a single hidden layer of ﬁnite size can represent any
continuous function on a compact subset of Rn arbitrary well. The universal approximation result
was ﬁrst given by Cybenko in 1989 for sigmoidal activation function (Cybenko, 1989), and later
generalized by Hornik to an arbitrary bounded and nonconstant activation function Hornik (1991).
Furthermore, neural networks have ﬁnite VC dimension (depending polynomially on the number of
edges in the network), and therefore, are PAC (probably approximately correct) learnable using a
sample of size that is polynomial in the size of the networks Anthony & Bartlett (1999). However,
neural networks based methods were shown to be computationally hard to learn (Anthony & Bartlett,
1999) and had mixed empirical success. Consequently, DNNs fell out of favor by late 90s.
Recently, there has been a resurgence of DNNs with the advent of deep learning LeCun et al. (2015).
Deep learning, loosely speaking, refers to a suite of computational techniques that have been devel-
oped recently for training DNNs. It started with the work of Hinton et al. (2006), which gave empir-
ical evidence that if DNNs are initialized properly (for instance, using unsupervised pre-training),
then we can ﬁnd good solutions in a reasonable amount of runtime. This work was soon followed by
a series of early successes of deep learning at signiﬁcantly improving the state-of-the-art in speech
recognition Hinton et al. (2012). Since then, deep learning has received immense attention from
the machine learning community with several state-of-the-art AI systems in speech recognition, im-
age classiﬁcation, and natural language processing based on deep neural nets Hinton et al. (2012);
Dahl et al. (2013); Krizhevsky et al. (2012); Le (2013); Sutskever et al. (2014). While there is less
of evidence now that pre-training actually helps, several other solutions have since been put forth

∗Department of Computer Science, Email: arora@cs.jhu.edu
†Department of Applied Mathematics and Statistics, Email: basu.amitabh@jhu.edu
‡Department of Computer Science, Email: mianjy@jhu.edu
§Department of Applied Mathematics and Statistics, Email: amukhe14@jhu.edu

1

