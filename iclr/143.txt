Published as a conference paper at ICLR 2018

BI-DIRECTIONAL BLOCK SELF-ATTENTION FOR FAST
AND MEMORY-EFFICIENT SEQUENCE MODELING

Tao Shen†, Tianyi Zhou‡, Guodong Long†, Jing Jiang†& Chengqi Zhang†
†Centre for Artiﬁcial Intelligence, School of Software, University of Technology Sydney
‡Paul G. Allen School of Computer Science & Engineering, University of Washington
tao.shen@student.uts.edu.au,tianyizh@uw.edu
{guodong.long,jing.jiang,chengqi.zhang}@uts.edu.au

ABSTRACT

Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-
attention networks (SAN) are commonly used to produce context-aware represen-
tations. RNN can capture long-range dependency but is hard to parallelize and
not time-efﬁcient. CNN focuses on local dependency but does not perform well
on some tasks. SAN can model both such dependencies via highly parallelizable
computation, but memory requirement grows rapidly in line with sequence length.
In this paper, we propose a model, called “bi-directional block self-attention net-
work (Bi-BloSAN)”, for RNN/CNN-free sequence encoding. It requires as little
memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire se-
quence into blocks, and applies an intra-block SAN to each block for modeling
local context, then applies an inter-block SAN to the outputs for all blocks to
capture long-range dependency. Thus, each SAN only needs to process a short
sequence, and only a small amount of memory is required. Additionally, we use
feature-level attention to handle the variation of contexts around the same word,
and use forward/backward masks to encode temporal order information. On nine
benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves
upon state-of-the-art accuracy, and shows better efﬁciency-memory trade-off than
existing RNN/CNN/SAN.

1

INTRODUCTION

Context dependency provides critical information for most natural language processing (NLP) tasks.
In deep neural networks (DNN), context dependency is usually modeled by a context fusion module,
whose goal is to learn a context-aware representation for each token from the input sequence. Re-
current neural networks (RNN), convolutional neural networks (CNN) and self-attention networks
(SAN) are commonly used as context fusion modules. However, each has its own merits and defects,
so which network to use is an open problem and mainly depends on the speciﬁc task.
RNN is broadly used given its capability in capturing long-range dependency through recurrent
computation. It has been applied to various NLP tasks, e.g., question answering (Wang et al., 2017),
neural machine translation (Bahdanau et al., 2015), sentiment analysis (Qian et al., 2017), natural
language inference (Liu et al., 2016), etc. However, training the basic RNN encounters the gradient
dispersion problem, and is difﬁcult to parallelize. Long short-term memory (LSTM) (Hochreiter &
Schmidhuber, 1997) effectively avoids the vanishing gradient. Gated recurrent unit (GRU) (Chung
et al., 2014) and simple recurrent unit (SRU) (Lei & Zhang, 2017) improve the efﬁciency by reducing
parameters and removing partial temporal-dependency, respectively. However, they still suffer from
expensive time cost, especially when applied to long sequences.
CNN becomes popular recently on some NLP tasks because of its the highly parallelizable convo-
lution computation (Dong et al., 2017). Unlike RNN, CNN can simultaneously apply convolutions
deﬁned by different kernels to multiple chunks of a sequence (Kim, 2014). It is mainly used for
sentence-encoding tasks (Lei et al., 2015; Kalchbrenner et al., 2014). Recently, hierarchical CNNs,
e.g. ByteNet (Kalchbrenner et al., 2016), and ConvS2S (Gehring et al., 2017), are proposed to cap-
ture relatively long-range dependencies by using stacking CNNs to increase the number of input

1

