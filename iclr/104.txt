Published as a conference paper at ICLR 2018

VITERBI-BASED PRUNING FOR SPARSE MATRIX WITH
FIXED AND HIGH INDEX COMPRESSION RATIO

Dongsoo Lee1,2∗, Daehyun Ahn1, Taesu Kim1, Pierce I. Chuang3, Jae-Joon Kim1
1POSTECH, Department of Creative IT Engineering, Korea
2Samsung Research, Korea
3IBM Thomas J. Watson Research Center, USA
dongsoo3.lee@samsung.com,{daehyun.ahn,taesukim}@postech.ac.kr
pchuang@us.ibm.com, jaejoon@postech.ac.kr

ABSTRACT

Weight pruning has proven to be an effective method of reducing the model size
and computation cost without sacriﬁcing its model accuracy. Conventional sparse
matrix formats, however, involve irregular index structures with large storage re-
quirement and a sequential reconstruction process, resulting in inefﬁcient use of
highly parallel computing resources. Hence, pruning is usually restricted to infer-
ence with a batch size of one, for which an efﬁcient parallel matrix-vector multi-
plication method exists. In this paper, a new class of sparse matrix representation
is proposed utilizing the Viterbi algorithm that has a high, and more importantly,
ﬁxed index compression ratio regardless of the pruning rate. In this approach,
numerous sparse matrix candidates are ﬁrst generated by the Viterbi encoder, and
the candidate that aims to minimize the model accuracy degradation is then se-
lected by the Viterbi algorithm. The model pruning process based on the proposed
Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be imple-
mented efﬁciently in hardware to achieve low-energy and a high-performance
index decoding process. Compared with the existing magnitude-based pruning
methods, the index data storage requirement can be further compressed by 85.2%
in MNIST and 83.9% in AlexNet while achieving a similar pruning rate. Even
compared with the relative index compression technique, our method can still re-
duce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.

1

INTRODUCTION

Deep neural networks (DNNs) demand an increasing number of parameters as the required com-
plexity of tasks and supporting number of training data continue to grow (Bengio & Lecun, 2007).
Correspondingly, DNN incurs a considerable number of computations and amount of memory foot-
print, and thus requires high performance parallel computing systems to meet the target response
time. As an effort to realize energy-efﬁcient DNN, researchers have suggested various low-cost
hardware implementation techniques. Among them, pruning has been actively studied to reduce the
redundant connections while not degrading the model accuracy. It has been shown that pruning can
achieve 9× to 13× reduction in connections (Han et al., 2015).
After pruning, the remaining parameters are often stored in sparse matrix formats. Different ways
of representing indices of non-zero values constitute the different sparse matrix format, and have
a signiﬁcant impact on the level of achievable computational parallelism when a sparse matrix is
used as an input operand (Bell & Garland, 2009). If the format is not properly designed, then the
performance of DNN with a sparse matrix can be even lower than the case with dense matrix (Yu
et al., 2017). The two most important characteristics of a hardware-friendly sparse matrix format are
1) reducing index storage footprint and 2) parallelizable index decoding process. As a compromise
between index size reduction and index decoding complexity, numerous formats have been proposed
(Bell & Garland, 2009).

∗Work done while at POSTECH.

1

