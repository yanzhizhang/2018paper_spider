Published as a conference paper at ICLR 2018

SPECTRAL NORMALIZATION
FOR GENERATIVE ADVERSARIAL NETWORKS

Takeru Miyato1, Toshiki Kataoka1, Masanori Koyama2, Yuichi Yoshida3
{miyato, kataoka}@preferred.jp
koyama.masanori@gmail.com
yyoshida@nii.ac.jp
1Preferred Networks, Inc. 2Ritsumeikan University 3National Institute of Informatics

ABSTRACT

One of the challenges in the study of generative adversarial networks is the insta-
bility of its training. In this paper, we propose a novel weight normalization tech-
nique called spectral normalization to stabilize the training of the discriminator.
Our new normalization technique is computationally light and easy to incorporate
into existing implementations. We tested the efﬁcacy of spectral normalization on
CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally conﬁrmed
that spectrally normalized GANs (SN-GANs) is capable of generating images of
better or equal quality relative to the previous training stabilization techniques.
The code with Chainer (Tokui et al., 2015), generated images and pretrained mod-
els are available at https://github.com/pfnet-research/sngan_
projection.

1

INTRODUCTION

Generative adversarial networks (GANs) (Goodfellow et al., 2014) have been enjoying considerable
success as a framework of generative models in recent years, and it has been applied to numerous
types of tasks and datasets (Radford et al., 2016; Salimans et al., 2016; Ho & Ermon, 2016; Li et al.,
2017). In a nutshell, GANs are a framework to produce a model distribution that mimics a given
target distribution, and it consists of a generator that produces the model distribution and a discrimi-
nator that distinguishes the model distribution from the target. The concept is to consecutively train
the model distribution and the discriminator in turn, with the goal of reducing the difference be-
tween the model distribution and the target distribution measured by the best discriminator possible
at each step of the training. GANs have been drawing attention in the machine learning community
not only for its ability to learn highly structured probability distribution but also for its theoretically
interesting aspects. For example, (Nowozin et al., 2016; Uehara et al., 2016; Mohamed & Laksh-
minarayanan, 2017) revealed that the training of the discriminator amounts to the training of a good
estimator for the density ratio between the model distribution and the target. This is a perspective
that opens the door to the methods of implicit models (Mohamed & Lakshminarayanan, 2017; Tran
et al., 2017) that can be used to carry out variational optimization without the direct knowledge of
the density function.
A persisting challenge in the training of GANs is the performance control of the discriminator. In
high dimensional spaces, the density ratio estimation by the discriminator is often inaccurate and
unstable during the training, and generator networks fail to learn the multimodal structure of the
target distribution. Even worse, when the support of the model distribution and the support of the
target distribution are disjoint, there exists a discriminator that can perfectly distinguish the model
distribution from the target (Arjovsky & Bottou, 2017). Once such discriminator is produced in
this situation, the training of the generator comes to complete stop, because the derivative of the
so-produced discriminator with respect to the input turns out to be 0. This motivates us to introduce
some form of restriction to the choice of the discriminator.
In this paper, we propose a novel weight normalization method called spectral normalization that
can stabilize the training of discriminator networks. Our normalization enjoys following favorable
properties.

1

