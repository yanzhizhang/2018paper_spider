Published as a conference paper at ICLR 2018

SMASH:
SEARCH THROUGH HYPERNETWORKS

ONE-SHOT MODEL

ARCHITECTURE

Andrew Brock, Theodore Lim, & J.M. Ritchie
School of Engineering and Physical Sciences
Heriot-Watt University
Edinburgh, UK
{ajb5, t.lim, j.m.ritchie}@hw.ac.uk

Nick Weston
Renishaw plc
Research Ave, North
Edinburgh, UK
Nick.Weston@renishaw.com

ABSTRACT

Designing architectures for deep neural networks requires expert knowledge and
substantial computation time. We propose a technique to accelerate architec-
ture selection by learning an auxiliary HyperNet that generates the weights of a
main model conditioned on that model’s architecture. By comparing the relative
validation performance of networks with HyperNet-generated weights, we can
effectively search over a wide range of architectures at the cost of a single training
run. To facilitate this search, we develop a ﬂexible mechanism based on memory
read-writes that allows us to deﬁne a wide range of network connectivity pat-
terns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate
our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and
Imagenet32x32, achieving competitive performance with similarly-sized hand-
designed networks.

1

INTRODUCTION

The high performance of deep neural nets is tempered by the cost of extensive engineering and
validation to ﬁnd the best architecture for a given problem. High-level design decisions such as
depth, units per layer, and layer connectivity are not always obvious, and the success of models
such as Inception (Szegedy et al., 2016), ResNets (He et al., 2016), FractalNets (Larsson et al.,
2017) and DenseNets (Huang et al., 2017) demonstrates the beneﬁts of intricate design patterns.
Even with expert knowledge, determining which design elements to weave together requires ample
experimentation.
In this work, we propose to bypass the expensive procedure of fully training candidate models by
instead training an auxiliary model, a HyperNet (Ha et al., 2017), to dynamically generate the weights
of a main model with variable architecture. Though these generated weights are worse than freely
learned weights for a ﬁxed architecture, we leverage the observation (Li et al., 2017) that the relative
performance of different networks early in training (i.e. some distance from the eventual optimum)
often provides a meaningful indication of performance at optimality. By comparing validation
performance for a set of architectures using generated weights, we can approximately rank numerous
architectures at the cost of a single training run.
To facilitate this search, we develop a ﬂexible scheme based on memory read-writes that allows
us to deﬁne a diverse range of architectures, with ResNets, DenseNets, and FractalNets as special
cases. We validate our one-Shot Model Architecture Search through HyperNetworks (SMASH)
for Convolutional Neural Networks (CNN) on CIFAR-10 and CIFAR-100 (Krizhevsky and Hinton,
2009), Imagenet32x32 (Chrabaszcz et al., 2017), ModelNet10 (Wu et al., 2015), and STL-10 (Coates
et al., 2011), achieving competitive performance with similarly-sized hand-designed networks.

2 RELATED WORK

Modern practical methods for optimizing hyperparameters rely on random search (Bergstra and Ben-
gio, 2012) or Bayesian Optimization (BO) (Snoek et al., 2012; 2015), treating the model performance

1

