Published as a conference paper at ICLR 2018

LEARNING INTRINSIC SPARSE STRUCTURES WITHIN
LONG SHORT-TERM MEMORY

Wei Wen∗, Yiran Chen & Hai Li
Electrical and Computer Engineering, Duke University
{wei.wen,yiran.chen,hai.li}@duke.edu

Yuxiong He†, Samyam Rajbhandari†, Minjia Zhang†, Wenhan Wang†, Fang Liu§ & Bin Hu§
Business AI† and Bing§, Microsoft
{yuxhe,samyamr,minjiaz,wenhanw,fangliu,binhu}@microsoft.com

ABSTRACT

Model compression is signiﬁcant for the wide adoption of Recurrent Neural Net-
works (RNNs) in both user devices possessing limited resources and business clus-
ters requiring quick responses to large-scale service requests. This work aims to
learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes
of basic structures within LSTM units, including input updates, gates, hidden
states, cell states and outputs. Independently reducing the sizes of basic struc-
tures can result in inconsistent dimensions among them, and consequently, end up
with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse
Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously
decrease the sizes of all basic structures by one and thereby always maintain the
dimension consistency. By learning ISS within LSTM units, the obtained LSTMs
remain regular while having much smaller basic structures. Based on group Lasso
regularization, our method achieves 10.59× speedup without losing any perplex-
ity of a language modeling of Penn TreeBank dataset. It is also successfully eval-
uated through a compact model with only 2.69M weights for machine Question
Answering of SQuAD dataset. Our approach is successfully extended to non-
LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is
available1.

1

INTRODUCTION

Model Compression (Jaderberg et al. (2014), Han et al. (2015a), Wen et al. (2017), Louizos et al.
(2017)) is a class of approaches of reducing the size of Deep Neural Networks (DNNs) to accelerate
inference. Structure Learning (Zoph & Le (2017), Philipp & Carbonell (2017), Cortes et al. (2017))
emerges as an active research area for DNN structure exploration, potentially replacing human la-
bor with machine automation for design space exploration. In the intersection of both techniques,
an important area is to learn compact structures in DNNs for efﬁcient inference computation us-
ing minimal memory and execution time without losing accuracy. Learning compact structures in
Convolutional Neural Networks (CNNs) have been widely explored in the past few years. Han
et al. (2015b) proposed connection pruning for sparse CNNs. Pruning method also works success-
fully in coarse-grain levels, such as pruning ﬁlters in CNNs (Li et al. (2017)) and reducing neuron
numbers (Alvarez & Salzmann (2016)). Wen et al. (2016) presented a general framework to learn
versatile compact structures (neurons, ﬁlters, ﬁlter shapes, channels and even layers) in DNNs.
Learning the compact structures in Recurrent Neural Networks (RNNs) is more challenging. As
a recurrent unit is shared across all the time steps in sequence, compressing the unit will aggres-
sively affect all the steps. A recent work by Narang et al. (2017) proposes a pruning approach that
deletes up to 90% connections in RNNs. Connection pruning methods sparsify weights of recur-
rent units but cannot explicitly change basic structures, e.g., the number of input updates, gates,

∗Major work was done as an intern in Microsoft Research and Bing.
1https://github.com/wenwei202/iss-rnns

1

