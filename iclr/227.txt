Published as a conference paper at ICLR 2018

CHARACTERIZING ADVERSARIAL SUBSPACES USING
LOCAL INTRINSIC DIMENSIONALITY

Xingjun Ma1, Bo Li2, Yisen Wang3, Sarah M. Erfani1, Sudanthi Wijewickrema1
Grant Schoenebeck4, Dawn Song2, Michael E. Houle5, James Bailey1
1The University of Melbourne, Parkville, Australia
2University of California, Berkeley, USA
3Tsinghua University, Beijing, China
4University of Michigan, Ann Arbor, USA
5National Institute of Informatics, Tokyo, Japan

ABSTRACT

Deep Neural Networks (DNNs) have recently been shown to be vulnerable against
adversarial examples, which are carefully crafted instances that can mislead DNNs
to make errors during prediction. To better understand such attacks, a characteri-
zation is needed of the properties of regions (the so-called ‘adversarial subspaces’)
in which adversarial examples lie. We tackle this challenge by characterizing the
dimensional properties of adversarial regions, via the use of Local Intrinsic Di-
mensionality (LID). LID assesses the space-ﬁlling capability of the region sur-
rounding a reference example, based on the distance distribution of the example
to its neighbors. We ﬁrst provide explanations about how adversarial perturbation
can affect the LID characteristic of adversarial regions, and then show empirically
that LID characteristics can facilitate the distinction of adversarial examples gen-
erated using state-of-the-art attacks. As a proof-of-concept, we show that a poten-
tial application of LID is to distinguish adversarial examples, and the preliminary
results show that it can outperform several state-of-the-art detection measures by
large margins for ﬁve attack strategies considered in this paper across three bench-
mark datasets . Our analysis of the LID characteristic for adversarial regions not
only motivates new directions of effective adversarial defense, but also opens up
more challenges for developing new attacks to better understand the vulnerabili-
ties of DNNs.

1

INTRODUCTION

Deep Neural Networks (DNNs) are highly expressive models that have achieved state-of-the-art per-
formance on a wide range of complex problems, such as speech recognition (Hinton et al., 2012) and
image classiﬁcation (Krizhevsky et al., 2012). However, recent studies have found that DNNs can be
compromised by adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014; Nguyen et al.,
2015). These intentionally-perturbed inputs can induce the network to make incorrect predictions
at test time with high conﬁdence, even when the examples are generated using different networks
(Liu et al., 2016; Carlini & Wagner, 2017b; Papernot et al., 2016b). The amount of perturbation
required is often small, and (in the case of images) imperceptible to human observers. This undesir-
able property of deep networks has become a major security concern in real-world applications of
DNNs, such as self-driving cars and identity recognition (Evtimov et al., 2017; Sharif et al., 2016).
In this paper, we aim to further understand adversarial attacks by characterizing the regions within
which adversarial examples reside.
Each adversarial example can be regarded as being surrounded by a connected region of the domain
(the ‘adversarial region’ or ‘adversarial subspace’) within which all points subvert the classiﬁer in a
similar way. Adversarial regions can be deﬁned not only in the input space, but also with respect to
the activation space of different DNN layers (Szegedy et al., 2013). Developing an understanding of
the properties of adversarial regions is a key requirement for adversarial defense. Under the assump-
tion that data can be modeled in terms of collections of manifolds, several works have attempted to

1

