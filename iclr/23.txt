Published as a conference paper at ICLR 2018

DEEP LEARNING AS A MIXED CONVEX-
COMBINATORIAL OPTIMIZATION PROBLEM

Abram L. Friesen and Pedro Domingos
Paul G. Allen School of Computer Science and Engineering
University of Washington
Seattle, WA 98195, USA
{afriesen,pedrod}@cs.washington.edu

ABSTRACT

As neural networks grow deeper and wider, learning networks with hard-threshold
activations is becoming increasingly important, both for network quantization,
which can drastically reduce time and energy requirements, and for creating large in-
tegrated systems of deep networks, which may have non-differentiable components
and must avoid vanishing and exploding gradients for effective learning. However,
since gradient descent is not applicable to hard-threshold functions, it is not clear
how to learn networks of them in a principled way. We address this problem by
observing that setting targets for hard-threshold hidden units in order to minimize
loss is a discrete optimization problem, and can be solved as such. The discrete opti-
mization goal is to ﬁnd a set of targets such that each unit, including the output, has
a linearly separable problem to solve. Given these targets, the network decomposes
into individual perceptrons, which can then be learned with standard convex ap-
proaches. Based on this, we develop a recursive mini-batch algorithm for learning
deep hard-threshold networks that includes the popular but poorly justiﬁed straight-
through estimator as a special case. Empirically, we show that our algorithm
improves classiﬁcation accuracy in a number of settings, including for AlexNet
and ResNet-18 on ImageNet, when compared to the straight-through estimator.

INTRODUCTION

1
The original approach to neural classiﬁcation was to learn single-layer models with hard-threshold ac-
tivations, like the perceptron (Rosenblatt, 1958). However, it proved difﬁcult to extend these methods
to multiple layers, because hard-threshold units, having zero derivative almost everywhere and being
discontinuous at the origin, cannot be trained by gradient descent. Instead, the community turned
to multilayer networks with soft activation functions, such as the sigmoid and, more recently, the
ReLU, for which gradients can be computed efﬁciently by backpropagation (Rumelhart et al., 1986).
This approach has enjoyed remarkable success, enabling researchers to train networks with hundreds
of layers and learn models that have signiﬁcantly higher accuracy on a variety of tasks than any
previous approach. However, as networks become deeper and wider, there has been a growing
trend towards using hard-threshold activations for quantization purposes, where they enable binary
or low-precision inference (e.g., Hubara et al. (2016); Rastegari et al. (2016); Zhou et al. (2016);
Lin & Talathi (2016); Zhu et al. (2017)) and training (e.g., Lin et al. (2016); Li et al. (2017); Tang
et al. (2017); Micikevicius et al. (2017)), which can greatly reduce the energy and computation time
required by modern deep networks. Beyond quantization, the scale of the output of hard-threshold
units is independent of (or insensitive to) the scale of their input, which can alleviate vanishing
and exploding gradient issues and should help avoid some of the pathologies that occur during
low-precision training with backpropagation (Li et al., 2017). Avoiding these issues is crucial for
developing large systems of deep networks that can be used to perform even more complex tasks.
For these reasons, we are interested in developing well-motivated and efﬁcient techniques for learning
deep neural networks with hard-threshold units. In this work, we propose a framework for learning
deep hard-threshold networks that stems from the observation that hard-threshold units output discrete
values, indicating that combinatorial optimization may provide a principled method for training these
networks. By specifying a set of discrete targets for each hidden-layer activation, the network

1

