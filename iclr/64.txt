Published as a conference paper at ICLR 2018

GRAPH ATTENTION NETWORKS

Petar Veliˇckovi´c∗
Department of Computer Science and Technology
University of Cambridge
petar.velickovic@cst.cam.ac.uk

Guillem Cucurull∗
Centre de Visi´o per Computador, UAB
gcucurull@gmail.com

Arantxa Casanova∗
Centre de Visi´o per Computador, UAB
ar.casanova.8@gmail.com

Adriana Romero
Montr´eal Institute for Learning Algorithms
Facebook AI Research
adrianars@fb.com

Pietro Li`o
Department of Computer Science and Technology
University of Cambridge
pietro.lio@cst.cam.ac.uk

Yoshua Bengio
Montr´eal Institute for Learning Algorithms
yoshua.umontreal@gmail.com

ABSTRACT

We present graph attention networks (GATs), novel neural network architectures
that operate on graph-structured data, leveraging masked self-attentional layers to
address the shortcomings of prior methods based on graph convolutions or their
approximations. By stacking layers in which nodes are able to attend over their
neighborhoods’ features, we enable (implicitly) specifying different weights to
different nodes in a neighborhood, without requiring any kind of computationally
intensive matrix operation (such as inversion) or depending on knowing the graph
structure upfront. In this way, we address several key challenges of spectral-based
graph neural networks simultaneously, and make our model readily applicable to
inductive as well as transductive problems. Our GAT models have achieved or
matched state-of-the-art results across four established transductive and inductive
graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as
well as a protein-protein interaction dataset (wherein test graphs remain unseen
during training).

1

INTRODUCTION

Convolutional Neural Networks (CNNs) have been successfully applied to tackle problems such
as image classiﬁcation (He et al., 2016), semantic segmentation (J´egou et al., 2017) or machine
translation (Gehring et al., 2016), where the underlying data representation has a grid-like structure.
These architectures efﬁciently reuse their local ﬁlters, with learnable parameters, by applying them
to all the input positions.
However, many interesting tasks involve data that can not be represented in a grid-like structure and
that instead lies in an irregular domain. This is the case of 3D meshes, social networks, telecommu-
nication networks, biological networks or brain connectomes. Such data can usually be represented
in the form of graphs.
There have been several attempts in the literature to extend neural networks to deal with arbitrarily
structured graphs. Early work used recursive neural networks to process data represented in graph
domains as directed acyclic graphs (Frasconi et al., 1998; Sperduti & Starita, 1997). Graph Neural
Networks (GNNs) were introduced in Gori et al. (2005) and Scarselli et al. (2009) as a generalization
of recursive neural networks that can directly deal with a more general class of graphs, e.g. cyclic,
directed and undirected graphs. GNNs consist of an iterative process, which propagates the node

∗Work performed while the author was at the Montr´eal Institute of Learning Algorithms.

1

