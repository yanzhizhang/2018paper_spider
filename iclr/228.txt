Published as a conference paper at ICLR 2018

WAVELET POOLING FOR CONVOLUTIONAL NEURAL
NETWORKS

Travis Williams
Department of Electrical Engineering
North Carolina A&T State University
Greensboro, NC 27410, USA
tlwilli3@aggies.ncat.edu

Robert Li
Department of Electrical Engineering
North Carolina A&T State University
Greensboro, NC 27410, USA
eeli@ncat.edu

ABSTRACT

Convolutional Neural Networks continuously advance the progress of 2D and 3D
image and object classiﬁcation. The steadfast usage of this algorithm requires
constant evaluation and upgrading of foundational concepts to maintain progress.
Network regularization techniques typically focus on convolutional layer opera-
tions, while leaving pooling layer operations without suitable options. We intro-
duce Wavelet Pooling as another alternative to traditional neighborhood pooling.
This method decomposes features into a second level decomposition, and discards
the ﬁrst-level subbands to reduce feature dimensions. This method addresses the
overﬁtting problem encountered by max pooling, while reducing features in a
more structurally compact manner than pooling via neighborhood regions. Ex-
perimental results on four benchmark classiﬁcation datasets demonstrate our pro-
posed method outperforms or performs comparatively with methods like max,
mean, mixed, and stochastic pooling.

1

INTRODUCTION

Convolutional Neural Networks (CNNs) have become the standard-bearer in image and object clas-
siﬁcation (Nielsen, 2015). Due to the layer structures conforming to the shape of the inputs, CNNs
consistently classify images, objects, videos, etc. at a higher accuracy rate than vector-based deep
learning techniques (Nielsen, 2015). The strength of this algorithm motivates researchers to con-
stantly evaluate and upgrade foundational concepts to continue growth and progress. The key com-
ponents of CNN, the convolutional layer and pooling layer, consistently undergo modiﬁcations and
innovations to elevate accuracy and efﬁciency of CNNs beyond previous benchmarks.
Pooling has roots in predecessors to CNN such as Neocognitron, which manual subsampling by the
user occurs (Fukushima, 1979), and Cresceptron, which introduces the ﬁrst max pooling operation
in deep learning (Weng et al., 1992). Pooling subsamples the results of the convolutional layers,
gradually reducing spatial dimensions of the data throughout the network. The beneﬁts of this oper-
ation are to reduce parameters, increase computational efﬁciency, and regulate overﬁtting (Boureau
et al., 2010).
Methods of pooling vary, with the most popular form being max pooling, and secondarily, average
pooling (Nielsen, 2015; Lee et al., 2016). These forms of pooling are deterministic, efﬁcient, and
simple, but have weaknesses hindering the potential for optimal network learning (Lee et al., 2016;
Yu et al., 2014). Other pooling operations, notably mixed pooling and stochastic pooling, use prob-
abilistic approaches to correct some of the issues of the prior methods (Yu et al., 2014; Zeiler &
Fergus, 2013).
However, one commonality all these pooling operations employ a neighborhood approach to sub-
sampling, reminiscent of nearest neighbor interpolation in image processing. Neighborhood inter-
polation techniques perform fast, with simplicity and efﬁciency, but introduce artifacts such as edge
halos, blurring, and aliasing (Parker et al., 1983). Minimizing discontinuities in the data are critical
to aiding in network regularization, and increasing classiﬁcation accuracy.

1

