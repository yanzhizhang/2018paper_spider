Published as a conference paper at ICLR 2018

PROXIMAL BACKPROPAGATION

Thomas Frerix1∗, Thomas M¨ollenhoff1∗, Michael Moeller2∗, Daniel Cremers1
thomas.frerix@tum.de
thomas.moellenhoff@tum.de
michael.moeller@uni-siegen.de
cremers@tum.de
1 Technical University of Munich
2 University of Siegen

ABSTRACT

We propose proximal backpropagation (ProxProp) as a novel algorithm that takes
implicit instead of explicit gradient steps to update the network parameters during
neural network training. Our algorithm is motivated by the step size limitation
of explicit gradient descent, which poses an impediment for optimization. Prox-
Prop is developed from a general point of view on the backpropagation algorithm,
currently the most common technique to train neural networks via stochastic gra-
dient descent and variants thereof. Speciﬁcally, we show that backpropagation of
a prediction error is equivalent to sequential gradient descent steps on a quadratic
penalty energy, which comprises the network activations as variables of the op-
timization. We further analyze theoretical properties of ProxProp and in partic-
ular prove that the algorithm yields a descent direction in parameter space and
can therefore be combined with a wide variety of convergent algorithms. Finally,
we devise an efﬁcient numerical implementation that integrates well with popular
deep learning frameworks. We conclude by demonstrating promising numerical
results and show that ProxProp can be effectively combined with common ﬁrst
order optimizers such as Adam.

1

INTRODUCTION

In recent years neural networks have gained considerable attention in solving difﬁcult correla-
tion tasks such as classiﬁcation in computer vision (Krizhevsky et al., 2012) or sequence learn-
ing (Sutskever et al., 2014) and as building blocks of larger learning systems (Silver et al., 2016).
Training neural networks is accomplished by optimizing a nonconvex, possibly nonsmooth, nested
function of the network parameters. Since the introduction of stochastic gradient descent (SGD)
(Robbins & Monro, 1951; Bottou, 1991), several more sophisticated optimization methods have
been studied. One such class is that of quasi-Newton methods, as for example the comparison of
L-BFGS with SGD in (Le et al., 2011), Hessian-free approaches (Martens, 2010), and the Sum of
Functions Optimizer in (Sohl-Dickstein et al., 2013). Several works consider speciﬁc properties of
energy landscapes of deep learning models such as frequent saddle points (Dauphin et al., 2014) and
well-generalizable local optima (Chaudhari et al., 2017a). Among the most popular optimization
methods in currently used deep learning frameworks are momentum based improvements of classi-
cal SGD, notably Nesterov’s Accelerated Gradient (Nesterov, 1983; Sutskever et al., 2013), and the
Adam optimizer (Kingma & Ba, 2015), which uses estimates of ﬁrst and second order moments of
the gradients for parameter updates.
Nevertheless, the optimization of these models remains challenging, as learning with SGD and its
variants requires careful weight initialization and a sufﬁciently small learning rate in order to yield
a stable and convergent algorithm. Moreover, SGD often has difﬁculties in propagating a learning
signal deeply into a network, commonly referred to as the vanishing gradient problem (Hochreiter
et al., 2001).

∗contributed equally

1

