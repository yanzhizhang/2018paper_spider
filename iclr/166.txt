Published as a conference paper at ICLR 2018

POLICY OPTIMIZATION BY GENETIC DISTILLATION

Tanmay Gangwani
Computer Science
UIUC
Urbana, IL 61801
gangwan2@illinios.edu

Jian Peng
Computer Science
UIUC
Urbana, IL 61801
jianpeng@illinois.edu

ABSTRACT

Genetic algorithms have been widely used in many practical optimization prob-
lems.
Inspired by natural selection, operators, including mutation, crossover
and selection, provide effective heuristics for search and black-box optimization.
However, they have not been shown useful for deep reinforcement learning, pos-
sibly due to the catastrophic consequence of parameter crossovers of neural net-
works. Here, we present Genetic Policy Optimization (GPO), a new genetic algo-
rithm for sample-efﬁcient deep policy optimization. GPO uses imitation learning
for policy crossover in the state space and applies policy gradient methods for mu-
tation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm
is able to provide superior performance over the state-of-the-art policy gradient
methods and achieves comparable or higher sample efﬁciency.

1

INTRODUCTION

Reinforcement learning (RL) has recently demonstrated signiﬁcant progress and achieves state-of-
the-art performance in games (Mnih et al., 2015; Silver et al., 2016), locomotion control (Lillicrap
et al., 2015), visual-navigation (Zhu et al., 2017), and robotics (Levine et al., 2016). Among these
successes, deep neural networks (DNNs) are widely used as powerful functional approximators to
enable signal perception, feature extraction and complex decision making. For example, in con-
tinuous control tasks, the policy that determines which action to take is often parameterized by a
deep neural network that takes the current state observation or sensor measurements as input. In
order to optimize such policies, various policy gradient methods (Mnih et al., 2016; Schulman
et al., 2015; 2017; Heess et al., 2017) have been proposed to estimate gradients approximately from
rollout trajectories. The core idea of these policy gradient methods is to take advantage of the tem-
poral structure in the rollout trajectories to construct a Monte Carlo estimator of the gradient of the
expected return.
In addition to the popular policy gradient methods, other alternative solutions, such as those for
black-box optimization or stochastic optimization, have been recently studied for policy optimiza-
tion. Evolution strategies (ES) is a class of stochastic optimization techniques that can search the
policy space without relying on the backpropagation of gradients. At each iteration, ES samples
a candidate population of parameter vectors (“genotypes”) from a probability distribution over the
parameter space, evaluates the objective function (“ﬁtness”) on these candidates, and constructs a
new probability distribution over the parameter space using the candidates with the high ﬁtness.
This process is repeated iteratively until the objective is maximized. Covariance matrix adaptation
evolution strategy (CMA-ES; Hansen & Ostermeier (2001)) and recent work from Salimans et al.
(2017) are examples of this procedure. These ES algorithms have also shown promising results on
continuous control tasks and Atari games, but their sample efﬁciency is often not comparable to
the advanced policy gradient methods, because ES is black-box and thus does not fully exploit the
policy network architectures or the temporal structure of the RL problems.
Very similar to ES, genetic algorithms (GAs) are a heuristic search technique for search and opti-
mization. Inspired by the process of natural selection, GAs evolve an initial population of genotypes
by repeated application of three genetic operators - mutation, crossover and selection. One of the
main differences between GA and ES is the use of the crossover operator in GA, which is able
to provide higher diversity of good candidates in the population. However, the crossover operator

1

