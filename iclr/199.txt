Published as a conference paper at ICLR 2018

INTRINSIC MOTIVATION AND AUTOMATIC
CURRICULA VIA ASYMMETRIC SELF-PLAY

Sainbayar Sukhbaatar
Dept. of Computer Science
New York University
sainbar@cs.nyu.edu

Zeming Lin
Facebook AI Research
New York
zlin@fb.com

Ilya Kostrikov
Dept. of Computer Science
New York University
kostrikov@cs.nyu.edu

Gabriel Synnaeve, Arthur Szlam & Rob Fergus
Facebook AI Research
New York
{gab,aszlam,robfergus}@fb.com

ABSTRACT

We describe a simple scheme that allows an agent to learn about its environment
in an unsupervised manner. Our scheme pits two versions of the same agent, Al-
ice and Bob, against one another. Alice proposes a task for Bob to complete; and
then Bob attempts to complete the task. In this work we will focus on two kinds
of environments: (nearly) reversible environments and environments that can be
reset. Alice will “propose” the task by doing a sequence of actions and then Bob
must undo or repeat them, respectively. Via an appropriate reward structure, Alice
and Bob automatically generate a curriculum of exploration, enabling unsuper-
vised training of the agent. When Bob is deployed on an RL task within the en-
vironment, this unsupervised training reduces the number of supervised episodes
needed to learn, and in some cases converges to a higher reward.

1

INTRODUCTION

Model-free approaches to reinforcement learning are sample inefﬁcient, typically requiring a huge
number of episodes to learn a satisfactory policy. The lack of an explicit environment model means
the agent must learn the rules of the environment from scratch at the same time as it tries to un-
derstand which trajectories lead to rewards. In environments where reward is sparse, only a small
fraction of the agents’ experience is directly used to update the policy, contributing to the inefﬁ-
ciency.
In this paper we introduce a novel form of unsupervised training for an agent that enables exploration
and learning about the environment without any external reward that incentivizes the agents to learn
how to transition between states as efﬁciently as possible. We demonstrate that this unsupervised
training allows the agent to learn new tasks within the environment quickly.

2 APPROACH
We consider environments with a single physical agent (or multiple physical units controlled by
a single agent), but we allow it to have two separate “minds”: Alice and Bob, each with its own
objective and parameters. During self-play episodes, Alice’s job is to propose a task for Bob to
complete, and Bob’s job is to complete the task. When presented with a target task episode, Bob is
then used to perform it (Alice plays no role). The key idea is that the Bob’s play with Alice should
help him understand how the environment works and enabling him to learn the target task more
quickly.
Our approach is restricted to two classes of environment: (i) those that are (nearly) reversible, or
(ii) ones that can be reset to their initial state (at least once). These restrictions allow us to sidestep
complications around how to communicate the task and determine its difﬁculty (see Appendix F.2

1

