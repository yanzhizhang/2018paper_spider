Published as a conference paper at ICLR 2018

THE HIGH-DIMENSIONAL GEOMETRY OF BINARY
NEURAL NETWORKS

Alexander G. Anderson
Redwood Center for Theoretical Neuroscience
University of California, Berkeley
aga@berkeley.edu

Cory P. Berg
Redwood Center for Theoretical Neuroscience
University of California, Berkeley
cberg500@berkeley.edu

ABSTRACT

Recent research has shown that one can train a neural network with binary weights
and activations at train time by augmenting the weights with a high-precision
continuous latent variable that accumulates small changes from stochastic gradient
descent. However, there is a dearth of work to explain why one can effectively cap-
ture the features in data with binary weights and activations. Our main result is that
the neural networks with binary weights and activations trained using the method
of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geom-
etry of binary vectors. In particular, the ideal continuous vectors that extract out
features in the intermediate representations of these BNNs are well-approximated
by binary vectors in the sense that dot products are approximately preserved. Fur-
thermore, the results and analysis used on BNNs are shown to generalize to neural
networks with ternary weights and activations. Compared to previous research that
demonstrated good classiﬁcation performance with BNNs, our work explains why
these BNNs work in terms of HD geometry. Our theory serves as a foundation
for understanding not only BNNs but a variety of methods that seek to compress
traditional neural networks. Furthermore, a better understanding of multilayer
binary neural networks serves as a starting point for generalizing BNNs to other
neural network architectures such as recurrent neural networks.

1

INTRODUCTION

The rapidly decreasing cost of computation has driven many successes in the ﬁeld of deep learning in
recent years. Consequently, researchers are now considering applications of deep learning in resource
limited hardware such as neuromorphic chips, embedded devices and smart phones (Esser et al.
(2016), Neftci et al. (2016), Andri et al. (2017)). A recent realization for both theoretical researchers
and industry practitioners is that traditional neural networks can be compressed because they are
highly over-parameterized. While there has been a large amount of experimental work dedicated to
compressing neural networks (Sec. 2), we focus on the particular approach that replaces costly 32-bit
ﬂoating point multiplications with cheap binary operations. Our analysis reveals a simple geometric
picture based on the geometry of high dimensional binary vectors that allows us to understand the
successes of the recent efforts to compress neural networks.
Courbariaux et al. (2016) and Hubara et al. (2016) showed that one can efﬁciently train neural
networks with binary weights and activations that have similar performance to their continuous
counterparts. Such BNNs execute 7 times faster using a dedicated GPU kernel at test time. Fur-
thermore, they argue that such BNNs require at least a factor of 32 fewer memory accesses at test
time that should result in an even larger energy savings. There are two key ideas in their papers
(Fig. 1). First, a continuous weight, wc, is associated with each binary weight, wb, that accumulates
small changes from stochastic gradient descent. Second, the non-differentiable binarize function
(θ(x) = 1 if x > 0 and −1 otherwise) is replaced with a continuous one during backpropagation.
These modiﬁcations allow one to train neural networks that have binary weights and activations with
stochastic gradient descent. While the work showed how to train such networks, the existence of
neural networks with binary weights and activations needs to be reconciled with previous work that
has sought to understand weight matrices as extracting out continuous features in data (e.g. Zeiler &
Fergus (2014)). Summary of contributions:

1

