Published as a conference paper at ICLR 2018

DEEP VOICE 3: SCALING TEXT-TO-SPEECH WITH
CONVOLUTIONAL SEQUENCE LEARNING

Wei Ping∗, Kainan Peng∗, Andrew Gibiansky∗, Sercan ¨O. Arık∗
Ajay Kannan, Sharan Narang
Baidu Research
{pingwei01, pengkainan, gibianskyandrew, sercanarik,
kannanajay, sharan}@baidu.com
Jonathan Raiman∗†
OpenAI
raiman@openai.com

John Miller∗†
University of California, Berkeley
miller john@berkeley.edu

ABSTRACT

We present Deep Voice 3, a fully-convolutional attention-based neural text-
to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech
synthesis systems in naturalness while training an order of magnitude faster.
We scale Deep Voice 3 to dataset sizes unprecedented for TTS, training on
more than eight hundred hours of audio from over two thousand speakers. In
addition, we identify common error modes of attention-based speech synthe-
sis networks, demonstrate how to mitigate them, and compare several differ-
ent waveform synthesis methods. We also describe how to scale inference to
ten million queries per day on a single GPU server.

1

INTRODUCTION

Text-to-speech (TTS) systems convert written language into human speech. TTS systems are used
in a variety of applications, such as human-technology interfaces, accessibility for the visually-
impaired, media and entertainment. Traditional TTS systems are based on complex multi-stage
hand-engineered pipelines (Taylor, 2009). Typically, these systems ﬁrst transform text into a com-
pact audio representation, and then convert this representation into audio using an audio waveform
synthesis method called a vocoder.
Recent work on neural TTS has demonstrated impressive results, yielding pipelines with simpler
features, fewer components, and higher quality synthesized speech. There is not yet a consensus
on the optimal neural network architecture for TTS. However, sequence-to-sequence models (Wang
et al., 2017; Sotelo et al., 2017; Arık et al., 2017) have shown promising results.
In this paper, we propose a novel, fully-convolutional architecture for speech synthesis, scale it to
very large audio data sets, and address several real-world issues that arise when attempting to deploy
an attention-based TTS system. Speciﬁcally, we make the following contributions:

1. We propose a fully-convolutional character-to-spectrogram architecture, which enables fully
parallel computation and trains an order of magnitude faster than analogous architectures
using recurrent cells (e.g., Wang et al., 2017).

2. We show that our architecture trains quickly and scales to the LibriSpeech ASR dataset

(Panayotov et al., 2015), which consists of 820 hours of audio data from 2484 speakers.

3. We demonstrate that we can generate monotonic attention behavior, avoiding error modes

commonly affecting sequence-to-sequence models.

4. We compare the quality of several waveform synthesis methods, including WORLD (Morise

et al., 2016), Grifﬁn-Lim (Grifﬁn & Lim, 1984), and WaveNet (Oord et al., 2016).

∗Authors listed in reverse alphabetical order.
†These authors contributed to this work while members of Baidu Research.

1

