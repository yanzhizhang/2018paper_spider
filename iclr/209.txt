Published as a conference paper at ICLR 2018

CERTIFIED DEFENSES AGAINST ADVERSARIAL EX-
AMPLES

Aditi Raghunathan, Jacob Steinhardt & Percy Liang
Department of Computer Science
Stanford University
{aditir,jsteinhardt,pliang}@cs.stanford.edu

ABSTRACT

While neural networks have achieved high accuracy on standard image classiﬁ-
cation benchmarks, their accuracy drops to nearly zero in the presence of small
adversarial perturbations to test inputs. Defenses based on regularization and ad-
versarial training have been proposed, but often followed by new, stronger attacks
that defeat these defenses. Can we somehow end this arms race? In this work,
we study this problem for neural networks with one hidden layer. We ﬁrst pro-
pose a method based on a semideﬁnite relaxation that outputs a certiﬁcate that for
a given network and test input, no attack can force the error to exceed a certain
value. Second, as this certiﬁcate is differentiable, we jointly optimize it with the
network parameters, providing an adaptive regularizer that encourages robustness
against all attacks. On MNIST, our approach produces a network and a certiﬁcate
that no attack that perturbs each pixel by at most  = 0.1 can cause more than
35% test error.

1

INTRODUCTION

Despite the impressive (and sometimes even superhuman) accuracies of machine learning on diverse
tasks such as object recognition (He et al., 2015), speech recognition (Xiong et al., 2016), and play-
ing Go (Silver et al., 2016), classiﬁers still fail catastrophically in the presence of small imperceptible
but adversarial perturbations (Szegedy et al., 2014; Goodfellow et al., 2015; Kurakin et al., 2016). In
addition to being an intriguing phenonemon, the existence of such “adversarial examples” exposes a
serious vulnerability in current ML systems (Evtimov et al., 2017; Sharif et al., 2016; Carlini et al.,
2016). While formally deﬁning an “imperceptible” perturbation is difﬁcult, a commonly-used proxy
is perturbations that are bounded in (cid:96)∞-norm (Goodfellow et al., 2015; Madry et al., 2017; Tramèr
et al., 2017); we focus on this attack model in this paper, as even for this proxy it is not known how
to construct high-performing image classiﬁers that are robust to perturbations.
While a proposed defense (classiﬁer) is often empirically shown to be successful against the set of
attacks known at the time, new stronger attacks are subsequently discovered that render the defense
useless. For example, defensive distillation (Papernot et al., 2016c) and adversarial training against
the Fast Gradient Sign Method (Goodfellow et al., 2015) were two defenses that were later shown
to be ineffective against stronger attacks (Carlini & Wagner, 2016; Tramèr et al., 2017). In order to
break this arms race between attackers and defenders, we need to come up with defenses that are
successful against all attacks within a certain class.
However, even computing the worst-case error for a given network against all adversarial pertur-
bations in an (cid:96)∞-ball is computationally intractable. One common approximation is to replace the
worst-case loss with the loss from a given heuristic attack strategy, such as the Fast Gradient Sign
Method (Goodfellow et al., 2015) or more powerful iterative methods (Carlini & Wagner, 2017a;
Madry et al., 2017). Adversarial training minimizes the loss with respect to these heuristics. How-
ever, this essentially minimizes a lower bound on the worst-case loss, which is problematic since
points where the bound is loose have disproportionately lower objective values, which could lure
and mislead an optimizer. Indeed, while adversarial training often provides robustness against a
speciﬁc attack, it often fails to generalize to new attacks, as described above. Another approach is to
compute the worst-case perturbation exactly using discrete optimization (Katz et al., 2017a; Carlini

1

