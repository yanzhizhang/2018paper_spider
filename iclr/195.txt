Published as a conference paper at ICLR 2018

GENERATING NATURAL ADVERSARIAL EXAMPLES

Zhengli Zhao
University of California
Irvine, CA 92697, USA
zhengliz@uci.edu

Dheeru Dua
University of California
Irvine, CA 92697, USA
ddua@uci.edu

Sameer Singh
University of California
Irvine, CA 92697, USA
sameer@uci.edu

ABSTRACT

Due to their complex nature, it is hard to characterize the ways in which machine
learning models can misbehave or be exploited when deployed. Recent work on
adversarial examples, i.e. inputs with minor perturbations that result in substantially
different model predictions, is helpful in evaluating the robustness of these models
by exposing the adversarial scenarios where they fail. However, these malicious
perturbations are often unnatural, not semantically meaningful, and not applicable
to complicated domains such as language. In this paper, we propose a framework to
generate natural and legible adversarial examples that lie on the data manifold, by
searching in semantic space of dense and continuous data representation, utilizing
the recent advances in generative adversarial networks. We present generated
adversaries to demonstrate the potential of the proposed approach for black-box
classiﬁers for a wide range of applications such as image classiﬁcation, textual
entailment, and machine translation. We include experiments to show that the
generated adversaries are natural, legible to humans, and useful in evaluating and
analyzing black-box classiﬁers.

1

INTRODUCTION

With the impressive success and extensive use of machine learning models in various security-
sensitive applications, it has become crucial to study vulnerabilities in these systems. Dalvi et al.
(2004) show that adversarial manipulations of input data often result in incorrect predictions from
classiﬁers. This raises serious concerns regarding the security and integrity of existing machine
learning algorithms, especially when even state-of-the-art models including deep neural networks have
been shown to be highly vulnerable to adversarial attacks with intentionally worst-case perturbations
to the input (Szegedy et al., 2014; Goodfellow et al., 2015; Kurakin et al., 2016; Papernot et al.,
2016b; Kurakin et al., 2017). These adversaries are generated effectively with access to the gradients
of target models, resulting in much higher successful attack rates than data perturbed by random
noise of even larger magnitude. Further, training models by including such adversaries can provide
machine learning models with additional regularization beneﬁts (Goodfellow et al., 2015).
Although these adversarial examples expose “blind spots” in machine learning models, they are
unnatural, i.e. these worst-case perturbed instances are not ones the classiﬁer is likely to face when
deployed. Due to this, it is difﬁcult to gain helpful insights into the fundamental decision behavior
inside the black-box classiﬁer: why is the decision different for the adversary, what can we change in
order to prevent this behavior, and is the classiﬁer robust to natural variations in the data when not in
an adversarial scenario? Moreover, there is often a mismatch between the input space and the semantic
space that we can understand. Changes to the input we may not think meaningful, like slight rotation
or translation in images, often lead to substantial differences in the input instance. For example, Pei
et al. (2017) show that minimal changes in the lighting conditions can fool automated-driving systems,
a behavior adversarial examples are unable to discover. Due to the unnatural perturbations, these
approaches cannot be applied to complex domains such as language, in which enforcing grammar and
semantic similarity is difﬁcult when perturbing instances. Therefore, existing approaches that ﬁnd
adversarial examples for text often result in ungrammatical sentences, as in the examples generated
by Li et al. (2016), or require manual intervention, as in Jia & Liang (2017).
In this paper, we introduce a framework to generate natural adversarial examples, i.e. instances
that are meaningfully similar, valid/legible, and helpful for interpretation. The primary intuition

1

