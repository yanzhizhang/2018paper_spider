Published as a conference paper at ICLR 2018

EVIDENCE AGGREGATION FOR ANSWER RE-RANKING
IN OPEN-DOMAIN QUESTION ANSWERING

Shuohang Wang1∗, Mo Yu2∗, Jing Jiang1,Wei Zhang2, Xiaoxiao Guo2, Shiyu Chang2, Zhiguo Wang2
Tim Klinger2, Gerald Tesauro2 and Murray Campbell2
1School of Information System, Singapore Management University
2AI Foundations - Learning, IBM Research AI
shwang.2014@smu.edu.sg, yum@us.ibm.com, jingjiang@smu.edu.sg

ABSTRACT

A popular recent approach to answering open-domain questions is to ﬁrst search
for question-related passages and then apply reading comprehension models to
extract answers. Existing methods usually extract answers from single passages
independently. But some questions require a combination of evidence from across
different sources to answer correctly. In this paper, we propose two models which
make use of multiple passages to generate their answers. Both use an answer-
reranking approach which reorders the answer candidates generated by an ex-
isting state-of-the-art QA model. We propose two methods, namely, strength-
based re-ranking and coverage-based re-ranking, to make use of the aggregated
evidence from different passages to better determine the answer. Our models
have achieved state-of-the-art results on three public open-domain QA datasets:
Quasar-T, SearchQA and the open-domain version of TriviaQA, with about 8 per-
centage points of improvement over the former two datasets.

1

INTRODUCTION

Open-domain question answering (QA) aims to answer questions from a broad range of domains
by effectively marshalling evidence from large open-domain knowledge sources. Such resources
can be Wikipedia (Chen et al., 2017), the whole web (Ferrucci et al., 2010), structured knowledge
bases (Berant et al., 2013; Yu et al., 2017) or combinations of the above (Baudiˇs & ˇSediv`y, 2015).
Recent work on open-domain QA has focused on using unstructured text retrieved from the web
to build machine comprehension models (Chen et al., 2017; Dhingra et al., 2017b; Wang et al.,
2017). These studies adopt a two-step process: an information retrieval (IR) model to coarsely
select passages relevant to a question, followed by a reading comprehension (RC) model (Wang
& Jiang, 2017; Seo et al., 2017; Chen et al., 2017) to infer an answer from the passages. These
studies have made progress in bringing together evidence from large data sources, but they predict
an answer to the question with only a single retrieved passage at a time. However, answer accuracy
can often be improved by using multiple passages. In some cases, the answer can only be determined
by combining multiple passages.
In this paper, we propose a method to improve open-domain QA by explicitly aggregating evidence
from across multiple passages. Our method is inspired by two notable observations from previous
open-domain QA results analysis:
• First, compared with incorrect answers, the correct answer is often suggested by more passages
repeatedly. For example, in Figure 1(a), the correct answer “danny boy” has more passages
providing evidence relevant to the question compared to the incorrect one. This observation can
be seen as multiple passages collaboratively enhancing the evidence for the correct answer.

• Second, sometimes the question covers multiple answer aspects, which spreads over multiple
passages. In order to infer the correct answer, one has to ﬁnd ways to aggregate those multiple
passages in an effective yet sensible way to try to cover all aspects. In Figure 1(b), for example,
∗Equal contribution.

1

