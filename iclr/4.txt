Published as a conference paper at ICLR 2018

BREAKING THE SOFTMAX BOTTLENECK:
A HIGH-RANK RNN LANGUAGE MODEL

Zhilin Yang∗, Zihang Dai∗, Ruslan Salakhutdinov, William W. Cohen
School of Computer Science
Carnegie Mellon University
{zhiliny,dzihang,rsalakhu,wcohen}@cs.cmu.edu

ABSTRACT

We formulate language modeling as a matrix factorization problem, and show
that the expressiveness of Softmax-based models (including the majority of neu-
ral language models) is limited by a Softmax bottleneck. Given that natural lan-
guage is highly context-dependent, this further implies that in practice Softmax
with distributed word embeddings does not have enough capacity to model nat-
ural language. We propose a simple and effective method to address this issue,
and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to
47.69 and 40.68 respectively. The proposed method also excels on the large-scale
1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.1

1

INTRODUCTION

As a fundamental task in natural language processing, statistical language modeling has gone
through signiﬁcant development from traditional Ngram language models to neural language mod-
els in the last decade (Bengio et al., 2003; Mnih & Hinton, 2007; Mikolov et al., 2010). Despite
the huge variety of models, as a density estimation problem, language modeling mostly relies on a
universal auto-regressive factorization of the joint probability and then models each conditional fac-
tor using different approaches. Speciﬁcally, given a corpus of tokens X = (X1, . . . , XT ), the joint
t P (Xt | Ct), where Ct = X<t is

probability P (X) factorizes as P (X) =(cid:81)

t P (Xt | X<t) =(cid:81)

referred to as the context of the conditional probability hereafter.
Based on the factorization, recurrent neural networks (RNN) based language models achieve state-
of-the-art results on various benchmarks (Merity et al., 2017; Melis et al., 2017; Krause et al., 2017).
A standard approach is to use a recurrent network to encode the context into a ﬁxed size vector,
which is then multiplied by the word embeddings (Inan et al., 2016; Press & Wolf, 2017) using dot
product to obtain the logits. The logits are consumed by the Softmax function to give a categorical
probability distribution over the next token. In spite of the expressiveness of RNNs as universal
approximators (Schäfer & Zimmermann, 2006), an unclear question is whether the combination
of dot product and Softmax is capable of modeling the conditional probability, which can vary
dramatically with the change of the context.
In this work, we study the expressiveness of the aforementioned Softmax-based recurrent language
models from a perspective of matrix factorization. We show that learning a Softmax-based recurrent
language model with the standard formulation is essentially equivalent to solving a matrix factoriza-
tion problem. More importantly, due to the fact that natural language is highly context-dependent,
the matrix to be factorized can be high-rank. This further implies that standard Softmax-based lan-
guage models with distributed (output) word embeddings do not have enough capacity to model
natural language. We call this the Softmax bottleneck.
We propose a simple and effective method to address the Softmax bottleneck. Speciﬁcally, we
introduce discrete latent variables into a recurrent language model, and formulate the next-token
probability distribution as a Mixture of Softmaxes (MoS). Mixture of Softmaxes is more expressive
than Softmax and other surrogates considered in prior work. Moreover, we show that MoS learns

∗Equal contribution. Ordering determined by dice rolling.
1Code is available at https://github.com/zihangdai/mos.

1

