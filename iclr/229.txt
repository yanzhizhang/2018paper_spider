Published as a conference paper at ICLR 2018

VARIATIONAL INFERENCE OF DISENTANGLED LATENT
CONCEPTS FROM UNLABELED OBSERVATIONS

Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan
IBM Research AI
Yorktown Heights, NY
{abhishk,psattig,avinash.bala}@us.ibm.com

ABSTRACT

Disentangled representations, where the higher level data generative factors are
reﬂected in disjoint latent dimensions, offer several beneﬁts such as ease of deriving
invariant representations, transferability to other tasks, interpretability, etc. We
consider the problem of unsupervised learning of disentangled representations from
large pool of unlabeled observations, and propose a variational inference based
approach to infer disentangled latent factors. We introduce a regularizer on the
expectation of the approximate posterior over observed data that encourages the
disentanglement. We also propose a new disentanglement metric which is better
aligned with the qualitative disentanglement observed in the decoder’s output. We
empirically observe signiﬁcant improvement over existing methods in terms of
both disentanglement and data likelihood (reconstruction quality).

1

INTRODUCTION

Feature representations of the observed raw data play a crucial role in the success of machine learning
algorithms. Effective representations should be able to capture the underlying (abstract or high-level)
latent generative factors that are relevant for the end task while ignoring the inconsequential or
nuisance factors. Disentangled feature representations have the property that the generative factors
are revealed in disjoint subsets of the feature dimensions, such that a change in a single generative
factor causes a highly sparse change in the representation. Disentangled representations offer several
advantages – (i) Invariance: it is easier to derive representations that are invariant to nuisance factors
by simply marginalizing over the corresponding dimensions, (ii) Transferability: they are arguably
more suitable for transfer learning as most of the key underlying generative factors appear segregated
along feature dimensions, (iii) Interpretability: a human expert may be able to assign meanings to
the dimensions, (iv) Conditioning and intervention: they allow for interpretable conditioning and/or
intervention over a subset of the latents and observe the effects on other nodes in the graph. Indeed,
the importance of learning disentangled representations has been argued in several recent works
(Bengio et al., 2013; Lake et al., 2016; Ridgeway, 2016).
Recognizing the signiﬁcance of disentangled representations, several attempts have been made in this
direction in the past (Ridgeway, 2016). Much of the earlier work assumes some sort of supervision in
terms of: (i) partial or full access to the generative factors per instance (Reed et al., 2014; Yang et al.,
2015; Kulkarni et al., 2015; Karaletsos et al., 2015), (ii) knowledge about the nature of generative
factors (e.g, translation, rotation, etc.) (Hinton et al., 2011; Cohen & Welling, 2014), (iii) knowledge
about the changes in the generative factors across observations (e.g., sparse changes in consecutive
frames of a Video) (Goroshin et al., 2015; Whitney et al., 2016; Fraccaro et al., 2017; Denton &
Birodkar, 2017; Hsu et al., 2017), (iv) knowledge of a complementary signal to infer representations
that are conditionally independent of it1 (Cheung et al., 2014; Mathieu et al., 2016; Siddharth et al.,
2017). However, in most real scenarios, we only have access to raw observations without any
supervision about the generative factors. It is a challenging problem and many of the earlier attempts
have not been able to scale well for realistic settings (Schmidhuber, 1992; Desjardins et al., 2012;
Cohen & Welling, 2015) (see also, Higgins et al. (2017)).

1The representation itself can still be entangled in rest of the generative factors.

1

