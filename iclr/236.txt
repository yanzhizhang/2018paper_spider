Published as a conference paper at ICLR 2018

NEURAL LANGUAGE MODELING
BY JOINTLY LEARNING SYNTAX AND LEXICON

Yikang Shen, Zhouhan Lin, Chin-Wei Huang & Aaron Courville
Department of Computer Science and Operations Research
Universit de Montral
Montral, QC H3C3J7, Canada
{yi-kang.shen, zhouhan.lin, chin-wei.huang, aaron.courville}@umontreal.ca

ABSTRACT

We propose a neural language model capable of unsupervised syntactic structure
induction. The model leverages the structure information to form better semantic
representations and better language modeling. Standard recurrent neural networks
are limited by their structure and fail to efﬁciently use syntactic information. On
the other hand, tree-structured recursive networks usually require additional struc-
tural supervision at the cost of human expert annotation. In this paper, We pro-
pose a novel neural language model, called the Parsing-Reading-Predict Networks
(PRPN), that can simultaneously induce the syntactic structure from unannotated
sentences and leverage the inferred structure to learn a better language model. In
our model, the gradient can be directly back-propagated from the language model
loss into the neural parsing network. Experiments show that the proposed model
can discover the underlying syntactic structure and achieve state-of-the-art perfor-
mance on word/character-level language model tasks.

1

INTRODUCTION

Linguistic theories generally regard natural language as consisting of two part: a lexicon, the com-
plete set of all possible words in a language; and a syntax, the set of rules, principles, and processes
that govern the structure of sentences (Sandra & Taft, 1994). To generate a proper sentence, tokens
are put together with a speciﬁc syntactic structure. Understanding a sentence also requires lexical
information to provide meanings, and syntactical knowledge to correctly combine meanings. Cur-
rent neural language models can provide meaningful word represent (Bengio et al., 2003; Mikolov
et al., 2013; Chen et al., 2013). However, standard recurrent neural networks only implicitly model
syntax, thus fail to efﬁciently use structure information (Tai et al., 2015).
Developing a deep neural network that can leverage syntactic knowledge to form a better semantic
representation has received a great deal of attention in recent years (Socher et al., 2013; Tai et al.,
2015; Chung et al., 2016). Integrating syntactic structure into a language model is important for dif-
ferent reasons: 1) to obtain a hierarchical representation with increasing levels of abstraction, which
is a key feature of deep neural networks and of the human brain (Bengio et al., 2009; LeCun et al.,
2015; Schmidhuber, 2015); 2) to capture complex linguistic phenomena, like long-term dependency
problem (Tai et al., 2015) and the compositional effects (Socher et al., 2013); 3) to provide shortcut
for gradient back-propagation (Chung et al., 2016).
A syntactic parser is the most common source for structure information. Supervised parsers can
achieve very high performance on well constructed sentences. Hence, parsers can provide accurate
information about how to compose word semantics into sentence semantics (Socher et al., 2013),
or how to generate the next word given previous words (Wu et al., 2017). However, only major
languages have treebank data for training parsers, and it request expensive human expert annotation.
People also tend to break language rules in many circumstances (such as writing a tweet). These
defects limit the generalization capability of supervised parsers.
Unsupervised syntactic structure induction has been among the longstanding challenges of compu-
tational linguistic (Klein & Manning, 2002; 2004; Bod, 2006). Researchers are interested in this

1

