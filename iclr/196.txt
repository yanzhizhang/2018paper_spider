Published as a conference paper at ICLR 2018

LEARNING DIFFERENTIALLY PRIVATE RECURRENT
LANGUAGE MODELS

H. Brendan McMahan
mcmahan@google.com

Daniel Ramage
dramage@google.com

Kunal Talwar
kunal@google.com

Li Zhang
liqzhang@google.com

ABSTRACT

We demonstrate that it is possible to train large recurrent language models with
user-level differential privacy guarantees with only a negligible cost in predictive
accuracy. Our work builds on recent advances in the training of deep networks
on user-partitioned data and privacy accounting for stochastic gradient descent. In
particular, we add user-level privacy protection to the federated averaging algo-
rithm, which makes “large step” updates from user-level data. Our work demon-
strates that given a dataset with a sufﬁciently large number of users (a requirement
easily met by even small internet-scale datasets), achieving differential privacy
comes at the cost of increased computation, rather than in decreased utility as
in most prior work. We ﬁnd that our private LSTM language models are quan-
titatively and qualitatively similar to un-noised models when trained on a large
dataset.

1

INTRODUCTION

Deep recurrent models like long short-term memory (LSTM) recurrent neural networks (RNNs)
have become a standard building block in modern approaches to language modeling, with applica-
tions in speech recognition, input decoding for mobile keyboards, and language translation. Because
language usage varies widely by problem domain and dataset, training a language model on data
from the right distribution is critical. For example, a model to aid typing on a mobile keyboard is
better served by training data typed in mobile apps rather than from scanned books or transcribed
utterances. However, language data can be uniquely privacy sensitive. In the case of text typed
on a mobile phone, this sensitive information might include passwords, text messages, and search
queries. In general, language data may identify a speaker—explicitly by name or implicitly, for
example via a rare or unique phrase—and link that speaker to secret or sensitive information.
Ideally, a language model’s parameters would encode patterns of language use common to many
users without memorizing any individual user’s unique input sequences. However, we know convo-
lutional NNs can memorize arbitrary labelings of the training data (Zhang et al., 2017) and recurrent
language models are also capable of memorizing unique patterns in the training data (Carlini et al.,
2018). Recent attacks on neural networks such as those of Shokri et al. (2017) underscore the im-
plicit risk. The main goal of our work is to provide a strong guarantee that the trained model protects
the privacy of individuals’ data without undue sacriﬁce in model quality.
We are motivated by the problem of training models for next-word prediction in a mobile keyboard,
and use this as a running example. This problem is well suited to the techniques we introduce, as
differential privacy may allow for training on data from the true distribution (actual mobile usage)
rather than on proxy data from some other source that would produce inferior models. However,
to facilitate reproducibility and comparison to non-private models, our experiments are conducted
on a public dataset as is standard in differential privacy research. The remainder of this paper is
structured around the following contributions:
1. We apply differential privacy to model training using the notion of user-adjacent datasets, leading
to formal guarantees of user-level privacy, rather than privacy for single examples.

1

