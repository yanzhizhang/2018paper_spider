Published as a conference paper at ICLR 2018

LEARNING AWARENESS MODELS

Brandon Amos1∗ Laurent Dinh2

Serkan Cabi3 Thomas Roth¨orl3

Sergio G´omez Colmenarejo3

Alistair Muldal3 Tom Erez3 Yuval Tassa3 Nando de Freitas3,4 Misha Denil3

1Carnegie Mellon University

2University of Montreal

3DeepMind

4CIFAR

ABSTRACT

We consider the setting of an agent with a ﬁxed body interacting with an unknown
and uncertain external world. We show that models trained to predict propriocep-
tive information about the agent’s body come to represent objects in the external
world. In spite of being trained with only internally available signals, these dy-
namic body models come to represent external objects through the necessity of
predicting their effects on the agent’s own body. That is, the model learns holistic
persistent representations of objects in the world, even though the only training
signals are body signals. Our dynamics model is able to successfully predict dis-
tributions over 132 sensor readings over 100 steps into the future and we demon-
strate that even when the body is no longer in contact with an object, the latent
variables of the dynamics model continue to represent its shape. We show that
active data collection by maximizing the entropy of predictions about the body—
touch sensors, proprioception and vestibular information—leads to learning of
dynamic models that show superior performance when used for control. We also
collect data from a real robotic hand and show that the same models can be used
to answer questions about properties of objects in the real world. Videos with
qualitative results of our models are available at https://goo.gl/mZuqAV.

1

INTRODUCTION

Situation awareness is the perception of the elements in the environment within
a volume of time and space, and the comprehension of their meaning, and the
projection of their status in the near future. — Endsley (1987)

As artiﬁcial intelligence moves off of the server and out into the world at large; be this the virtual
world, in the form of simulated walkers, climbers and other creatures (Heess et al., 2017), or the real
world in the form of virtual assistants, self driving vehicles (Bojarski et al., 2016), and household
robots (Jain et al., 2013); we are increasingly faced with the need to build systems that understand
and reason about the world around them.
When building systems like this it is natural to think of the physical world as breaking into two parts.
The ﬁrst part is the platform, the part we design and build, and therefore know quite a lot about; and
the second part is everything else, which comprises all the strange and exciting situations that the
platform might encounter. As designers, we have very little control over the external part of the
world, and the variety of situations that might arise are too numerous to anticipate in advance. Addi-
tionally, while the state of the platform is readily accessible (e.g. through deployment of integrated
sensors), the state of the external world is generally not available to the system.
The platform hosts any sensors and actuators that are part of the system, and importantly it can be
relied on to be the same across the wide variety situations where the system might be deployed.
A virtual assistant can rely on having access to the camera and microphone on your smart phone,
and the control system for a self driving car can assume it is controlling a speciﬁc make and model
of vehicle, and that it has access to any specialized hardware installed by the manufacturer. These
consistency assumptions hold regardless of what is happening in the external world.
This same partitioning of the world occurs naturally for living creatures as well. As a human being
your platform is your body; it maintains a constant size and shape throughout your life (or at least

∗Work done while BA and LD were interns at DeepMind.

1

