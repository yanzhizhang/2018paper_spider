Published as a conference paper at ICLR 2018

LEARNING TO MULTI-TASK BY ACTIVE SAMPLING

Sahil Sharma*
Department of Computer Science and Engineering
Indian Institute of Technology, Madras

Ashutosh Kumar Jha*
Department of Mechanical Engineering
Indian Institute of Technology, Madras

Parikshit S Hegde
Department of Electrical Engineering
Indian Institute of Technology, Madras

Balaraman Ravindran
Department of Computer Science and Engineering
and Robert Bosch Centre for Data Science
and AI (RBC-DSAI)
Indian Institute of Technology, Madras

ABSTRACT

One of the long-standing challenges in Artiﬁcial Intelligence for learning goal-
directed behavior is to build a single agent which can solve multiple tasks. Recent
progress in multi-task learning for goal-directed sequential problems has been
in the form of distillation based learning wherein a student network learns from
multiple task-speciﬁc expert networks by mimicking the task-speciﬁc policies of
the expert networks. While such approaches offer a promising solution to the
multi-task learning problem, they require supervision from large expert networks
which require extensive data and computation time for training. In this work, we
propose an efﬁcient multi-task learning framework which solves multiple goal-
directed tasks in an on-line setup without the need for expert supervision. Our
work uses active learning principles to achieve multi-task learning by sampling the
harder tasks more than the easier ones. We propose three distinct models under
our active sampling framework. An adaptive method with extremely competitive
multi-tasking performance. A UCB-based meta-learner which casts the problem of
picking the next task to train on as a multi-armed bandit problem. A meta-learning
method that casts the next-task picking problem as a full Reinforcement Learning
problem and uses actor critic methods for optimizing the multi-tasking performance
directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking
instances: three 6-task instances, one 8-task instance, two 12-task instances and
one 21-task instance.

1

INTRODUCTION

Deep Reinforcement Learning (DRL) arises from the combination of the representation power of
Deep learning (DL) (LeCun et al., 2015; Bengio et al., 2009) with the use of Reinforcement Learning
(RL) (Sutton & Barto, 1998) objective functions. DRL agents can solve complex visual control tasks
directly from raw pixels (Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al.,
2015; Schaul et al., 2015; Mnih et al., 2016a; Van Hasselt et al., 2016; Vezhnevets et al., 2016; Bacon
et al., 2017; Sharma et al., 2017; Jaderberg et al., 2017).
However, models trained using such algorithms tend to be task-speciﬁc because they train a different
network for different tasks, however similar the tasks are. This inability of the AI agents to generalize
across tasks motivates the ﬁeld of multi-task learning which seeks to ﬁnd a single agent (in the case
of DRL algorithms, a single deep neural network) which can perform well on all the tasks. Training
a neural network with a multi-task learning (MTL) algorithm on any ﬁxed set of tasks (which we call
a multi tasking instance (MTI)) leads to an instantiation of a multi-tasking agent (MTA) (we use the
terms Multi-Tasking Network (MTN) and MTA interchangeably). Such an MTA would possess the
ability to learn task-agnostic representations and thus generalize learning across different tasks.
Successful DRL approaches to the goal-directed MTL problem fall into two categories. First, there
are approaches that seek to extract the prowess of multiple task-speciﬁc expert networks into a

* - The authors had equal contribution

1

