Published as a conference paper at ICLR 2018

WRPN: WIDE REDUCED-PRECISION NETWORKS

Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook & Debbie Marr
Accelerator Architecture Lab
Intel Labs
{asit.k.mishra,eriko.nurvitadhi,jeffrey.j.cook,debbie.marr}@intel.com

ABSTRACT

For computer vision applications, prior works have shown the efﬁcacy of re-
ducing numeric precision of model parameters (network weights) in deep neu-
ral networks. Activation maps, however, occupy a large memory footprint dur-
ing both the training and inference step when using mini-batches of inputs. One
way to reduce this large memory footprint is to reduce the precision of activa-
tions. However, past works have shown that reducing the precision of activations
hurts model accuracy. We study schemes to train networks from scratch using
reduced-precision activations without hurting accuracy. We reduce the precision
of activation maps (along with model parameters) and increase the number of ﬁl-
ter maps in a layer, and ﬁnd that this scheme matches or surpasses the accuracy
of the baseline full-precision network. As a result, one can signiﬁcantly improve
the execution efﬁciency (e.g. reduce dynamic memory footprint, memory band-
width and computational energy) and speed up the training and inference process
with appropriate hardware support. We call our scheme WRPN - wide reduced-
precision networks. We report results and show that WRPN scheme is better than
previously reported accuracies on ILSVRC-12 dataset while being computation-
ally less expensive compared to previously reported reduced-precision networks.

1

INTRODUCTION

A promising approach to lower the compute and memory requirements of convolutional deep-
learning workloads is through the use of low numeric precision algorithms. Operating in lower
precision mode reduces computation as well as data movement and storage requirements. Due to
such efﬁciency beneﬁts, there are many existing works which propose low-precision deep neural net-
works (DNNs) (Zhou et al., 2017; Lin et al., 2015; Miyashita et al., 2016; Gupta et al., 2015b; Van-
houcke et al., 2011), even down to 2-bit ternary mode (Zhu et al., 2016; Li & Liu, 2016; Venkatesh
et al., 2016) and 1-bit binary mode (Zhou et al., 2016; Courbariaux & Bengio, 2016; Rastegari
et al., 2016; Courbariaux et al., 2015; Umuroglu et al., 2016). However, majority of existing works
in low-precision DNNs sacriﬁce accuracy over the baseline full-precision networks. Further, most
prior works target reducing the precision of the model parameters (network weights). This primarily
beneﬁts the inference step only when batch sizes are small.
We observe that activation maps (neuron outputs) occupy more memory compared to the model
parameters for batch sizes typical during training. This observation holds even during inference
when batch size is around eight or more. Based on this observation, we study schemes for training
and inference using low-precision DNNs where we reduce the precision of activation maps as well
as the model parameters without sacriﬁcing network accuracy.
To improve both execution efﬁciency and accuracy of low-precision networks, we reduce both the
precision of activation maps and model parameters and increase the number of ﬁlter maps in a layer.
We call networks using this scheme wide reduced-precision networks (WRPN) and ﬁnd that this
scheme compensates or surpasses the accuracy of the baseline full-precision network. Although the
number of raw compute operations increases as we increase the number of ﬁlter maps in a layer, the
compute bits required per operation is now a fraction of what is required when using full-precision
operations (e.g. going from FP32 AlexNet to 4-bits precision and doubling the number of ﬁlters
increases the number of compute operations by 4x, but each operation is 8x more efﬁcient than
FP32).

1

